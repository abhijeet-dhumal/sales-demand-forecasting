{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Distributed Training with Feast + Remote KubeRay\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook trains a **PyTorch MLP model** for sales forecasting using features retrieved from Feast via a **remote KubeRay cluster**. The key operation is `get_historical_features()` which performs distributed point-in-time (PIT) joins.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "| Step | Component | Action |\n",
    "|------|-----------|--------|\n",
    "| 1 | Feast + Ray | `get_historical_features()` via remote KubeRay cluster |\n",
    "| 2 | PyTorch | Train MLP model with proper scaling |\n",
    "| 3 | Artifacts | Save model, scalers, metadata to shared PVC |\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        TRAINING DATA FLOW                                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\n",
    "â”‚  â”‚     Feast       â”‚â”€â”€â”€â”€â–¶â”‚    KubeRay      â”‚â”€â”€â”€â”€â–¶â”‚   Training      â”‚        â”‚\n",
    "â”‚  â”‚  FeatureStore   â”‚     â”‚  \"feast-ray\"    â”‚     â”‚  (PyTorch)      â”‚        â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\n",
    "â”‚          â”‚                       â”‚                       â”‚                  â”‚\n",
    "â”‚          â”‚ get_historical_       â”‚ Distributed           â”‚ Model           â”‚\n",
    "â”‚          â”‚ features()            â”‚ PIT Joins             â”‚ Training        â”‚\n",
    "â”‚          â”‚                       â”‚ (65K rows)            â”‚                 â”‚\n",
    "â”‚          â–¼                       â–¼                       â–¼                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚                         OUTPUT ARTIFACTS                             â”‚   â”‚\n",
    "â”‚  â”‚  /shared/models/                                                     â”‚   â”‚\n",
    "â”‚  â”‚  â”œâ”€â”€ model_best.pt          # Best model weights                     â”‚   â”‚\n",
    "â”‚  â”‚  â”œâ”€â”€ model_final.pt         # Final model weights                    â”‚   â”‚\n",
    "â”‚  â”‚  â”œâ”€â”€ scalers.joblib         # Feature + target scalers               â”‚   â”‚\n",
    "â”‚  â”‚  â””â”€â”€ model_metadata.json    # Architecture, feature columns          â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Why Ray for `get_historical_features()`?\n",
    "\n",
    "| Approach | 65K rows | 1M rows | 10M rows |\n",
    "|----------|----------|---------|----------|\n",
    "| **Local (Pandas)** | 2 min | 30+ min | OOM crash |\n",
    "| **Ray (Distributed)** | 30 sec | 3 min | 15 min |\n",
    "\n",
    "Ray distributes the point-in-time joins across the cluster, preventing memory issues and speeding up processing.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. `01-feast-features.ipynb` completed (features registered)\n",
    "2. RayCluster `feast-ray` running\n",
    "3. PostgreSQL online store populated\n",
    "\n",
    "**Recommended for Production:** Use `kubectl apply -f manifests/06-trainjob.yaml` for multi-node DDP training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Install Dependencies & Setup Auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"feast[postgres,ray]==0.59.0\" codeflare-sdk psycopg2-binary scikit-learn joblib pandas pyarrow torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup CodeFlare SDK auth from service account token\n",
    "token_path = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n",
    "if os.path.exists(token_path):\n",
    "    with open(token_path) as f:\n",
    "        os.environ[\"FEAST_RAY_AUTH_TOKEN\"] = f.read().strip()\n",
    "    k8s_host = os.environ.get(\"KUBERNETES_SERVICE_HOST\")\n",
    "    k8s_port = os.environ.get(\"KUBERNETES_SERVICE_PORT\", \"443\")\n",
    "    os.environ[\"FEAST_RAY_AUTH_SERVER\"] = f\"https://{k8s_host}:{k8s_port}\"\n",
    "    os.environ[\"FEAST_RAY_SKIP_TLS\"] = \"true\"\n",
    "    print(\"ðŸ” CodeFlare SDK auth configured from service account\")\n",
    "else:\n",
    "    print(\"âš ï¸ Not running in cluster - some features may not work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Configuration\n",
    "\n",
    "| Parameter | Value | Purpose |\n",
    "|-----------|-------|---------|\n",
    "| `NUM_EPOCHS` | 20 | Training iterations |\n",
    "| `BATCH_SIZE` | 512 | Samples per gradient update |\n",
    "| `LEARNING_RATE` | 1e-3 | AdamW optimizer LR |\n",
    "| `RAY_CLUSTER` | feast-ray | KubeRay cluster for Feast |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "SHARED_ROOT = Path(\"/opt/app-root/src/shared\")\n",
    "FEATURE_REPO = SHARED_ROOT / \"feature_repo\"\n",
    "DATA_DIR = SHARED_ROOT / \"data\"\n",
    "OUTPUT_DIR = SHARED_ROOT / \"models\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training params\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Ray/CodeFlare config\n",
    "RAY_CLUSTER = \"feast-ray\"\n",
    "NAMESPACE = \"feast-trainer-demo\"\n",
    "\n",
    "# Set Ray cluster environment\n",
    "os.environ[\"FEAST_RAY_USE_KUBERAY\"] = \"true\"\n",
    "os.environ[\"FEAST_RAY_CLUSTER_NAME\"] = RAY_CLUSTER\n",
    "os.environ[\"FEAST_RAY_NAMESPACE\"] = NAMESPACE\n",
    "\n",
    "print(f\"ðŸ“ Feature repo: {FEATURE_REPO}\")\n",
    "print(f\"ðŸ“ Output dir: {OUTPUT_DIR}\")\n",
    "print(f\"ðŸŽ¯ Epochs: {NUM_EPOCHS}, Batch: {BATCH_SIZE}, LR: {LEARNING_RATE}\")\n",
    "print(f\"ðŸ”— Ray cluster: {RAY_CLUSTER} in namespace {NAMESPACE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Setup Feast with Ray Config\n",
    "\n",
    "Use the Ray-enabled configuration for distributed `get_historical_features()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Ray config as main config (if not already done)\n",
    "ray_config = FEATURE_REPO / \"feature_store_ray.yaml\"\n",
    "if ray_config.exists():\n",
    "    shutil.copy(ray_config, FEATURE_REPO / \"feature_store.yaml\")\n",
    "    print(f\"âœ… Using Ray config: {ray_config}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Ray config not found, using existing feature_store.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "\n",
    "store = FeatureStore(repo_path=str(FEATURE_REPO))\n",
    "print(f\"âœ… FeatureStore loaded\")\n",
    "print(f\"   Feature services: {[fs.name for fs in store.list_feature_services()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Get Historical Features via Ray\n",
    "\n",
    "This is the **key Feast operation** - retrieve features with point-in-time correctness.\n",
    "\n",
    "### What Happens Under the Hood\n",
    "\n",
    "```\n",
    "Entity DataFrame (65K rows)          Feature Views\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ store_id, dept_id, ts     â”‚       â”‚ sales_features.parquet    â”‚\n",
    "â”‚ 1, 1, 2022-01-01          â”‚  â”€â”€â”€â–¶ â”‚ store_features.parquet    â”‚\n",
    "â”‚ 1, 1, 2022-01-08          â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "â”‚ ...65,520 rows            â”‚                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\n",
    "              â”‚                               â–¼\n",
    "              â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  KubeRay Distributed PIT Joins    â”‚\n",
    "                        â”‚  â€¢ Partition by entity keys       â”‚\n",
    "                        â”‚  â€¢ As-of join on timestamp        â”‚\n",
    "                        â”‚  â€¢ Parallel across workers        â”‚\n",
    "                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                          â”‚\n",
    "                                          â–¼\n",
    "                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                        â”‚  Feature Matrix (65K Ã— 22)        â”‚\n",
    "                        â”‚  Ready for model training         â”‚\n",
    "                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Point-in-Time Correctness\n",
    "For each entity row, Feast returns features **as they were known at that timestamp**, preventing data leakage from future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity DataFrame (what we want features for)\n",
    "print(\"Creating entity DataFrame...\")\n",
    "\n",
    "entity_rows = []\n",
    "base_date = datetime(2022, 1, 1, tzinfo=timezone.utc)\n",
    "NUM_WEEKS, NUM_STORES, NUM_DEPTS = 104, 45, 14\n",
    "\n",
    "for week in range(NUM_WEEKS):\n",
    "    event_ts = base_date + timedelta(weeks=week)\n",
    "    for store_id in range(1, NUM_STORES + 1):\n",
    "        for dept_id in range(1, NUM_DEPTS + 1):\n",
    "            entity_rows.append({\n",
    "                \"store_id\": store_id,\n",
    "                \"dept_id\": dept_id,\n",
    "                \"event_timestamp\": event_ts\n",
    "            })\n",
    "\n",
    "entity_df = pd.DataFrame(entity_rows)\n",
    "print(f\"âœ… Entity DataFrame: {len(entity_df):,} rows\")\n",
    "print(f\"   ({NUM_WEEKS} weeks Ã— {NUM_STORES} stores Ã— {NUM_DEPTS} depts)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š SAMPLE DATA: Entity DataFrame\n",
    "print(\"ðŸ“Š Sample: Entity DataFrame (first 5 rows)\")\n",
    "entity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get historical features via Ray\n",
    "print(\"\\nðŸš€ Calling get_historical_features() via KubeRay...\")\n",
    "print(\"   This distributes PIT joins across the Ray cluster\")\n",
    "print(\"   Expected time: 30 seconds - 2 minutes for 65K rows\")\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "training_data = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=store.get_feature_service(\"training_features\"),\n",
    ")\n",
    "training_df = training_data.to_df()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "throughput = len(entity_df) / elapsed if elapsed > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… Features retrieved!\")\n",
    "print(f\"   Rows: {len(training_df):,}\")\n",
    "print(f\"   Columns: {len(training_df.columns)}\")\n",
    "print(f\"   Time: {elapsed:.1f}s\")\n",
    "print(f\"   Throughput: {throughput:,.0f} rows/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š SAMPLE DATA: Retrieved features from KubeRay\n",
    "print(\"ðŸ“Š Sample: Retrieved Features via Remote KubeRay\")\n",
    "print(f\"   Total columns: {len(training_df.columns)}\")\n",
    "feature_names = [c for c in training_df.columns if c not in ['store_id', 'dept_id', 'event_timestamp']]\n",
    "print(f\"   Feature columns: {feature_names}\")\n",
    "print()\n",
    "training_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Prepare Training Data\n",
    "\n",
    "### Data Preparation Steps\n",
    "\n",
    "1. **Drop missing targets**: Remove rows without `weekly_sales`\n",
    "2. **Temporal split**: 80% train, 20% validation (chronological)\n",
    "3. **Feature scaling**: StandardScaler for features\n",
    "4. **Target scaling**: Log transform + StandardScaler for better MAPE optimization\n",
    "\n",
    "### Why Log Transform Target?\n",
    "Retail sales are often skewed. Log transform:\n",
    "- Normalizes the distribution\n",
    "- Makes the model optimize for percentage error (MAPE) rather than absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing target\n",
    "training_df = training_df.dropna(subset=[\"weekly_sales\"])\n",
    "print(f\"Rows after dropping NaN: {len(training_df):,}\")\n",
    "\n",
    "# Temporal split (80/20) - important for time series!\n",
    "training_df = training_df.sort_values(\"event_timestamp\")\n",
    "split_idx = int(len(training_df) * 0.8)\n",
    "train_df = training_df.iloc[:split_idx]\n",
    "val_df = training_df.iloc[split_idx:]\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows ({train_df['event_timestamp'].min().date()} to {train_df['event_timestamp'].max().date()})\")\n",
    "print(f\"Val: {len(val_df):,} rows ({val_df['event_timestamp'].min().date()} to {val_df['event_timestamp'].max().date()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns (exclude identifiers and target)\n",
    "exclude_cols = [\"store_id\", \"dept_id\", \"date\", \"event_timestamp\", \"weekly_sales\"]\n",
    "feature_cols = [\n",
    "    c for c in training_df.columns \n",
    "    if c not in exclude_cols\n",
    "    and training_df[c].dtype in [np.float64, np.int64, np.float32, np.int32]\n",
    "]\n",
    "\n",
    "print(f\"Feature columns ({len(feature_cols)}):\")\n",
    "for i, col in enumerate(feature_cols):\n",
    "    print(f\"  {i+1:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare arrays\n",
    "X_train = train_df[feature_cols].fillna(0).values\n",
    "y_train = train_df[\"weekly_sales\"].values\n",
    "X_val = val_df[feature_cols].fillna(0).values\n",
    "y_val = val_df[\"weekly_sales\"].values\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Log transform + scale target (better for % error optimization)\n",
    "y_scaler = StandardScaler()\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_val_log = np.log1p(y_val)\n",
    "y_train_scaled = y_scaler.fit_transform(y_train_log.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = y_scaler.transform(y_val_log.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train_scaled.shape}\")\n",
    "print(f\"y_train range: [{y_train.min():.0f}, {y_train.max():.0f}] â†’ scaled: [{y_train_scaled.min():.2f}, {y_train_scaled.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scalers and feature columns for inference\n",
    "joblib.dump({\n",
    "    \"scaler_X\": scaler,\n",
    "    \"scaler_y\": y_scaler,\n",
    "    \"use_log_transform\": True\n",
    "}, OUTPUT_DIR / \"scalers.joblib\")\n",
    "joblib.dump(feature_cols, OUTPUT_DIR / \"feature_cols.pkl\")\n",
    "print(f\"âœ… Scalers saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Model Definition\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "```\n",
    "Input (21 features)\n",
    "    â”‚\n",
    "    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Linear(21 â†’ 256)         â”‚\n",
    "â”‚ BatchNorm + ReLU         â”‚\n",
    "â”‚ Dropout(0.2)             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Linear(256 â†’ 128)        â”‚\n",
    "â”‚ BatchNorm + ReLU         â”‚\n",
    "â”‚ Dropout(0.2)             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Linear(128 â†’ 64)         â”‚\n",
    "â”‚ BatchNorm + ReLU         â”‚\n",
    "â”‚ Dropout(0.2)             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Linear(64 â†’ 1)           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â”‚\n",
    "    â–¼\n",
    "Output (scaled prediction)\n",
    "```\n",
    "\n",
    "### Why This Architecture?\n",
    "- **BatchNorm**: Stabilizes training, allows higher learning rates\n",
    "- **Dropout**: Prevents overfitting on tabular data\n",
    "- **Decreasing width**: 256â†’128â†’64 captures feature interactions then compresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalesMLP(nn.Module):\n",
    "    \"\"\"Multi-layer perceptron for sales prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=None, dropout=0.2):\n",
    "        super().__init__()\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [256, 128, 64]\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.BatchNorm1d(dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "\n",
    "class SalesDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for sales data.\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, i): \n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Training Loop\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "| Component | Setting | Why |\n",
    "|-----------|---------|-----|\n",
    "| Optimizer | AdamW | Weight decay regularization built-in |\n",
    "| Scheduler | CosineAnnealing | Smooth LR decay |\n",
    "| Loss | MSE | Standard regression loss |\n",
    "| Gradient clipping | 1.0 | Prevents exploding gradients |\n",
    "\n",
    "### Metrics\n",
    "- **Train/Val Loss**: MSE on scaled target\n",
    "- **MAPE**: Mean Absolute Percentage Error (business metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_dataset = SalesDataset(X_train, y_train_scaled)\n",
    "val_dataset = SalesDataset(X_val, y_val_scaled)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "input_dim = X_train.shape[1]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SalesMLP(input_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Input features: {input_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "history = []\n",
    "\n",
    "print(f\"\\nðŸš€ Training for {NUM_EPOCHS} epochs...\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Epoch':>6} {'Train Loss':>12} {'Val Loss':>12} {'MAPE':>10} {'LR':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            pred = model(X_batch)\n",
    "            val_loss += criterion(pred, y_batch).item()\n",
    "            val_preds.extend(pred.cpu().numpy())\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    # Calculate MAPE (business metric)\n",
    "    val_preds = np.array(val_preds)\n",
    "    pred_orig = np.expm1(y_scaler.inverse_transform(val_preds.reshape(-1, 1)).flatten())\n",
    "    mask = y_val > 1000  # Exclude very small values\n",
    "    mape = np.mean(np.abs((y_val[mask] - pred_orig[mask]) / y_val[mask])) * 100\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step()\n",
    "    history.append({\"epoch\": epoch + 1, \"train_loss\": train_loss, \"val_loss\": val_loss, \"mape\": mape, \"lr\": current_lr})\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), OUTPUT_DIR / \"model_best.pt\")\n",
    "    \n",
    "    # Print progress\n",
    "    marker = \"*\" if val_loss <= best_val_loss else \" \"\n",
    "    print(f\"{epoch+1:>5}{marker} {train_loss:>12.4f} {val_loss:>12.4f} {mape:>9.1f}% {current_lr:>12.6f}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "torch.save(model.state_dict(), OUTPUT_DIR / \"model_final.pt\")\n",
    "\n",
    "# Save model metadata\n",
    "best_mape = min(h[\"mape\"] for h in history)\n",
    "model_info = {\n",
    "    \"model_type\": \"SalesMLP\",\n",
    "    \"input_dim\": input_dim,\n",
    "    \"hidden_dims\": [256, 128, 64],\n",
    "    \"dropout\": 0.2,\n",
    "    \"best_val_loss\": best_val_loss,\n",
    "    \"best_mape\": best_mape,\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"feature_columns\": feature_cols,\n",
    "    \"timestamp\": datetime.now(timezone.utc).isoformat()\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / \"model_metadata.json\", \"w\") as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"âœ… TRAINING COMPLETE\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best MAPE: {best_mape:.1f}%\")\n",
    "print(f\"\\nArtifacts saved to: {OUTPUT_DIR}\")\n",
    "print(f\"  - model_best.pt\")\n",
    "print(f\"  - model_final.pt\")\n",
    "print(f\"  - scalers.joblib\")\n",
    "print(f\"  - model_metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Training History\n",
    "print(\"ðŸ“Š Training History:\")\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## âœ… Complete!\n",
    "\n",
    "### Artifacts Summary\n",
    "\n",
    "| File | Purpose | Size |\n",
    "|------|---------|------|\n",
    "| `model_best.pt` | Best model weights (lowest val loss) | ~500KB |\n",
    "| `model_final.pt` | Final model weights | ~500KB |\n",
    "| `scalers.joblib` | Feature and target scalers | ~10KB |\n",
    "| `model_metadata.json` | Architecture, feature columns | ~2KB |\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "| Metric | Description | Good Range |\n",
    "|--------|-------------|------------|\n",
    "| **MAPE** | Mean Absolute % Error | <10% excellent, <20% good |\n",
    "| **Val Loss** | MSE on scaled target | Lower is better |\n",
    "\n",
    "### Alternative: Use TrainJob Manifest (Multi-Node DDP)\n",
    "\n",
    "For production distributed training across multiple nodes:\n",
    "\n",
    "```bash\n",
    "kubectl apply -f manifests/06-trainjob.yaml\n",
    "kubectl logs -f -l trainer.kubeflow.org/trainjob-name=sales-training\n",
    "```\n",
    "\n",
    "The TrainJob uses:\n",
    "- **Multi-node DDP**: Distributes training across 2+ nodes\n",
    "- **torchrun**: Handles distributed process management\n",
    "- **Same Feast integration**: `get_historical_features()` via Ray\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- `03-inference.ipynb` â†’ Deploy model with KServe for real-time predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
