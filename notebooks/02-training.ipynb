{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - Distributed Training with Feast + Remote KubeRay\n",
        "\n",
        "![Workflow](../docs/02-training-workflow.png)\n",
        "\n",
        "## What This Notebook Does\n",
        "\n",
        "| Step | Component | Action |\n",
        "|------|-----------|--------|\n",
        "| 1 | Feast + Ray | `get_historical_features()` via remote KubeRay cluster |\n",
        "| 2 | PyTorch | GPU-accelerated training |\n",
        "| 3 | Artifacts | Save model to shared PVC |\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚     Feast       â”‚â”€â”€â”€â”€â–¶â”‚    KubeRay      â”‚â”€â”€â”€â”€â–¶â”‚   Training      â”‚\n",
        "â”‚  FeatureStore   â”‚     â”‚  \"feast-ray\"    â”‚     â”‚  (PyTorch GPU)  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "        â”‚                       â”‚                       â”‚\n",
        "        â”‚ get_historical_       â”‚ Distributed           â”‚ GPU\n",
        "        â”‚ features()            â”‚ PIT Joins             â”‚ Training\n",
        "        â”‚                       â”‚ (with mTLS)           â”‚\n",
        "```\n",
        "\n",
        "**Recommended:** Use `kubectl apply -f manifests/06-trainjob.yaml` instead of this notebook for production.\n",
        "\n",
        "**Prerequisites:** `01-feast-features.ipynb` completed, RayCluster `feast-ray` running."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install -q \"feast[postgres,ray]==0.59.0\" codeflare-sdk psycopg2-binary scikit-learn joblib pandas pyarrow torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "from datetime import datetime, timezone, timedelta\n",
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Paths\n",
        "SHARED_ROOT = Path(\"/opt/app-root/src/shared\")\n",
        "FEATURE_REPO = SHARED_ROOT / \"feature_repo\"\n",
        "DATA_DIR = SHARED_ROOT / \"data\"\n",
        "OUTPUT_DIR = SHARED_ROOT / \"models\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Training params\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 512\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "# Ray/CodeFlare config\n",
        "RAY_CLUSTER = \"feast-ray\"\n",
        "NAMESPACE = \"feast-trainer-demo\"\n",
        "\n",
        "print(f\"ðŸ“ Feature repo: {FEATURE_REPO}\")\n",
        "print(f\"ðŸ“ Output dir: {OUTPUT_DIR}\")\n",
        "print(f\"ðŸŽ¯ Epochs: {NUM_EPOCHS}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup CodeFlare SDK Auth\n",
        "\n",
        "Configure authentication for KubeRay cluster:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup CodeFlare SDK auth from service account token\n",
        "token_path = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n",
        "if os.path.exists(token_path):\n",
        "    with open(token_path) as f:\n",
        "        os.environ[\"FEAST_RAY_AUTH_TOKEN\"] = f.read().strip()\n",
        "    k8s_host = os.environ.get(\"KUBERNETES_SERVICE_HOST\")\n",
        "    k8s_port = os.environ.get(\"KUBERNETES_SERVICE_PORT\", \"443\")\n",
        "    os.environ[\"FEAST_RAY_AUTH_SERVER\"] = f\"https://{k8s_host}:{k8s_port}\"\n",
        "    os.environ[\"FEAST_RAY_SKIP_TLS\"] = \"true\"\n",
        "    print(\"ðŸ” CodeFlare SDK auth configured from service account\")\n",
        "else:\n",
        "    print(\"âš ï¸ No service account token found - using default auth\")\n",
        "\n",
        "# Set Ray cluster info\n",
        "os.environ[\"FEAST_RAY_USE_KUBERAY\"] = \"true\"\n",
        "os.environ[\"FEAST_RAY_CLUSTER_NAME\"] = RAY_CLUSTER\n",
        "os.environ[\"FEAST_RAY_NAMESPACE\"] = NAMESPACE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Feast with Ray Config\n",
        "\n",
        "Use the Ray-enabled config for distributed `get_historical_features()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Copy Ray config as main config\n",
        "ray_config = FEATURE_REPO / \"feature_store_ray.yaml\"\n",
        "if ray_config.exists():\n",
        "    shutil.copy(ray_config, FEATURE_REPO / \"feature_store.yaml\")\n",
        "    print(f\"âœ… Using Ray config: {ray_config}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ Ray config not found, using existing feature_store.yaml\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get Historical Features via Ray\n",
        "\n",
        "This is the key Feast operation - retrieve features with point-in-time correctness:\n",
        "\n",
        "```\n",
        "Entity DataFrame (65K rows)\n",
        "    â”‚\n",
        "    â–¼\n",
        "Feast get_historical_features()\n",
        "    â”‚\n",
        "    â”œâ”€â”€ Connect to KubeRay via CodeFlare SDK\n",
        "    â”œâ”€â”€ Distribute PIT joins across workers\n",
        "    â””â”€â”€ Return feature matrix\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from feast import FeatureStore\n",
        "\n",
        "store = FeatureStore(repo_path=str(FEATURE_REPO))\n",
        "print(f\"âœ… FeatureStore loaded\")\n",
        "print(f\"   Feature services: {[fs.name for fs in store.list_feature_services()]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create entity DataFrame (what we want features for)\n",
        "print(\"Creating entity DataFrame...\")\n",
        "\n",
        "entity_rows = []\n",
        "base_date = datetime(2022, 1, 1, tzinfo=timezone.utc)\n",
        "NUM_WEEKS, NUM_STORES, NUM_DEPTS = 104, 45, 14\n",
        "\n",
        "for week in range(NUM_WEEKS):\n",
        "    event_ts = base_date + timedelta(weeks=week)\n",
        "    for store_id in range(1, NUM_STORES + 1):\n",
        "        for dept_id in range(1, NUM_DEPTS + 1):\n",
        "            entity_rows.append({\n",
        "                \"store_id\": store_id,\n",
        "                \"dept_id\": dept_id,\n",
        "                \"event_timestamp\": event_ts\n",
        "            })\n",
        "\n",
        "entity_df = pd.DataFrame(entity_rows)\n",
        "print(f\"âœ… Entity DataFrame: {len(entity_df):,} rows\")\n",
        "print(f\"   ({NUM_WEEKS} weeks Ã— {NUM_STORES} stores Ã— {NUM_DEPTS} depts)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get historical features via Ray\n",
        "print(\"\\nðŸš€ Calling get_historical_features() via KubeRay...\")\n",
        "print(\"   This distributes PIT joins across the Ray cluster\")\n",
        "print(\"   Expected time: 2-5 minutes for 65K rows\")\n",
        "print()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "training_data = store.get_historical_features(\n",
        "    entity_df=entity_df,\n",
        "    features=store.get_feature_service(\"training_features\"),\n",
        ")\n",
        "training_df = training_data.to_df()\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "throughput = len(entity_df) / elapsed if elapsed > 0 else 0\n",
        "\n",
        "print(f\"\\nâœ… Features retrieved!\")\n",
        "print(f\"   Rows: {len(training_df):,}\")\n",
        "print(f\"   Columns: {len(training_df.columns)}\")\n",
        "print(f\"   Time: {elapsed:.1f}s\")\n",
        "print(f\"   Throughput: {throughput:,.0f} rows/sec\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ðŸ“Š SAMPLE DATA: Retrieved features from KubeRay\n",
        "print(\"ðŸ“Š Sample: Retrieved features via Remote KubeRay\")\n",
        "print(f\"   Total columns: {len(training_df.columns)}\")\n",
        "print(f\"   Feature columns: {[c for c in training_df.columns if c not in ['store_id', 'dept_id', 'event_timestamp']]}\")\n",
        "print()\n",
        "training_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Drop rows with missing target\n",
        "training_df = training_df.dropna(subset=[\"weekly_sales\"])\n",
        "print(f\"Rows after dropping NaN: {len(training_df):,}\")\n",
        "\n",
        "# Temporal split (80/20)\n",
        "training_df = training_df.sort_values(\"event_timestamp\")\n",
        "split_idx = int(len(training_df) * 0.8)\n",
        "train_df = training_df.iloc[:split_idx]\n",
        "val_df = training_df.iloc[split_idx:]\n",
        "\n",
        "print(f\"Train: {len(train_df):,} rows ({train_df['event_timestamp'].min().date()} to {train_df['event_timestamp'].max().date()})\")\n",
        "print(f\"Val: {len(val_df):,} rows ({val_df['event_timestamp'].min().date()} to {val_df['event_timestamp'].max().date()})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Feature columns (exclude identifiers and target)\n",
        "exclude_cols = [\"store_id\", \"dept_id\", \"date\", \"event_timestamp\", \"weekly_sales\"]\n",
        "feature_cols = [\n",
        "    c for c in training_df.columns \n",
        "    if c not in exclude_cols\n",
        "    and training_df[c].dtype in [np.float64, np.int64, np.float32, np.int32]\n",
        "]\n",
        "\n",
        "print(f\"Feature columns ({len(feature_cols)}): {feature_cols}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare arrays\n",
        "X_train = train_df[feature_cols].fillna(0).values\n",
        "y_train = train_df[\"weekly_sales\"].values\n",
        "X_val = val_df[feature_cols].fillna(0).values\n",
        "y_val = val_df[\"weekly_sales\"].values\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Log transform + scale target (better for % error optimization)\n",
        "y_scaler = StandardScaler()\n",
        "y_train_log = np.log1p(y_train)\n",
        "y_val_log = np.log1p(y_val)\n",
        "y_train_scaled = y_scaler.fit_transform(y_train_log.reshape(-1, 1)).flatten()\n",
        "y_val_scaled = y_scaler.transform(y_val_log.reshape(-1, 1)).flatten()\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train_scaled.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save scalers\n",
        "joblib.dump(scaler, OUTPUT_DIR / \"scaler.pkl\")\n",
        "joblib.dump(y_scaler, OUTPUT_DIR / \"y_scaler.pkl\")\n",
        "joblib.dump(feature_cols, OUTPUT_DIR / \"feature_cols.pkl\")\n",
        "joblib.dump({\n",
        "    \"scaler_X\": scaler,\n",
        "    \"scaler_y\": y_scaler,\n",
        "    \"use_log_transform\": True\n",
        "}, OUTPUT_DIR / \"scalers.joblib\")\n",
        "print(f\"âœ… Scalers saved to {OUTPUT_DIR}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class SalesMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout=0.2):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, dim),\n",
        "                nn.BatchNorm1d(dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ])\n",
        "            prev_dim = dim\n",
        "        layers.append(nn.Linear(prev_dim, 1))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(-1)\n",
        "\n",
        "\n",
        "class SalesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "    def __len__(self): \n",
        "        return len(self.X)\n",
        "    def __getitem__(self, i): \n",
        "        return self.X[i], self.y[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create dataloaders\n",
        "train_dataset = SalesDataset(X_train, y_train_scaled)\n",
        "val_dataset = SalesDataset(X_val, y_val_scaled)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize model\n",
        "input_dim = X_train.shape[1]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = SalesMLP(input_dim).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Model params: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training loop\n",
        "best_val_loss = float('inf')\n",
        "history = []\n",
        "\n",
        "print(f\"\\nðŸš€ Training for {NUM_EPOCHS} epochs...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Train\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(X_batch)\n",
        "        loss = criterion(pred, y_batch)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    train_loss /= len(train_loader)\n",
        "    \n",
        "    # Validate\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_preds = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            pred = model(X_batch)\n",
        "            val_loss += criterion(pred, y_batch).item()\n",
        "            val_preds.extend(pred.cpu().numpy())\n",
        "    val_loss /= len(val_loader)\n",
        "    \n",
        "    # Calculate MAPE\n",
        "    val_preds = np.array(val_preds)\n",
        "    pred_orig = np.expm1(y_scaler.inverse_transform(val_preds.reshape(-1, 1)).flatten())\n",
        "    mask = y_val > 1000\n",
        "    mape = np.mean(np.abs((y_val[mask] - pred_orig[mask]) / y_val[mask])) * 100\n",
        "    \n",
        "    scheduler.step()\n",
        "    history.append({\"epoch\": epoch + 1, \"train_loss\": train_loss, \"val_loss\": val_loss, \"mape\": mape})\n",
        "    \n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), OUTPUT_DIR / \"model_best.pt\")\n",
        "    \n",
        "    # Print progress\n",
        "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "        print(f\"Epoch {epoch+1:3d}/{NUM_EPOCHS} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | MAPE: {mape:.1f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save final model\n",
        "torch.save(model.state_dict(), OUTPUT_DIR / \"model_final.pt\")\n",
        "\n",
        "# Save model metadata\n",
        "best_mape = min(h[\"mape\"] for h in history)\n",
        "model_info = {\n",
        "    \"model_type\": \"SalesMLP\",\n",
        "    \"input_dim\": input_dim,\n",
        "    \"hidden_dims\": [256, 128, 64],\n",
        "    \"dropout\": 0.2,\n",
        "    \"best_val_loss\": best_val_loss,\n",
        "    \"best_mape\": best_mape,\n",
        "    \"epochs\": NUM_EPOCHS,\n",
        "    \"feature_columns\": feature_cols,\n",
        "    \"timestamp\": datetime.now(timezone.utc).isoformat()\n",
        "}\n",
        "\n",
        "with open(OUTPUT_DIR / \"model_metadata.json\", \"w\") as f:\n",
        "    json.dump(model_info, f, indent=2)\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(f\"âœ… TRAINING COMPLETE\")\n",
        "print(f\"=\" * 60)\n",
        "print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
        "print(f\"Best MAPE: {best_mape:.1f}%\")\n",
        "print(f\"\\nArtifacts saved to: {OUTPUT_DIR}\")\n",
        "print(f\"  - model_best.pt\")\n",
        "print(f\"  - model_final.pt\")\n",
        "print(f\"  - scalers.joblib\")\n",
        "print(f\"  - model_metadata.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training History"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history_df = pd.DataFrame(history)\n",
        "history_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ… Complete!\n",
        "\n",
        "### Artifacts\n",
        "\n",
        "| File | Purpose |\n",
        "|------|----------|\n",
        "| `model_best.pt` | Best model weights |\n",
        "| `model_final.pt` | Final model weights |\n",
        "| `scalers.joblib` | Feature and target scalers |\n",
        "| `model_metadata.json` | Architecture info, feature columns |\n",
        "\n",
        "### Alternative: Use TrainJob Manifest\n",
        "\n",
        "For production distributed training:\n",
        "\n",
        "```bash\n",
        "kubectl apply -f manifests/06-trainjob.yaml\n",
        "kubectl logs -f -l app.kubernetes.io/name=sales-forecasting -n feast-trainer-demo\n",
        "```\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- `03-inference.ipynb` â†’ Deploy model with KServe"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}