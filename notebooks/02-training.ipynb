{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - Distributed Training with Kubeflow & Feast\n",
        "\n",
        "![Workflow](../docs/02-training-workflow.png)\n",
        "\n",
        "## What This Notebook Does\n",
        "\n",
        "| Step | Component | Action |\n",
        "|------|-----------|--------|\n",
        "| 1 | Kubeflow SDK | Submit TrainJob to cluster |\n",
        "| 2 | Feast + Ray | Distributed feature retrieval (65K rows) |\n",
        "| 3 | PyTorch DDP | Multi-GPU distributed training |\n",
        "| 4 | MLflow | Track experiments, log model |\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502  Kubeflow   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   TrainJob      \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   MLflow    \u2502\n",
        "\u2502    SDK      \u2502     \u2502  (2 GPU nodes)  \u2502     \u2502  Tracking   \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "                             \u2502\n",
        "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "                    \u2502   Feast + Ray   \u2502\n",
        "                    \u2502  (distributed   \u2502\n",
        "                    \u2502   PIT join)     \u2502\n",
        "                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "```\n",
        "\n",
        "**Prerequisites:** `01-feast-features.ipynb` completed, MLflow & RayCluster running."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run this cell first, then restart kernel if needed\n",
        "%pip install -q kubeflow kubernetes \"mlflow>=3.0\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip show kubeflow"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "| Parameter | Value | Purpose |\n",
        "|-----------|-------|----------|\n",
        "| `NAMESPACE` | `feast-trainer-demo` | K8s namespace |\n",
        "| `PVC` | `shared` | Persistent storage for model artifacts |\n",
        "| `RUNTIME` | `torch-distributed` | ClusterTrainingRuntime for DDP |\n",
        "| `USE_RAY` | `True` | Enable Ray for distributed feature retrieval |\n",
        "| `EPOCHS` | 50 | Training iterations |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "NAMESPACE = \"feast-trainer-demo\"\n",
        "PVC = \"shared\"\n",
        "RUNTIME = \"torch-distributed\"\n",
        "MLFLOW_URI = f\"http://mlflow.{NAMESPACE}.svc.cluster.local:5000\"\n",
        "EPOCHS = 50\n",
        "USE_RAY = True\n",
        "RAY_CLUSTER = \"feast-ray\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kubernetes Authentication\n",
        "\n",
        "Connect to the cluster using service account token:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "K8S_TOKEN = os.getenv(\"K8S_TOKEN\", \"sha256~JcXJKSaqqrzvpE0OIAeOLhDkKofm9kQqRLOjkgHJMsY\")\n",
        "K8S_API = os.getenv(\"K8S_API_SERVER\", \"https://api.oai-kft-ibm.ibm.rh-ods.com:6443\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Kubeflow TrainerClient\n",
        "\n",
        "The `TrainerClient` manages TrainJob lifecycle:\n",
        "- `train()` \u2192 Submit job\n",
        "- `get_job()` \u2192 Check status\n",
        "- `wait_for_job_status()` \u2192 Block until complete\n",
        "- `delete_job()` \u2192 Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from kubernetes import client as k8s\n",
        "from kubeflow.trainer import TrainerClient, CustomTrainer\n",
        "from kubeflow.common.types import KubernetesBackendConfig\n",
        "from kubeflow.trainer.options import Labels, PodTemplateOverrides, PodTemplateOverride, PodSpecOverride, ContainerOverride\n",
        "\n",
        "cfg = k8s.Configuration()\n",
        "if K8S_TOKEN:\n",
        "    cfg.host = K8S_API\n",
        "    cfg.verify_ssl = False\n",
        "    cfg.api_key = {\"authorization\": f\"Bearer {K8S_TOKEN}\"}\n",
        "\n",
        "trainer = TrainerClient(KubernetesBackendConfig(namespace=NAMESPACE, client_configuration=cfg if K8S_TOKEN else None))\n",
        "runtime = trainer.get_runtime(RUNTIME)\n",
        "print(f\"Runtime: {RUNTIME}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Function\n",
        "\n",
        "This function runs **inside the TrainJob pods**. Key components:\n",
        "\n",
        "### 1. Feast Feature Retrieval (Rank 0 only)\n",
        "```python\n",
        "entity_df = DataFrame([{store_id, dept_id, event_timestamp} \u00d7 65K])\n",
        "df = store.get_historical_features(entity_df, 'training_features').to_df()\n",
        "```\n",
        "Uses Ray cluster for distributed point-in-time joins.\n",
        "\n",
        "### 2. PyTorch DDP Training (All ranks)\n",
        "```python\n",
        "model = DDP(MLP(...).to(device))  # Wrap model for distributed\n",
        "sampler = DistributedSampler(...)  # Shard data across GPUs\n",
        "```\n",
        "\n",
        "### 3. MLflow Logging (Rank 0 only)\n",
        "```python\n",
        "mlflow.log_metrics({'mape': mape, 'loss': loss})\n",
        "mlflow.pytorch.log_model(model, 'model')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_fn(epochs=50, use_ray=True, ray_cluster='feast-ray', namespace='feast-trainer-demo', output_dir='/shared/models', feature_repo='/shared/feature_repo'):\n",
        "    \"\"\"Training function - SDK passes func_args as **kwargs\"\"\"\n",
        "    import os, json, numpy as np, pandas as pd, torch, torch.nn as nn, torch.distributed as dist, joblib, mlflow, shutil, time\n",
        "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "    from torch.utils.data import DataLoader, Dataset, DistributedSampler\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from datetime import datetime, timezone, timedelta\n",
        "    \n",
        "    dist.init_process_group(backend='nccl' if torch.cuda.is_available() else 'gloo')\n",
        "    rank, world = dist.get_rank(), dist.get_world_size()\n",
        "    device = torch.device(f\"cuda:{int(os.environ.get('LOCAL_RANK', 0))}\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"[Rank {rank}] Device: {device}\")\n",
        "    \n",
        "    OUT = output_dir\n",
        "    REPO = feature_repo\n",
        "    EPOCHS = epochs\n",
        "    \n",
        "    class MLP(nn.Module):\n",
        "        def __init__(self, inp, hidden=[512, 256, 128, 64], drop=0.3):\n",
        "            super().__init__()\n",
        "            layers = []\n",
        "            for h in hidden:\n",
        "                layers.extend([nn.Linear(inp, h), nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(drop)])\n",
        "                inp = h\n",
        "            layers.append(nn.Linear(inp, 1))\n",
        "            self.net = nn.Sequential(*layers)\n",
        "        def forward(self, x):\n",
        "            return self.net(x).squeeze(-1)\n",
        "    \n",
        "    class DS(Dataset):\n",
        "        def __init__(self, X, y):\n",
        "            self.X = torch.tensor(X, dtype=torch.float32)\n",
        "            self.y = torch.tensor(y, dtype=torch.float32)\n",
        "        def __len__(self): return len(self.X)\n",
        "        def __getitem__(self, i): return self.X[i], self.y[i]\n",
        "    \n",
        "    if rank == 0:\n",
        "        os.makedirs(OUT, exist_ok=True)\n",
        "        mlflow.set_tracking_uri(os.getenv('MLFLOW_TRACKING_URI', 'http://mlflow:5000'))\n",
        "        mlflow.set_experiment('sales-forecasting')\n",
        "        mlflow.start_run(run_name=f\"train-{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
        "        \n",
        "        # Use Ray config for Feast\n",
        "        if use_ray:\n",
        "            os.environ['FEAST_RAY_USE_KUBERAY'] = 'true'\n",
        "            os.environ['FEAST_RAY_CLUSTER_NAME'] = ray_cluster\n",
        "            os.environ['FEAST_RAY_NAMESPACE'] = namespace\n",
        "            os.environ['FEAST_RAY_SKIP_TLS'] = 'true'\n",
        "            token_path = '/var/run/secrets/kubernetes.io/serviceaccount/token'\n",
        "            if os.path.exists(token_path):\n",
        "                with open(token_path) as f:\n",
        "                    os.environ['FEAST_RAY_AUTH_TOKEN'] = f.read()\n",
        "                os.environ['FEAST_RAY_AUTH_SERVER'] = f\"https://{os.environ.get('KUBERNETES_SERVICE_HOST')}:{os.environ.get('KUBERNETES_SERVICE_PORT')}\"\n",
        "            ray_cfg = f\"{REPO}/feature_store_ray.yaml\"\n",
        "            if os.path.exists(ray_cfg):\n",
        "                shutil.copy(ray_cfg, f\"{REPO}/feature_store.yaml\")\n",
        "        \n",
        "        from feast import FeatureStore\n",
        "        store = FeatureStore(repo_path=REPO)\n",
        "        \n",
        "        # Entity DF\n",
        "        entity_rows = [{'store_id': s, 'dept_id': d, 'event_timestamp': datetime(2022,1,1,tzinfo=timezone.utc) + timedelta(weeks=w)}\n",
        "            for w in range(104) for s in range(1, 46) for d in range(1, 15)]\n",
        "        entity_df = pd.DataFrame(entity_rows)\n",
        "        print(f\"Entity DF: {len(entity_df):,} rows\")\n",
        "        \n",
        "        t0 = time.time()\n",
        "        df = store.get_historical_features(entity_df=entity_df, features=store.get_feature_service('training_features')).to_df()\n",
        "        print(f\"\u2705 Feast: {len(df):,} rows in {time.time()-t0:.1f}s\")\n",
        "        \n",
        "        df = df.dropna(subset=['weekly_sales']).sort_values('event_timestamp')\n",
        "        split = int(len(df) * 0.8)\n",
        "        train_df, val_df = df.iloc[:split], df.iloc[split:]\n",
        "        \n",
        "        exclude = ['store_id', 'dept_id', 'event_timestamp', 'date', 'weekly_sales']\n",
        "        feat_cols = [c for c in df.columns if c not in exclude and df[c].dtype in [np.float64, np.int64, np.float32, np.int32]]\n",
        "        print(f\"Features ({len(feat_cols)}): {feat_cols[:5]}...\")\n",
        "        \n",
        "        X_train, y_train = train_df[feat_cols].fillna(0).values, train_df['weekly_sales'].values\n",
        "        X_val, y_val = val_df[feat_cols].fillna(0).values, val_df['weekly_sales'].values\n",
        "        \n",
        "        scaler, y_scaler = StandardScaler(), StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_val = scaler.transform(X_val)\n",
        "        y_train_s = y_scaler.fit_transform(np.log1p(y_train).reshape(-1,1)).flatten()\n",
        "        y_val_s = y_scaler.transform(np.log1p(y_val).reshape(-1,1)).flatten()\n",
        "        \n",
        "        joblib.dump({'scaler_X': scaler, 'scaler_y': y_scaler, 'use_log_transform': True}, f'{OUT}/scalers.joblib')\n",
        "        joblib.dump(feat_cols, f'{OUT}/feature_cols.pkl')\n",
        "        np.savez(f'{OUT}/.data.npz', X_train=X_train, y_train=y_train_s, X_val=X_val, y_val=y_val_s, y_val_orig=y_val)\n",
        "        np.save(f'{OUT}/.dim.npy', [X_train.shape[1]])\n",
        "        mlflow.log_params({'epochs': EPOCHS, 'train_rows': len(train_df), 'val_rows': len(val_df), 'features': len(feat_cols)})\n",
        "    \n",
        "    dist.barrier()\n",
        "    data = np.load(f'{OUT}/.data.npz')\n",
        "    X_train, y_train, X_val, y_val, y_val_orig = data['X_train'], data['y_train'], data['X_val'], data['y_val'], data['y_val_orig']\n",
        "    inp_dim = int(np.load(f'{OUT}/.dim.npy')[0])\n",
        "    dist.barrier()\n",
        "    \n",
        "    train_ds = DS(X_train, y_train)\n",
        "    val_ds = DS(X_val, y_val)\n",
        "    sampler = DistributedSampler(train_ds, num_replicas=world, rank=rank)\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, sampler=sampler)\n",
        "    val_loader = DataLoader(val_ds, batch_size=64)\n",
        "    \n",
        "    model = DDP(MLP(inp_dim).to(device), device_ids=[int(os.environ.get('LOCAL_RANK',0))] if torch.cuda.is_available() else None)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-3)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=3)\n",
        "    crit = nn.MSELoss()\n",
        "    \n",
        "    best_loss, best_mape = float('inf'), float('inf')\n",
        "    for ep in range(EPOCHS):\n",
        "        sampler.set_epoch(ep)\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for X_b, y_b in train_loader:\n",
        "            X_b, y_b = X_b.to(device), y_b.to(device)\n",
        "            opt.zero_grad()\n",
        "            loss = crit(model(X_b), y_b)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(train_loader)\n",
        "        \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            preds = np.concatenate([model(X.to(device)).cpu().numpy() for X, _ in val_loader])\n",
        "        val_loss = np.mean((preds - y_val)**2)\n",
        "        sched.step(val_loss)\n",
        "        \n",
        "        if rank == 0:\n",
        "            y_sc = joblib.load(f'{OUT}/scalers.joblib')['scaler_y']\n",
        "            pred_orig = np.expm1(y_sc.inverse_transform(preds.reshape(-1,1)).flatten())\n",
        "            mask = y_val_orig > 1000\n",
        "            mape = np.mean(np.abs((y_val_orig[mask] - pred_orig[mask]) / y_val_orig[mask])) * 100\n",
        "            mlflow.log_metrics({'train_loss': train_loss, 'val_loss': val_loss, 'mape': mape, 'lr': opt.param_groups[0]['lr']}, step=ep)\n",
        "            if (ep + 1) % 10 == 0:\n",
        "                print(f\"Ep {ep+1}/{EPOCHS} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | MAPE: {mape:.1f}%\")\n",
        "            if val_loss < best_loss:\n",
        "                best_loss, best_mape = val_loss, mape\n",
        "                torch.save(model.module.state_dict(), f'{OUT}/best_model.pt')\n",
        "        dist.barrier()\n",
        "    \n",
        "    if rank == 0:\n",
        "        print(f\"\\n\u2705 DONE: MAPE {best_mape:.1f}%\")\n",
        "        mlflow.log_metrics({'best_mape': best_mape, 'best_val_loss': best_loss})\n",
        "        m = MLP(inp_dim)\n",
        "        m.load_state_dict(torch.load(f'{OUT}/best_model.pt'))\n",
        "        m.eval()\n",
        "        mlflow.pytorch.log_model(m, 'model')\n",
        "        feat_cols = joblib.load(f'{OUT}/feature_cols.pkl')\n",
        "        json.dump({'model_type': 'SalesMLP', 'input_dim': inp_dim, 'hidden_dims': [512,256,128,64], 'dropout': 0.3, 'best_mape': float(best_mape), 'feature_columns': feat_cols}, open(f'{OUT}/model_metadata.json', 'w'))\n",
        "        mlflow.end_run()\n",
        "        for f in ['.data.npz', '.dim.npy']:\n",
        "            try: os.remove(f'{OUT}/{f}')\n",
        "            except: pass\n",
        "    dist.destroy_process_group()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submit TrainJob\n",
        "\n",
        "The `CustomTrainer` packages the function for distributed execution:\n",
        "\n",
        "| Parameter | Value | Purpose |\n",
        "|-----------|-------|----------|\n",
        "| `func` | `train_fn` | Python function to run |\n",
        "| `func_args` | `{epochs, use_ray, ...}` | Arguments passed as kwargs |\n",
        "| `num_nodes` | 1 | Number of worker pods |\n",
        "| `resources_per_node` | `{gpu:1, cpu:4}` | Resources per pod |\n",
        "| `packages_to_install` | `[feast, mlflow, ...]` | Pip packages |\n",
        "\n",
        "**Environment Variables:**\n",
        "- `RDZV_TIMEOUT=1800` \u2192 DDP rendezvous timeout (30 min for Feast retrieval)\n",
        "- `FEAST_DATA_ROOT=/shared/data` \u2192 Path to parquet files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datetime import datetime\n",
        "from kubeflow.trainer.options import Name, Labels, PodTemplateOverrides, PodTemplateOverride, PodSpecOverride, ContainerOverride\n",
        "\n",
        "job_id = datetime.now().strftime('%m%d-%H%M')\n",
        "func_args = {'epochs': EPOCHS, 'use_ray': USE_RAY, 'ray_cluster': RAY_CLUSTER, 'namespace': NAMESPACE, 'output_dir': '/shared/models', 'feature_repo': '/shared/feature_repo'}\n",
        "\n",
        "job = trainer.train(\n",
        "    trainer=CustomTrainer(\n",
        "        func=train_fn,\n",
        "        func_args=func_args,  # Pass args here, not to train()\n",
        "        num_nodes=1,\n",
        "        resources_per_node={'gpu': 1, 'cpu': 4, 'memory': '8Gi'},  # Use 'gpu' not 'nvidia.com/gpu'\n",
        "        packages_to_install=['feast[ray,postgres]==0.59.0', 'codeflare-sdk', 'psycopg2-binary', 'scikit-learn', 'pandas', 'pyarrow', 'joblib', 'mlflow>=3.0'],\n",
        "        env={\n",
        "            'MLFLOW_TRACKING_URI': MLFLOW_URI,\n",
        "            'FEAST_DATA_ROOT': '/shared/data',  # Path to parquet files\n",
        "            'RDZV_TIMEOUT': '1800',  # 30 min for DDP rendezvous (feature retrieval takes time)\n",
        "            'NCCL_IB_DISABLE': '1'   # Disable InfiniBand if not available\n",
        "        }\n",
        "    ),\n",
        "    runtime=runtime,\n",
        "    options=[\n",
        "        Name(\"sales-forecast\"),\n",
        "        Labels({'app': 'sales-forecasting'}),\n",
        "        PodTemplateOverrides(PodTemplateOverride(\n",
        "            target_jobs=['node'],\n",
        "            spec=PodSpecOverride(\n",
        "                volumes=[{'name': 'shared', 'persistentVolumeClaim': {'claimName': PVC}}],\n",
        "                containers=[ContainerOverride(name='node', volume_mounts=[{'name': 'shared', 'mountPath': '/shared'}])]\n",
        "            )\n",
        "        ))\n",
        "    ]\n",
        ")\n",
        "print(f\"\u2705 Submitted: {job}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# trainer.delete_job(job)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Monitor Training\n",
        "\n",
        "Wait for job completion (max 1 hour):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trainer.wait_for_job_status(name=job, status={'Complete', 'Failed'}, timeout=3600)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View MLflow Results\n",
        "\n",
        "Check training metrics and artifacts:\n",
        "\n",
        "| Metric | Meaning |\n",
        "|--------|----------|\n",
        "| `best_mape` | Mean Absolute Percentage Error (lower is better) |\n",
        "| `train_loss` | MSE on training set |\n",
        "| `val_loss` | MSE on validation set |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import mlflow\n",
        "mlflow.set_tracking_uri(MLFLOW_URI)\n",
        "exp = mlflow.get_experiment_by_name('sales-forecasting')\n",
        "if exp:\n",
        "    runs = mlflow.search_runs([exp.experiment_id], max_results=5, order_by=['start_time DESC'])\n",
        "    display(runs[['tags.mlflow.runName', 'metrics.best_mape']])\n",
        "else:\n",
        "    print(\"Experiment not found yet\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## \u2705 Training Complete!\n",
        "\n",
        "**Artifacts saved to `/shared/models/`:**\n",
        "- `best_model.pt` \u2192 PyTorch model weights\n",
        "- `scalers.joblib` \u2192 Feature scalers\n",
        "- `feature_cols.pkl` \u2192 Feature column names\n",
        "- `model_metadata.json` \u2192 Architecture info\n",
        "\n",
        "**Next:** `03-inference.ipynb` \u2192 Deploy model with KServe"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}