{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 - Model Serving with KServe\n",
        "\n",
        "![Workflow](../docs/03-inference-workflow.png)\n",
        "\n",
        "## What This Notebook Does\n",
        "\n",
        "| Step | Action | Component |\n",
        "|------|--------|------------|\n",
        "| 1 | Create serving script | ConfigMap |\n",
        "| 2 | Deploy InferenceService | KServe |\n",
        "| 3 | Make predictions | REST API (V2 protocol) |\n",
        "\n",
        "## Inference Flow\n",
        "\n",
        "```\n",
        "Client                 KServe                Feast Server\n",
        "  â”‚                      â”‚                        â”‚\n",
        "  â”‚ {store_id, dept_id}  â”‚                        â”‚\n",
        "  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                        â”‚\n",
        "  â”‚                      â”‚  get-online-features   â”‚\n",
        "  â”‚                      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚\n",
        "  â”‚                      â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n",
        "  â”‚                      â”‚  model.predict()       â”‚\n",
        "  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                        â”‚\n",
        "  â”‚  prediction: $96,763 â”‚                        â”‚\n",
        "```\n",
        "\n",
        "**Key:** Client sends entity IDs only; KServe fetches features from Feast.\n",
        "\n",
        "**Prerequisites:** `01-feast-features.ipynb` and `02-training.ipynb` completed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install -q kserve pandas requests kubernetes"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from kserve import KServeClient, V1beta1InferenceService, V1beta1InferenceServiceSpec, V1beta1PredictorSpec\n",
        "from kubernetes import client as k8s_client, config\n",
        "from kubernetes.client import V1Container, V1ResourceRequirements, V1VolumeMount, V1Volume\n",
        "from kubernetes.client import V1PersistentVolumeClaimVolumeSource, V1ConfigMapVolumeSource\n",
        "from kubernetes.client import V1EnvVar, V1Probe, V1HTTPGetAction, V1ConfigMap, V1ObjectMeta\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "NAMESPACE = \"feast-trainer-demo\"\n",
        "MODEL_NAME = \"sales-forecast\"\n",
        "MODEL_DIR = \"/shared/models\"\n",
        "PVC_NAME = \"shared\"\n",
        "CONFIGMAP_NAME = \"sales-forecast-serve\"\n",
        "\n",
        "# Trainer image (has PyTorch pre-installed, matches Ray cluster Python version)\n",
        "TRAINER_IMAGE = \"quay.io/modh/training:py312-cuda128-torch280\"\n",
        "\n",
        "# Feast Feature Server\n",
        "FEAST_SERVER_URL = f\"http://feast-server.{NAMESPACE}.svc.cluster.local:6566\"\n",
        "\n",
        "print(f\"Namespace: {NAMESPACE}\")\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Image: {TRAINER_IMAGE}\")\n",
        "print(f\"PVC: {PVC_NAME}\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Create Serving Script\n",
        "\n",
        "The serving script implements KServe's `Model` interface:\n",
        "\n",
        "| Method | Purpose |\n",
        "|--------|----------|\n",
        "| `load()` | Load model, scalers, feature columns |\n",
        "| `preprocess()` | Extract entities â†’ Call Feast â†’ Build feature matrix |\n",
        "| `predict()` | Scale features â†’ Model inference â†’ Inverse transform |\n",
        "| `postprocess()` | Format as V2 InferResponse |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "SERVE_SCRIPT = '''#!/usr/bin/env python3\n",
        "\"\"\"Sales Forecasting Inference Server (KServe V2 Protocol)\"\"\"\n",
        "import os, json, requests, torch, torch.nn as nn, joblib, numpy as np, logging\n",
        "from typing import Dict, Union\n",
        "from kserve import Model, ModelServer, InferRequest, InferResponse, InferOutput\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "FEAST_SERVER_URL = os.getenv(\"FEAST_SERVER_URL\", \"http://feast-server.feast-trainer-demo.svc.cluster.local:6566\")\n",
        "\n",
        "class SalesMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout=0.2):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for h_dim in hidden_dims:\n",
        "            layers.extend([nn.Linear(prev_dim, h_dim), nn.BatchNorm1d(h_dim), nn.ReLU(), nn.Dropout(dropout)])\n",
        "            prev_dim = h_dim\n",
        "        layers.append(nn.Linear(prev_dim, 1))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(-1)\n",
        "\n",
        "class SalesForecastModel(Model):\n",
        "    def __init__(self, name: str):\n",
        "        super().__init__(name)\n",
        "        self.model = None\n",
        "        self.scalers = None\n",
        "        self.feature_cols = None\n",
        "        self.use_log_transform = False\n",
        "        self.ready = False\n",
        "    \n",
        "    def load(self):\n",
        "        model_dir = os.getenv(\"MODEL_DIR\", \"/shared/models\")\n",
        "        logger.info(f\"Loading model from {model_dir}...\")\n",
        "        with open(f\"{model_dir}/model_metadata.json\") as f:\n",
        "            metadata = json.load(f)\n",
        "        hidden_dims = metadata.get(\"hidden_dims\", [256, 128, 64])\n",
        "        dropout = metadata.get(\"dropout\", 0.2)\n",
        "        input_dim = metadata[\"input_dim\"]\n",
        "        self.model = SalesMLP(input_dim, hidden_dims, dropout)\n",
        "        self.model.load_state_dict(torch.load(f\"{model_dir}/model_best.pt\", map_location=\"cpu\", weights_only=True))\n",
        "        self.model.eval()\n",
        "        self.scalers = joblib.load(f\"{model_dir}/scalers.joblib\")\n",
        "        self.feature_cols = metadata[\"feature_columns\"]\n",
        "        self.use_log_transform = self.scalers.get(\"use_log_transform\", False)\n",
        "        logger.info(f\"Model loaded: {len(self.feature_cols)} features\")\n",
        "        self.ready = True\n",
        "    \n",
        "    def _get_features_from_feast(self, entity_rows):\n",
        "        payload = {\n",
        "            \"feature_service\": \"inference_features\",\n",
        "            \"entities\": {\n",
        "                \"store_id\": [row[\"store_id\"] for row in entity_rows],\n",
        "                \"dept_id\": [row[\"dept_id\"] for row in entity_rows],\n",
        "            }\n",
        "        }\n",
        "        response = requests.post(f\"{FEAST_SERVER_URL}/get-online-features\", json=payload, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        feature_names = result.get(\"metadata\", {}).get(\"feature_names\", [])\n",
        "        results = result.get(\"results\", [])\n",
        "        features = []\n",
        "        for i in range(len(entity_rows)):\n",
        "            row = {}\n",
        "            for j, name in enumerate(feature_names):\n",
        "                if name in self.feature_cols and j < len(results):\n",
        "                    val = results[j].get(\"values\", [None])[i] if i < len(results[j].get(\"values\", [])) else None\n",
        "                    row[name] = val if val is not None else 0\n",
        "            features.append(row)\n",
        "        return features\n",
        "    \n",
        "    def preprocess(self, payload: Union[Dict, InferRequest], headers: Dict = None) -> np.ndarray:\n",
        "        if isinstance(payload, InferRequest):\n",
        "            inputs = {inp.name: inp.data for inp in payload.inputs}\n",
        "        else:\n",
        "            inputs = {inp[\"name\"]: inp.get(\"data\") for inp in payload.get(\"inputs\", [])}\n",
        "        if \"entities\" in inputs:\n",
        "            entities = inputs[\"entities\"]\n",
        "            if isinstance(entities, list) and len(entities) > 0:\n",
        "                feature_rows = self._get_features_from_feast(entities)\n",
        "                X = np.array([[row.get(c, 0) for c in self.feature_cols] for row in feature_rows], dtype=np.float32)\n",
        "                return X\n",
        "        if \"features\" in inputs:\n",
        "            return np.array(inputs[\"features\"], dtype=np.float32)\n",
        "        raise ValueError(\"Input must have \\'entities\\' or \\'features\\'\")\n",
        "    \n",
        "    def predict(self, X: np.ndarray, headers: Dict = None) -> np.ndarray:\n",
        "        X_scaled = self.scalers[\"scaler_X\"].transform(X)\n",
        "        with torch.no_grad():\n",
        "            preds = self.model(torch.FloatTensor(X_scaled)).numpy()\n",
        "        predictions = self.scalers[\"scaler_y\"].inverse_transform(preds.reshape(-1, 1)).flatten()\n",
        "        if self.use_log_transform:\n",
        "            predictions = np.expm1(predictions)\n",
        "        return predictions\n",
        "    \n",
        "    def postprocess(self, predictions: np.ndarray, headers: Dict = None) -> Union[Dict, InferResponse]:\n",
        "        return InferResponse(\n",
        "            model_name=self.name,\n",
        "            infer_outputs=[InferOutput(name=\"predictions\", shape=list(predictions.shape), datatype=\"FP32\", data=predictions.tolist())]\n",
        "        )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = SalesForecastModel(\"sales-forecast\")\n",
        "    model.load()\n",
        "    ModelServer().start([model])\n",
        "'''"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load Kubernetes config\n",
        "try:\n",
        "    config.load_incluster_config()\n",
        "except:\n",
        "    config.load_kube_config()\n",
        "\n",
        "core_v1 = k8s_client.CoreV1Api()\n",
        "\n",
        "# Create ConfigMap\n",
        "configmap = V1ConfigMap(\n",
        "    metadata=V1ObjectMeta(name=CONFIGMAP_NAME, namespace=NAMESPACE, labels={\"app\": \"sales-forecasting\"}),\n",
        "    data={\"serve.py\": SERVE_SCRIPT}\n",
        ")\n",
        "\n",
        "try:\n",
        "    core_v1.read_namespaced_config_map(CONFIGMAP_NAME, NAMESPACE)\n",
        "    print(f\"âš ï¸ ConfigMap {CONFIGMAP_NAME} exists, replacing...\")\n",
        "    core_v1.replace_namespaced_config_map(CONFIGMAP_NAME, NAMESPACE, configmap)\n",
        "except:\n",
        "    print(f\"ğŸ“¦ Creating ConfigMap {CONFIGMAP_NAME}...\")\n",
        "    core_v1.create_namespaced_config_map(NAMESPACE, configmap)\n",
        "\n",
        "print(f\"âœ… ConfigMap created with serve.py script\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Deploy InferenceService"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "kserve_client = KServeClient()\n",
        "print(f\"âœ… KServeClient initialized\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build InferenceService spec\n",
        "isvc = V1beta1InferenceService(\n",
        "    api_version=\"serving.kserve.io/v1beta1\",\n",
        "    kind=\"InferenceService\",\n",
        "    metadata=k8s_client.V1ObjectMeta(\n",
        "        name=MODEL_NAME,\n",
        "        namespace=NAMESPACE,\n",
        "        labels={\"app\": \"sales-forecasting\"},\n",
        "        annotations={\"serving.kserve.io/deploymentMode\": \"RawDeployment\"}\n",
        "    ),\n",
        "    spec=V1beta1InferenceServiceSpec(\n",
        "        predictor=V1beta1PredictorSpec(\n",
        "            min_replicas=1,\n",
        "            max_replicas=3,\n",
        "            containers=[\n",
        "                V1Container(\n",
        "                    name=\"kserve-container\",\n",
        "                    image=TRAINER_IMAGE,\n",
        "                    command=[\"/bin/bash\", \"-c\"],\n",
        "                    args=[\"pip install -q kserve joblib numpy scikit-learn && python /scripts/serve.py\"],\n",
        "                    ports=[k8s_client.V1ContainerPort(container_port=8080, protocol=\"TCP\")],\n",
        "                    env=[\n",
        "                        V1EnvVar(name=\"MODEL_DIR\", value=MODEL_DIR),\n",
        "                        V1EnvVar(name=\"FEAST_SERVER_URL\", value=FEAST_SERVER_URL),\n",
        "                    ],\n",
        "                    resources=V1ResourceRequirements(\n",
        "                        requests={\"cpu\": \"500m\", \"memory\": \"1Gi\"},\n",
        "                        limits={\"cpu\": \"2\", \"memory\": \"4Gi\"}\n",
        "                    ),\n",
        "                    readiness_probe=V1Probe(\n",
        "                        http_get=V1HTTPGetAction(path=\"/v2/health/ready\", port=8080),\n",
        "                        initial_delay_seconds=60,\n",
        "                        period_seconds=10\n",
        "                    ),\n",
        "                    volume_mounts=[\n",
        "                        V1VolumeMount(name=\"scripts\", mount_path=\"/scripts\"),\n",
        "                        V1VolumeMount(name=\"model-storage\", mount_path=\"/shared\"),\n",
        "                    ]\n",
        "                )\n",
        "            ],\n",
        "            volumes=[\n",
        "                V1Volume(name=\"scripts\", config_map=V1ConfigMapVolumeSource(name=CONFIGMAP_NAME)),\n",
        "                V1Volume(name=\"model-storage\", persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(claim_name=PVC_NAME)),\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"âœ… InferenceService spec created\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Deploy InferenceService\n",
        "try:\n",
        "    existing = kserve_client.get(MODEL_NAME, namespace=NAMESPACE)\n",
        "    print(f\"âš ï¸ InferenceService '{MODEL_NAME}' exists, replacing...\")\n",
        "    kserve_client.replace(MODEL_NAME, isvc, namespace=NAMESPACE)\n",
        "except Exception:\n",
        "    print(f\"ğŸ“¦ Creating InferenceService '{MODEL_NAME}'...\")\n",
        "    kserve_client.create(isvc)\n",
        "\n",
        "print(f\"âœ… InferenceService submitted to namespace '{NAMESPACE}'\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Wait for ready\n",
        "print(\"â³ Waiting for InferenceService to be ready...\")\n",
        "kserve_client.wait_isvc_ready(MODEL_NAME, namespace=NAMESPACE, timeout_seconds=300)\n",
        "\n",
        "isvc_status = kserve_client.get(MODEL_NAME, namespace=NAMESPACE)\n",
        "url = isvc_status.get(\"status\", {}).get(\"url\", \"\")\n",
        "print(f\"âœ… InferenceService ready!\")\n",
        "print(f\"   URL: {url}\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Make Predictions (V2 Protocol)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ENDPOINT = f\"http://{MODEL_NAME}-predictor.{NAMESPACE}.svc.cluster.local:8080\"\n",
        "\n",
        "# Check health\n",
        "try:\n",
        "    resp = requests.get(f\"{ENDPOINT}/v2/health/ready\", timeout=5)\n",
        "    ready = resp.status_code == 200\n",
        "except:\n",
        "    ready = False\n",
        "    \n",
        "print(f\"âœ… Inference endpoint ready: {ready}\")\n",
        "print(f\"   Endpoint: {ENDPOINT}\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def predict_with_feast(entities: list) -> dict:\n",
        "    \"\"\"Make prediction using KServe V2 protocol with Feast feature lookup.\"\"\"\n",
        "    url = f\"{ENDPOINT}/v2/models/{MODEL_NAME}/infer\"\n",
        "    payload = {\n",
        "        \"inputs\": [\n",
        "            {\"name\": \"entities\", \"shape\": [len(entities)], \"datatype\": \"BYTES\", \"data\": entities}\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    t0 = time.time()\n",
        "    response = requests.post(url, json=payload, timeout=30)\n",
        "    latency = (time.time() - t0) * 1000\n",
        "    \n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Prediction failed: {response.text}\")\n",
        "    \n",
        "    result = response.json()\n",
        "    predictions = result.get(\"outputs\", [{}])[0].get(\"data\", [])\n",
        "    \n",
        "    return {\"predictions\": predictions, \"latency_ms\": latency, \"entities\": entities}"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Single prediction\n",
        "entity = {\"store_id\": 1, \"dept_id\": 3}\n",
        "try:\n",
        "    result = predict_with_feast([entity])\n",
        "    print(f\"âœ… Store {entity['store_id']}, Dept {entity['dept_id']}: ${result['predictions'][0]:,.0f}\")\n",
        "    print(f\"   Latency: {result['latency_ms']:.0f}ms\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Prediction error: {e}\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Batch predictions\n",
        "entities = [{\"store_id\": s, \"dept_id\": d} for s in [1, 10, 25, 45] for d in [1, 5, 10, 14]]\n",
        "print(f\"Scoring {len(entities)} entities...\")\n",
        "\n",
        "try:\n",
        "    result = predict_with_feast(entities)\n",
        "    results = pd.DataFrame([{**e, \"prediction\": p} for e, p in zip(entities, result[\"predictions\"])])\n",
        "    print(f\"\\nâœ… {len(results)} predictions in {result['latency_ms']:.0f}ms\")\n",
        "    display(results)\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Batch prediction error: {e}\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ… Pipeline Complete!\n",
        "\n",
        "### End-to-End Summary\n",
        "\n",
        "| Notebook | Component | Output |\n",
        "|----------|-----------|--------|\n",
        "| 01 | Feast | Features registered, online store populated |\n",
        "| 02 | Training | Model trained with `get_historical_features()` |\n",
        "| 03 | KServe | Model serving with Feast integration |\n",
        "\n",
        "### Key Benefits\n",
        "\n",
        "| Benefit | How |\n",
        "|---------|-----|\n",
        "| **No feature skew** | Same FeatureService for train & serve |\n",
        "| **Scalable** | Ray distributes training, KServe scales serving |\n",
        "| **Low latency** | Feast online store (<50ms) |\n",
        "| **Simple client** | Send entity IDs, get predictions |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}