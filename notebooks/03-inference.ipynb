{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Model Serving with KServe\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook deploys the trained model as a **KServe InferenceService** for real-time predictions. The key feature is **Feast integration** - clients send only entity IDs, and the server fetches features from Feast's online store.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "| Step | Action | Component |\n",
    "|------|--------|-----------|\n",
    "| 1 | Create serving script | ConfigMap with KServe Model class |\n",
    "| 2 | Deploy InferenceService | KServe with Feast integration |\n",
    "| 3 | Make predictions | REST API (V2 protocol) |\n",
    "\n",
    "### Inference Flow\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        INFERENCE DATA FLOW                                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                             â”‚\n",
    "â”‚  Client                    KServe                   Feast Online Store      â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n",
    "â”‚  â”‚ {store_id: 1,   â”‚      â”‚                 â”‚      â”‚                 â”‚      â”‚\n",
    "â”‚  â”‚  dept_id: 3}    â”‚â”€â”€â”€â”€â”€â–¶â”‚  1. Receive     â”‚â”€â”€â”€â”€â”€â–¶â”‚  2. Fetch       â”‚      â”‚\n",
    "â”‚  â”‚                 â”‚      â”‚     request     â”‚      â”‚     features    â”‚      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚                 â”‚â—€â”€â”€â”€â”€â”€â”‚     (<50ms)     â”‚      â”‚\n",
    "â”‚                           â”‚  3. Scale       â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n",
    "â”‚                           â”‚     features    â”‚                               â”‚\n",
    "â”‚                           â”‚                 â”‚                               â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚  4. Model       â”‚                               â”‚\n",
    "â”‚  â”‚                 â”‚â—€â”€â”€â”€â”€â”€â”‚     inference   â”‚                               â”‚\n",
    "â”‚  â”‚ {prediction:    â”‚      â”‚                 â”‚                               â”‚\n",
    "â”‚  â”‚  $96,763}       â”‚      â”‚  5. Inverse     â”‚                               â”‚\n",
    "â”‚  â”‚                 â”‚      â”‚     transform   â”‚                               â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Why Feast Integration?\n",
    "\n",
    "| Approach | Client Sends | Pros | Cons |\n",
    "|----------|--------------|------|------|\n",
    "| **Without Feast** | All 21 features | Simple server | Feature drift, complex client |\n",
    "| **With Feast** | Entity IDs only | No drift, simple client | Requires Feast online store |\n",
    "\n",
    "**Key Benefit**: Training and serving use the **same FeatureService**, eliminating feature skew.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. `01-feast-features.ipynb` completed (online store populated)\n",
    "2. `02-training.ipynb` completed (model artifacts saved)\n",
    "3. KServe operator installed on cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q kserve pandas requests kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kserve import KServeClient, V1beta1InferenceService, V1beta1InferenceServiceSpec, V1beta1PredictorSpec\n",
    "from kubernetes import client as k8s_client, config\n",
    "from kubernetes.client import V1Container, V1ResourceRequirements, V1VolumeMount, V1Volume\n",
    "from kubernetes.client import V1PersistentVolumeClaimVolumeSource, V1ConfigMapVolumeSource\n",
    "from kubernetes.client import V1EnvVar, V1Probe, V1HTTPGetAction, V1ConfigMap, V1ObjectMeta\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Configuration\n",
    "\n",
    "| Parameter | Value | Purpose |\n",
    "|-----------|-------|---------|\n",
    "| `MODEL_NAME` | sales-forecast | InferenceService name |\n",
    "| `MODEL_DIR` | /shared/models | Path to model artifacts |\n",
    "| `FEAST_SERVER_URL` | feast-server:6566 | Feast online feature server |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMESPACE = \"feast-trainer-demo\"\n",
    "MODEL_NAME = \"sales-forecast\"\n",
    "MODEL_DIR = \"/shared/models\"\n",
    "PVC_NAME = \"shared\"\n",
    "CONFIGMAP_NAME = \"sales-forecast-serve\"\n",
    "\n",
    "# Trainer image (has PyTorch pre-installed, matches Ray cluster Python version)\n",
    "TRAINER_IMAGE = \"quay.io/modh/training:py312-cuda128-torch280\"\n",
    "\n",
    "# Feast Feature Server (deployed via manifests)\n",
    "FEAST_SERVER_URL = f\"http://feast-server.{NAMESPACE}.svc.cluster.local:6566\"\n",
    "\n",
    "print(f\"ğŸ“‹ Configuration:\")\n",
    "print(f\"   Namespace: {NAMESPACE}\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Image: {TRAINER_IMAGE}\")\n",
    "print(f\"   PVC: {PVC_NAME}\")\n",
    "print(f\"   Feast Server: {FEAST_SERVER_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Create Serving Script\n",
    "\n",
    "The serving script implements KServe's `Model` interface:\n",
    "\n",
    "| Method | Purpose | Called When |\n",
    "|--------|---------|-------------|\n",
    "| `load()` | Load model, scalers, feature columns | Server startup |\n",
    "| `preprocess()` | Extract entities â†’ Call Feast â†’ Build feature matrix | Each request |\n",
    "| `predict()` | Scale features â†’ Model inference â†’ Inverse transform | Each request |\n",
    "| `postprocess()` | Format as V2 InferResponse | Each request |\n",
    "\n",
    "### Request Flow\n",
    "\n",
    "```python\n",
    "# Input: Entity IDs\n",
    "{\"inputs\": [{\"name\": \"entities\", \"data\": [{\"store_id\": 1, \"dept_id\": 3}]}]}\n",
    "\n",
    "# preprocess() â†’ Feast lookup â†’ Feature matrix\n",
    "[[lag_1, lag_2, ..., temperature, fuel_price]]  # 21 features\n",
    "\n",
    "# predict() â†’ Model inference â†’ Inverse transform\n",
    "[96763.45]  # Predicted weekly sales\n",
    "\n",
    "# Output: V2 Response\n",
    "{\"outputs\": [{\"name\": \"predictions\", \"data\": [96763.45]}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVE_SCRIPT = '''#!/usr/bin/env python3\n",
    "\"\"\"Sales Forecasting Inference Server (KServe V2 Protocol)\n",
    "\n",
    "This server:\n",
    "1. Receives entity IDs (store_id, dept_id)\n",
    "2. Fetches features from Feast online store\n",
    "3. Runs model inference\n",
    "4. Returns predicted weekly sales\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import joblib\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Dict, Union\n",
    "from kserve import Model, ModelServer, InferRequest, InferResponse, InferOutput\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "FEAST_SERVER_URL = os.getenv(\"FEAST_SERVER_URL\", \"http://feast-server.feast-trainer-demo.svc.cluster.local:6566\")\n",
    "\n",
    "\n",
    "class SalesMLP(nn.Module):\n",
    "    \"\"\"Multi-layer perceptron matching training architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=None, dropout=0.2):\n",
    "        super().__init__()\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [256, 128, 64]\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "\n",
    "class SalesForecastModel(Model):\n",
    "    \"\"\"KServe Model with Feast feature lookup.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        super().__init__(name)\n",
    "        self.model = None\n",
    "        self.scalers = None\n",
    "        self.feature_cols = None\n",
    "        self.use_log_transform = False\n",
    "        self.ready = False\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"Load model and scalers from shared storage.\"\"\"\n",
    "        model_dir = os.getenv(\"MODEL_DIR\", \"/shared/models\")\n",
    "        logger.info(f\"Loading model from {model_dir}...\")\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(f\"{model_dir}/model_metadata.json\") as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        # Initialize model with same architecture\n",
    "        hidden_dims = metadata.get(\"hidden_dims\", [256, 128, 64])\n",
    "        dropout = metadata.get(\"dropout\", 0.2)\n",
    "        input_dim = metadata[\"input_dim\"]\n",
    "        \n",
    "        self.model = SalesMLP(input_dim, hidden_dims, dropout)\n",
    "        self.model.load_state_dict(\n",
    "            torch.load(f\"{model_dir}/model_best.pt\", map_location=\"cpu\", weights_only=True)\n",
    "        )\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Load scalers\n",
    "        self.scalers = joblib.load(f\"{model_dir}/scalers.joblib\")\n",
    "        self.feature_cols = metadata[\"feature_columns\"]\n",
    "        self.use_log_transform = self.scalers.get(\"use_log_transform\", False)\n",
    "        \n",
    "        logger.info(f\"Model loaded: {len(self.feature_cols)} features\")\n",
    "        self.ready = True\n",
    "    \n",
    "    def _get_features_from_feast(self, entity_rows):\n",
    "        \"\"\"Fetch features from Feast online store.\"\"\"\n",
    "        payload = {\n",
    "            \"feature_service\": \"inference_features\",\n",
    "            \"entities\": {\n",
    "                \"store_id\": [row[\"store_id\"] for row in entity_rows],\n",
    "                \"dept_id\": [row[\"dept_id\"] for row in entity_rows],\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{FEAST_SERVER_URL}/get-online-features\",\n",
    "            json=payload,\n",
    "            timeout=10\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        \n",
    "        # Parse response into feature rows\n",
    "        feature_names = result.get(\"metadata\", {}).get(\"feature_names\", [])\n",
    "        results = result.get(\"results\", [])\n",
    "        \n",
    "        features = []\n",
    "        for i in range(len(entity_rows)):\n",
    "            row = {}\n",
    "            for j, name in enumerate(feature_names):\n",
    "                if name in self.feature_cols and j < len(results):\n",
    "                    val = results[j].get(\"values\", [None])[i] if i < len(results[j].get(\"values\", [])) else None\n",
    "                    row[name] = val if val is not None else 0\n",
    "            features.append(row)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def preprocess(self, payload: Union[Dict, InferRequest], headers: Dict = None) -> np.ndarray:\n",
    "        \"\"\"Extract entities and fetch features from Feast.\"\"\"\n",
    "        if isinstance(payload, InferRequest):\n",
    "            inputs = {inp.name: inp.data for inp in payload.inputs}\n",
    "        else:\n",
    "            inputs = {inp[\"name\"]: inp.get(\"data\") for inp in payload.get(\"inputs\", [])}\n",
    "        \n",
    "        # Option 1: Client sends entity IDs â†’ fetch from Feast\n",
    "        if \"entities\" in inputs:\n",
    "            entities = inputs[\"entities\"]\n",
    "            if isinstance(entities, list) and len(entities) > 0:\n",
    "                feature_rows = self._get_features_from_feast(entities)\n",
    "                X = np.array(\n",
    "                    [[row.get(c, 0) for c in self.feature_cols] for row in feature_rows],\n",
    "                    dtype=np.float32\n",
    "                )\n",
    "                return X\n",
    "        \n",
    "        # Option 2: Client sends pre-computed features directly\n",
    "        if \"features\" in inputs:\n",
    "            return np.array(inputs[\"features\"], dtype=np.float32)\n",
    "        \n",
    "        raise ValueError(\"Input must have \\'entities\\' or \\'features\\'\")\n",
    "    \n",
    "    def predict(self, X: np.ndarray, headers: Dict = None) -> np.ndarray:\n",
    "        \"\"\"Run model inference.\"\"\"\n",
    "        X_scaled = self.scalers[\"scaler_X\"].transform(X)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = self.model(torch.FloatTensor(X_scaled)).numpy()\n",
    "        \n",
    "        # Inverse transform\n",
    "        predictions = self.scalers[\"scaler_y\"].inverse_transform(preds.reshape(-1, 1)).flatten()\n",
    "        if self.use_log_transform:\n",
    "            predictions = np.expm1(predictions)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def postprocess(self, predictions: np.ndarray, headers: Dict = None) -> Union[Dict, InferResponse]:\n",
    "        \"\"\"Format as V2 InferResponse.\"\"\"\n",
    "        return InferResponse(\n",
    "            model_name=self.name,\n",
    "            response_id=str(hash(str(predictions.tolist()))),\n",
    "            infer_outputs=[\n",
    "                InferOutput(\n",
    "                    name=\"predictions\",\n",
    "                    shape=list(predictions.shape),\n",
    "                    datatype=\"FP32\",\n",
    "                    data=predictions.tolist()\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = SalesForecastModel(\"sales-forecast\")\n",
    "    model.load()\n",
    "    ModelServer().start([model])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Kubernetes config\n",
    "try:\n",
    "    config.load_incluster_config()\n",
    "    print(\"âœ… Loaded in-cluster Kubernetes config\")\n",
    "except:\n",
    "    config.load_kube_config()\n",
    "    print(\"âœ… Loaded local Kubernetes config\")\n",
    "\n",
    "core_v1 = k8s_client.CoreV1Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ConfigMap with serving script\n",
    "configmap = V1ConfigMap(\n",
    "    metadata=V1ObjectMeta(\n",
    "        name=CONFIGMAP_NAME,\n",
    "        namespace=NAMESPACE,\n",
    "        labels={\"app\": \"sales-forecasting\"}\n",
    "    ),\n",
    "    data={\"serve.py\": SERVE_SCRIPT}\n",
    ")\n",
    "\n",
    "try:\n",
    "    core_v1.read_namespaced_config_map(CONFIGMAP_NAME, NAMESPACE)\n",
    "    print(f\"âš ï¸ ConfigMap '{CONFIGMAP_NAME}' exists, replacing...\")\n",
    "    core_v1.replace_namespaced_config_map(CONFIGMAP_NAME, NAMESPACE, configmap)\n",
    "except:\n",
    "    print(f\"ğŸ“¦ Creating ConfigMap '{CONFIGMAP_NAME}'...\")\n",
    "    core_v1.create_namespaced_config_map(NAMESPACE, configmap)\n",
    "\n",
    "print(f\"âœ… ConfigMap created with serve.py script\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Deploy InferenceService\n",
    "\n",
    "### InferenceService Configuration\n",
    "\n",
    "| Setting | Value | Purpose |\n",
    "|---------|-------|---------|\n",
    "| `minReplicas` | 1 | Always have at least 1 replica |\n",
    "| `maxReplicas` | 3 | Scale up under load |\n",
    "| `deploymentMode` | RawDeployment | Use standard K8s deployment |\n",
    "| `readinessProbe` | /v2/health/ready | Health check endpoint |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kserve_client = KServeClient()\n",
    "print(f\"âœ… KServeClient initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build InferenceService spec\n",
    "isvc = V1beta1InferenceService(\n",
    "    api_version=\"serving.kserve.io/v1beta1\",\n",
    "    kind=\"InferenceService\",\n",
    "    metadata=k8s_client.V1ObjectMeta(\n",
    "        name=MODEL_NAME,\n",
    "        namespace=NAMESPACE,\n",
    "        labels={\"app\": \"sales-forecasting\"},\n",
    "        annotations={\"serving.kserve.io/deploymentMode\": \"RawDeployment\"}\n",
    "    ),\n",
    "    spec=V1beta1InferenceServiceSpec(\n",
    "        predictor=V1beta1PredictorSpec(\n",
    "            min_replicas=1,\n",
    "            max_replicas=3,\n",
    "            containers=[\n",
    "                V1Container(\n",
    "                    name=\"kserve-container\",\n",
    "                    image=TRAINER_IMAGE,\n",
    "                    command=[\"/bin/bash\", \"-c\"],\n",
    "                    args=[\"pip install -q kserve joblib numpy scikit-learn && python /scripts/serve.py\"],\n",
    "                    ports=[k8s_client.V1ContainerPort(container_port=8080, protocol=\"TCP\")],\n",
    "                    env=[\n",
    "                        V1EnvVar(name=\"MODEL_DIR\", value=MODEL_DIR),\n",
    "                        V1EnvVar(name=\"FEAST_SERVER_URL\", value=FEAST_SERVER_URL),\n",
    "                    ],\n",
    "                    resources=V1ResourceRequirements(\n",
    "                        requests={\"cpu\": \"500m\", \"memory\": \"1Gi\"},\n",
    "                        limits={\"cpu\": \"2\", \"memory\": \"4Gi\"}\n",
    "                    ),\n",
    "                    readiness_probe=V1Probe(\n",
    "                        http_get=V1HTTPGetAction(path=\"/v2/health/ready\", port=8080),\n",
    "                        initial_delay_seconds=60,\n",
    "                        period_seconds=10\n",
    "                    ),\n",
    "                    volume_mounts=[\n",
    "                        V1VolumeMount(name=\"scripts\", mount_path=\"/scripts\"),\n",
    "                        V1VolumeMount(name=\"model-storage\", mount_path=\"/shared\"),\n",
    "                    ]\n",
    "                )\n",
    "            ],\n",
    "            volumes=[\n",
    "                V1Volume(name=\"scripts\", config_map=V1ConfigMapVolumeSource(name=CONFIGMAP_NAME)),\n",
    "                V1Volume(name=\"model-storage\", persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(claim_name=PVC_NAME)),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"âœ… InferenceService spec created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy InferenceService\n",
    "try:\n",
    "    existing = kserve_client.get(MODEL_NAME, namespace=NAMESPACE)\n",
    "    print(f\"âš ï¸ InferenceService '{MODEL_NAME}' exists, replacing...\")\n",
    "    kserve_client.replace(MODEL_NAME, isvc, namespace=NAMESPACE)\n",
    "except Exception:\n",
    "    print(f\"ğŸ“¦ Creating InferenceService '{MODEL_NAME}'...\")\n",
    "    kserve_client.create(isvc)\n",
    "\n",
    "print(f\"âœ… InferenceService submitted to namespace '{NAMESPACE}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for ready\n",
    "print(\"â³ Waiting for InferenceService to be ready...\")\n",
    "print(\"   This may take 1-2 minutes for the first deployment.\")\n",
    "\n",
    "try:\n",
    "    kserve_client.wait_isvc_ready(MODEL_NAME, namespace=NAMESPACE, timeout_seconds=300)\n",
    "    isvc_status = kserve_client.get(MODEL_NAME, namespace=NAMESPACE)\n",
    "    url = isvc_status.get(\"status\", {}).get(\"url\", \"\")\n",
    "    print(f\"âœ… InferenceService ready!\")\n",
    "    print(f\"   URL: {url}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Timeout waiting for ready: {e}\")\n",
    "    print(\"   Check pod status: kubectl get pods -n feast-trainer-demo -l app=sales-forecasting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Make Predictions (V2 Protocol)\n",
    "\n",
    "### V2 Inference Protocol\n",
    "\n",
    "KServe uses the V2 inference protocol. Request format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"inputs\": [\n",
    "    {\n",
    "      \"name\": \"entities\",\n",
    "      \"shape\": [n],\n",
    "      \"datatype\": \"BYTES\",\n",
    "      \"data\": [{\"store_id\": 1, \"dept_id\": 3}, ...]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Response format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"outputs\": [\n",
    "    {\n",
    "      \"name\": \"predictions\",\n",
    "      \"shape\": [n],\n",
    "      \"datatype\": \"FP32\",\n",
    "      \"data\": [96763.45, ...]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct endpoint URL\n",
    "ENDPOINT = f\"http://{MODEL_NAME}-predictor.{NAMESPACE}.svc.cluster.local:8080\"\n",
    "\n",
    "# Check health\n",
    "try:\n",
    "    resp = requests.get(f\"{ENDPOINT}/v2/health/ready\", timeout=5)\n",
    "    ready = resp.status_code == 200\n",
    "except:\n",
    "    ready = False\n",
    "    \n",
    "print(f\"Inference Endpoint: {ENDPOINT}\")\n",
    "print(f\"Ready: {'âœ… Yes' if ready else 'âŒ No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_feast(entities: list) -> dict:\n",
    "    \"\"\"Make prediction using KServe V2 protocol with Feast feature lookup.\n",
    "    \n",
    "    Args:\n",
    "        entities: List of entity dicts, e.g., [{\"store_id\": 1, \"dept_id\": 3}]\n",
    "    \n",
    "    Returns:\n",
    "        Dict with predictions, latency, and entities\n",
    "    \"\"\"\n",
    "    url = f\"{ENDPOINT}/v2/models/{MODEL_NAME}/infer\"\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"entities\",\n",
    "                \"shape\": [len(entities)],\n",
    "                \"datatype\": \"BYTES\",\n",
    "                \"data\": entities\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    t0 = time.time()\n",
    "    response = requests.post(url, json=payload, timeout=30)\n",
    "    latency = (time.time() - t0) * 1000\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Prediction failed: {response.text}\")\n",
    "    \n",
    "    result = response.json()\n",
    "    predictions = result.get(\"outputs\", [{}])[0].get(\"data\", [])\n",
    "    \n",
    "    return {\n",
    "        \"predictions\": predictions,\n",
    "        \"latency_ms\": latency,\n",
    "        \"entities\": entities\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single prediction\n",
    "print(\"ğŸ“Š Single Prediction Test\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "entity = {\"store_id\": 1, \"dept_id\": 3}\n",
    "try:\n",
    "    result = predict_with_feast([entity])\n",
    "    print(f\"âœ… Store {entity['store_id']}, Dept {entity['dept_id']}\")\n",
    "    print(f\"   Predicted Weekly Sales: ${result['predictions'][0]:,.0f}\")\n",
    "    print(f\"   Latency: {result['latency_ms']:.0f}ms\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Prediction error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch predictions\n",
    "print(\"ğŸ“Š Batch Prediction Test\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "entities = [\n",
    "    {\"store_id\": s, \"dept_id\": d}\n",
    "    for s in [1, 10, 25, 45]\n",
    "    for d in [1, 5, 10, 14]\n",
    "]\n",
    "print(f\"Scoring {len(entities)} entities...\")\n",
    "\n",
    "try:\n",
    "    result = predict_with_feast(entities)\n",
    "    results_df = pd.DataFrame([\n",
    "        {**e, \"prediction\": f\"${p:,.0f}\"}\n",
    "        for e, p in zip(entities, result[\"predictions\"])\n",
    "    ])\n",
    "    print(f\"\\nâœ… {len(results_df)} predictions in {result['latency_ms']:.0f}ms\")\n",
    "    print(f\"   Avg latency per prediction: {result['latency_ms']/len(entities):.1f}ms\")\n",
    "    print()\n",
    "    display(results_df)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Batch prediction error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## âœ… Pipeline Complete!\n",
    "\n",
    "### End-to-End Summary\n",
    "\n",
    "| Notebook | Component | Output |\n",
    "|----------|-----------|--------|\n",
    "| 01 - Features | Feast | Features registered, online store populated |\n",
    "| 02 - Training | PyTorch + Ray | Model trained with `get_historical_features()` |\n",
    "| 03 - Inference | KServe | Model serving with Feast integration |\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "| Benefit | How |\n",
    "|---------|----- |\n",
    "| **No feature skew** | Same FeatureService (`inference_features`) for train & serve |\n",
    "| **Scalable training** | Ray distributes `get_historical_features()` |\n",
    "| **Scalable serving** | KServe auto-scales 1-3 replicas |\n",
    "| **Low latency** | Feast online store (<50ms feature lookup) |\n",
    "| **Simple client** | Send entity IDs only, get predictions |\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| InferenceService not ready | Check pod logs: `kubectl logs -l app=sales-forecasting -n feast-trainer-demo` |\n",
    "| Feast lookup fails | Verify Feast server: `kubectl get pods -l app=feast-server -n feast-trainer-demo` |\n",
    "| Model load error | Check model artifacts exist in `/shared/models/` |\n",
    "| RBAC error (403) | Ensure ServiceAccount has ConfigMap access |\n",
    "\n",
    "### Production Deployment\n",
    "\n",
    "For production, use the manifest-based approach:\n",
    "\n",
    "```bash\n",
    "# Deploy all infrastructure\n",
    "kubectl apply -k manifests/\n",
    "\n",
    "# Monitor training\n",
    "kubectl logs -f -l trainer.kubeflow.org/trainjob-name=sales-training\n",
    "\n",
    "# Check serving\n",
    "kubectl get inferenceservice -n feast-trainer-demo\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
