{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 - Model Serving with KServe\n",
        "\n",
        "![Workflow](../docs/03-inference-workflow.png)\n",
        "\n",
        "## What This Notebook Does\n",
        "\n",
        "| Step | Action | Component |\n",
        "|------|--------|------------|\n",
        "| 1 | Create serving script | ConfigMap |\n",
        "| 2 | Deploy InferenceService | KServe |\n",
        "| 3 | Make predictions | REST API (V2 protocol) |\n",
        "\n",
        "## Inference Flow\n",
        "\n",
        "```\n",
        "Client                 KServe                Feast Server\n",
        "  ‚îÇ                      ‚îÇ                        ‚îÇ\n",
        "  ‚îÇ {store_id, dept_id}  ‚îÇ                        ‚îÇ\n",
        "  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ                        ‚îÇ\n",
        "  ‚îÇ                      ‚îÇ  get-online-features   ‚îÇ\n",
        "  ‚îÇ                      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ\n",
        "  ‚îÇ                      ‚îÇ                        ‚îÇ\n",
        "  ‚îÇ                      ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ\n",
        "  ‚îÇ                      ‚îÇ  [15 features]         ‚îÇ\n",
        "  ‚îÇ                      ‚îÇ                        ‚îÇ\n",
        "  ‚îÇ                      ‚îÇ  model.predict()       ‚îÇ\n",
        "  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                        ‚îÇ\n",
        "  ‚îÇ  prediction: $96,763 ‚îÇ                        ‚îÇ\n",
        "```\n",
        "\n",
        "**Key:** Client sends entity IDs only; KServe fetches features from Feast.\n",
        "\n",
        "**Prerequisites:** `01-feast-features.ipynb` and `02-training.ipynb` completed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q kserve pandas tqdm kubernetes\n",
        "from kserve import (\n",
        "    KServeClient,\n",
        "    V1beta1InferenceService,\n",
        "    V1beta1InferenceServiceSpec,\n",
        "    V1beta1PredictorSpec,\n",
        "    constants\n",
        ")\n",
        "from kubernetes import client as k8s_client\n",
        "from kubernetes.client import V1Container, V1ResourceRequirements, V1VolumeMount, V1Volume, V1PersistentVolumeClaimVolumeSource, V1ConfigMapVolumeSource, V1EnvVar, V1Probe, V1HTTPGetAction\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "| Parameter | Value | Purpose |\n",
        "|-----------|-------|----------|\n",
        "| `MODEL_NAME` | `sales-forecast` | InferenceService name |\n",
        "| `MODEL_DIR` | `/shared/models` | Path to trained model |\n",
        "| `FEAST_SERVER_URL` | `http://feast-server:6566` | Online feature store |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NAMESPACE = \"feast-trainer-demo\"\n",
        "MODEL_NAME = \"sales-forecast\"\n",
        "MODEL_DIR = \"/shared/models\"\n",
        "PVC_NAME = \"shared\"\n",
        "CONFIGMAP_NAME = \"sales-forecast-serve\"\n",
        "\n",
        "# Trainer image (has PyTorch pre-installed)\n",
        "TRAINER_IMAGE = \"quay.io/modh/training:py311-cuda124-torch251\"\n",
        "\n",
        "# Feast Feature Server\n",
        "FEAST_SERVER_URL = f\"http://feast-server.{NAMESPACE}.svc.cluster.local:6566\"\n",
        "\n",
        "print(f\"Namespace: {NAMESPACE}\")\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"PVC: {PVC_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Create Serving Script\n",
        "\n",
        "The serving script implements KServe's `Model` interface:\n",
        "\n",
        "| Method | Purpose |\n",
        "|--------|----------|\n",
        "| `load()` | Load model, scalers, feature columns |\n",
        "| `preprocess()` | Extract entities ‚Üí Call Feast ‚Üí Build feature matrix |\n",
        "| `predict()` | Scale features ‚Üí Model inference ‚Üí Inverse transform |\n",
        "| `postprocess()` | Format as V2 InferResponse |\n",
        "\n",
        "**Feast Integration:**\n",
        "```python\n",
        "def _get_features_from_feast(self, entities):\n",
        "    # POST to Feast server's online feature API\n",
        "    response = requests.post(\n",
        "        f\"{FEAST_SERVER_URL}/get-online-features\",\n",
        "        json={\"feature_service\": \"inference_features\", \"entities\": {...}}\n",
        "    )\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create ConfigMap with serving script (required by InferenceService)\n",
        "from kubernetes.client import V1ConfigMap, V1ObjectMeta\n",
        "from kubernetes import config\n",
        "\n",
        "# Load in-cluster config or local kubeconfig\n",
        "try:\n",
        "    config.load_incluster_config()\n",
        "except:\n",
        "    config.load_kube_config()\n",
        "\n",
        "core_v1 = k8s_client.CoreV1Api()\n",
        "\n",
        "SERVE_SCRIPT = '''#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Sales Forecasting Inference Server (KServe V2 Protocol)\n",
        "\n",
        "Uses KServe Model class for standard inference protocol.\n",
        "Feast features are fetched in preprocess step.\n",
        "\"\"\"\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import joblib\n",
        "import numpy as np\n",
        "import logging\n",
        "from typing import Dict, Union\n",
        "import kserve\n",
        "from kserve import Model, ModelServer, InferRequest, InferResponse, InferOutput\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "FEAST_SERVER_URL = os.getenv(\"FEAST_SERVER_URL\", \"http://feast-server.feast-trainer-demo.svc.cluster.local:6566\")\n",
        "\n",
        "class SalesMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout=0.2):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for h_dim in hidden_dims:\n",
        "            layers.extend([nn.Linear(prev_dim, h_dim), nn.BatchNorm1d(h_dim), nn.ReLU(), nn.Dropout(dropout)])\n",
        "            prev_dim = h_dim\n",
        "        layers.append(nn.Linear(prev_dim, 1))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(-1)\n",
        "\n",
        "\n",
        "class SalesForecastModel(Model):\n",
        "    def __init__(self, name: str):\n",
        "        super().__init__(name)\n",
        "        self.model = None\n",
        "        self.scalers = None\n",
        "        self.feature_cols = None\n",
        "        self.metadata = None\n",
        "        self.use_log_transform = False\n",
        "        self.ready = False\n",
        "    \n",
        "    def load(self):\n",
        "        model_dir = os.getenv(\"MODEL_DIR\", \"/shared/models\")\n",
        "        logger.info(f\"Loading model from {model_dir}...\")\n",
        "        \n",
        "        with open(f\"{model_dir}/model_metadata.json\") as f:\n",
        "            self.metadata = json.load(f)\n",
        "        \n",
        "        hidden_dims = self.metadata.get(\"hidden_dims\", [256, 128, 64])\n",
        "        dropout = self.metadata.get(\"dropout\", 0.2)\n",
        "        input_dim = self.metadata[\"input_dim\"]\n",
        "        \n",
        "        self.model = SalesMLP(input_dim, hidden_dims, dropout)\n",
        "        self.model.load_state_dict(torch.load(f\"{model_dir}/best_model.pt\", map_location=\"cpu\", weights_only=True))\n",
        "        self.model.eval()\n",
        "        \n",
        "        self.scalers = joblib.load(f\"{model_dir}/scalers.joblib\")\n",
        "        self.feature_cols = self.metadata[\"feature_columns\"]\n",
        "        self.use_log_transform = self.scalers.get(\"use_log_transform\", False)\n",
        "        \n",
        "        logger.info(f\"Model loaded: {len(self.feature_cols)} features, arch={hidden_dims}\")\n",
        "        self.ready = True\n",
        "    \n",
        "    def _get_features_from_feast(self, entity_rows):\n",
        "        payload = {\n",
        "            \"feature_service\": \"inference_features\",\n",
        "            \"entities\": {\n",
        "                \"store_id\": [row[\"store_id\"] for row in entity_rows],\n",
        "                \"dept_id\": [row[\"dept_id\"] for row in entity_rows],\n",
        "            }\n",
        "        }\n",
        "        response = requests.post(f\"{FEAST_SERVER_URL}/get-online-features\", json=payload, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        \n",
        "        feature_names = result.get(\"metadata\", {}).get(\"feature_names\", [])\n",
        "        results = result.get(\"results\", [])\n",
        "        \n",
        "        features = []\n",
        "        for i in range(len(entity_rows)):\n",
        "            row = {}\n",
        "            for j, name in enumerate(feature_names):\n",
        "                if name in self.feature_cols and j < len(results):\n",
        "                    val = results[j].get(\"values\", [None])[i] if i < len(results[j].get(\"values\", [])) else None\n",
        "                    row[name] = val if val is not None else 0\n",
        "            features.append(row)\n",
        "        return features\n",
        "    \n",
        "    def preprocess(self, payload: Union[Dict, InferRequest], headers: Dict = None) -> np.ndarray:\n",
        "        if isinstance(payload, InferRequest):\n",
        "            inputs = {inp.name: inp.data for inp in payload.inputs}\n",
        "        else:\n",
        "            inputs = {}\n",
        "            for inp in payload.get(\"inputs\", []):\n",
        "                inputs[inp[\"name\"]] = inp.get(\"data\", inp.get(\"datatype\"))\n",
        "        \n",
        "        if \"entities\" in inputs:\n",
        "            entities = inputs[\"entities\"]\n",
        "            if isinstance(entities, list) and len(entities) > 0:\n",
        "                logger.info(f\"Fetching features from Feast for {len(entities)} entities\")\n",
        "                feature_rows = self._get_features_from_feast(entities)\n",
        "                X = np.array([[row.get(c, 0) for c in self.feature_cols] for row in feature_rows], dtype=np.float32)\n",
        "                return X\n",
        "        \n",
        "        if \"features\" in inputs:\n",
        "            return np.array(inputs[\"features\"], dtype=np.float32)\n",
        "        \n",
        "        raise ValueError(\"Input must have 'entities' or 'features'\")\n",
        "    \n",
        "    def predict(self, X: np.ndarray, headers: Dict = None) -> np.ndarray:\n",
        "        X_scaled = self.scalers[\"scaler_X\"].transform(X)\n",
        "        with torch.no_grad():\n",
        "            preds = self.model(torch.FloatTensor(X_scaled)).numpy()\n",
        "        predictions = self.scalers[\"scaler_y\"].inverse_transform(preds.reshape(-1, 1)).flatten()\n",
        "        if self.use_log_transform:\n",
        "            predictions = np.expm1(predictions)\n",
        "        return predictions\n",
        "    \n",
        "    def postprocess(self, predictions: np.ndarray, headers: Dict = None) -> Union[Dict, InferResponse]:\n",
        "        return InferResponse(\n",
        "            model_name=self.name,\n",
        "            infer_outputs=[InferOutput(name=\"predictions\", shape=list(predictions.shape), datatype=\"FP32\", data=predictions.tolist())]\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = SalesForecastModel(\"sales-forecast\")\n",
        "    model.load()\n",
        "    ModelServer().start([model])\n",
        "'''\n",
        "\n",
        "configmap = V1ConfigMap(\n",
        "    metadata=V1ObjectMeta(name=CONFIGMAP_NAME, namespace=NAMESPACE, labels={\"app\": \"sales-forecasting\"}),\n",
        "    data={\"serve.py\": SERVE_SCRIPT}\n",
        ")\n",
        "\n",
        "try:\n",
        "    core_v1.read_namespaced_config_map(CONFIGMAP_NAME, NAMESPACE)\n",
        "    print(f\"‚ö†Ô∏è ConfigMap {CONFIGMAP_NAME} exists, replacing...\")\n",
        "    core_v1.replace_namespaced_config_map(CONFIGMAP_NAME, NAMESPACE, configmap)\n",
        "except:\n",
        "    print(f\"üì¶ Creating ConfigMap {CONFIGMAP_NAME}...\")\n",
        "    core_v1.create_namespaced_config_map(NAMESPACE, configmap)\n",
        "\n",
        "print(f\"‚úÖ ConfigMap created with serve.py script\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize KServe Client\n",
        "\n",
        "The `KServeClient` manages InferenceService lifecycle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize KServe client for model management\n",
        "kserve_client = KServeClient()\n",
        "print(f\"‚úÖ KServeClient initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build InferenceService Spec\n",
        "\n",
        "Define the model server deployment:\n",
        "\n",
        "| Component | Value | Purpose |\n",
        "|-----------|-------|----------|\n",
        "| `image` | `quay.io/modh/training:...` | Container with PyTorch |\n",
        "| `replicas` | 1-3 | Autoscaling range |\n",
        "| `resources` | 2 CPU, 4Gi | Per-pod limits |\n",
        "\n",
        "**Volume Mounts:**\n",
        "- `/scripts` ‚Üí ConfigMap with serve.py\n",
        "- `/shared` ‚Üí PVC with model artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build InferenceService spec using KServe SDK\n",
        "isvc = V1beta1InferenceService(\n",
        "    api_version=\"serving.kserve.io/v1beta1\",\n",
        "    kind=\"InferenceService\",\n",
        "    metadata=k8s_client.V1ObjectMeta(\n",
        "        name=MODEL_NAME,\n",
        "        namespace=NAMESPACE,\n",
        "        labels={\"app\": \"sales-forecasting\"},\n",
        "        annotations={\"serving.kserve.io/deploymentMode\": \"RawDeployment\"}\n",
        "    ),\n",
        "    spec=V1beta1InferenceServiceSpec(\n",
        "        predictor=V1beta1PredictorSpec(\n",
        "            min_replicas=1,\n",
        "            max_replicas=3,\n",
        "            containers=[\n",
        "                V1Container(\n",
        "                    name=\"kserve-container\",\n",
        "                    image=TRAINER_IMAGE,\n",
        "                    command=[\"/bin/bash\", \"-c\"],\n",
        "                    args=[\"pip install -q kserve joblib numpy scikit-learn && python /scripts/serve.py\"],\n",
        "                    ports=[k8s_client.V1ContainerPort(container_port=8080, protocol=\"TCP\")],\n",
        "                    env=[\n",
        "                        V1EnvVar(name=\"MODEL_DIR\", value=MODEL_DIR),\n",
        "                        V1EnvVar(name=\"FEAST_SERVER_URL\", value=FEAST_SERVER_URL),\n",
        "                    ],\n",
        "                    resources=V1ResourceRequirements(\n",
        "                        requests={\"cpu\": \"500m\", \"memory\": \"1Gi\"},\n",
        "                        limits={\"cpu\": \"2\", \"memory\": \"4Gi\"}\n",
        "                    ),\n",
        "                    readiness_probe=V1Probe(\n",
        "                        http_get=V1HTTPGetAction(path=\"/v2/health/ready\", port=8080),\n",
        "                        initial_delay_seconds=60,\n",
        "                        period_seconds=10\n",
        "                    ),\n",
        "                    volume_mounts=[\n",
        "                        V1VolumeMount(name=\"scripts\", mount_path=\"/scripts\"),\n",
        "                        V1VolumeMount(name=\"model-storage\", mount_path=\"/shared\"),\n",
        "                    ]\n",
        "                )\n",
        "            ],\n",
        "            volumes=[\n",
        "                V1Volume(name=\"scripts\", config_map=V1ConfigMapVolumeSource(name=CONFIGMAP_NAME)),\n",
        "                V1Volume(name=\"model-storage\", persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(claim_name=PVC_NAME)),\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"‚úÖ InferenceService spec created\")\n",
        "print(f\"   Image: {TRAINER_IMAGE}\")\n",
        "print(f\"   Model dir: {MODEL_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Deploy InferenceService\n",
        "\n",
        "Create or update the service in Kubernetes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deploy InferenceService (create or replace)\n",
        "try:\n",
        "    # Check if already exists\n",
        "    existing = kserve_client.get(MODEL_NAME, namespace=NAMESPACE)\n",
        "    print(f\"‚ö†Ô∏è InferenceService '{MODEL_NAME}' exists, replacing...\")\n",
        "    kserve_client.replace(MODEL_NAME, isvc, namespace=NAMESPACE)\n",
        "except Exception:\n",
        "    print(f\"üì¶ Creating InferenceService '{MODEL_NAME}'...\")\n",
        "    kserve_client.create(isvc)\n",
        "\n",
        "print(f\"‚úÖ InferenceService submitted to namespace '{NAMESPACE}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Wait for Ready\n",
        "\n",
        "The service is ready when:\n",
        "1. Pod is running\n",
        "2. Model is loaded\n",
        "3. Health check passes (`/v2/health/ready`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for InferenceService to be ready\n",
        "print(\"‚è≥ Waiting for InferenceService to be ready...\")\n",
        "kserve_client.wait_isvc_ready(MODEL_NAME, namespace=NAMESPACE, timeout_seconds=300)\n",
        "\n",
        "# Get service status\n",
        "isvc_status = kserve_client.get(MODEL_NAME, namespace=NAMESPACE)\n",
        "url = isvc_status.get(\"status\", {}).get(\"url\", \"\")\n",
        "print(f\"‚úÖ InferenceService ready!\")\n",
        "print(f\"   URL: {url}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Make Predictions (V2 Protocol)\n",
        "\n",
        "KServe V2 (Open Inference Protocol) format:\n",
        "\n",
        "**Request:**\n",
        "```json\n",
        "POST /v2/models/sales-forecast/infer\n",
        "{\n",
        "  \"inputs\": [{\n",
        "    \"name\": \"entities\",\n",
        "    \"data\": [{\"store_id\": 1, \"dept_id\": 3}]\n",
        "  }]\n",
        "}\n",
        "```\n",
        "\n",
        "**Response:**\n",
        "```json\n",
        "{\n",
        "  \"outputs\": [{\n",
        "    \"name\": \"predictions\",\n",
        "    \"data\": [96763.45]\n",
        "  }]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create inference endpoint (KServe adds -predictor suffix)\n",
        "# Headless service requires direct port 8080\n",
        "ENDPOINT = f\"http://{MODEL_NAME}-predictor.{NAMESPACE}.svc.cluster.local:8080\"\n",
        "\n",
        "# Check server health\n",
        "try:\n",
        "    resp = requests.get(f\"{ENDPOINT}/v2/health/ready\", timeout=5)\n",
        "    ready = resp.status_code == 200\n",
        "except:\n",
        "    ready = False\n",
        "print(f\"‚úÖ Inference endpoint ready: {ready}\")\n",
        "print(f\"   Endpoint: {ENDPOINT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_with_feast(entities: list) -> dict:\n",
        "    \"\"\"\n",
        "    Make prediction using KServe V2 protocol with Feast feature lookup.\n",
        "    \"\"\"\n",
        "    # V2 protocol endpoint\n",
        "    url = f\"{ENDPOINT}/v2/models/{MODEL_NAME}/infer\"\n",
        "    \n",
        "    # V2 protocol payload format\n",
        "    payload = {\n",
        "        \"inputs\": [\n",
        "            {\n",
        "                \"name\": \"entities\",\n",
        "                \"shape\": [len(entities)],\n",
        "                \"datatype\": \"BYTES\",\n",
        "                \"data\": entities\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    t0 = time.time()\n",
        "    response = requests.post(url, json=payload, timeout=30)\n",
        "    latency = (time.time() - t0) * 1000\n",
        "    \n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Prediction failed: {response.text}\")\n",
        "    \n",
        "    result = response.json()\n",
        "    # V2 protocol returns outputs array\n",
        "    predictions = result.get(\"outputs\", [{}])[0].get(\"data\", [])\n",
        "    \n",
        "    return {\"predictions\": predictions, \"latency_ms\": latency, \"entities\": entities}\n",
        "\n",
        "# Single prediction\n",
        "entity = {\"store_id\": 1, \"dept_id\": 3}\n",
        "try:\n",
        "    result = predict_with_feast([entity])\n",
        "    print(f\"‚úÖ Store {entity['store_id']}, Dept {entity['dept_id']}: ${result['predictions'][0]:,.0f}\")\n",
        "    print(f\"   Latency: {result['latency_ms']:.0f}ms\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Prediction error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Batch Scoring\n",
        "\n",
        "Score multiple entities in a single request for efficiency:\n",
        "\n",
        "| Approach | Latency | Use Case |\n",
        "|----------|---------|----------|\n",
        "| Single | ~30ms | Real-time API |\n",
        "| Batch (16) | ~40ms | Bulk scoring |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score multiple entities in a single request (batched)\n",
        "entities = [{\"store_id\": s, \"dept_id\": d} for s in [1, 10, 25, 45] for d in [1, 5, 10, 14]]\n",
        "print(f\"Scoring {len(entities)} entities...\")\n",
        "\n",
        "try:\n",
        "    result = predict_with_feast(entities)\n",
        "    \n",
        "    # Build results dataframe\n",
        "    results = pd.DataFrame([\n",
        "        {**e, \"prediction\": p}\n",
        "        for e, p in zip(entities, result[\"predictions\"])\n",
        "    ])\n",
        "    \n",
        "    print(f\"\\n‚úÖ {len(results)} predictions in {result['latency_ms']:.0f}ms (batched)\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Batch prediction error: {e}\")\n",
        "    results = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'prediction' in results.columns and results['prediction'].notna().any():\n",
        "    print(f\"üìä Summary:\")\n",
        "    print(f\"   Min: ${results['prediction'].min():,.0f}\")\n",
        "    print(f\"   Max: ${results['prediction'].max():,.0f}\")\n",
        "    print(f\"   Mean: ${results['prediction'].mean():,.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Predictions\n",
        "\n",
        "Save batch results to PVC for downstream processing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "if not results.empty:\n",
        "    os.makedirs('/opt/app-root/src/shared/predictions', exist_ok=True)\n",
        "    path = f\"/opt/app-root/src/shared/predictions/batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet\"\n",
        "    results.to_parquet(path, index=False)\n",
        "    print(f\"‚úÖ Saved: {path}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No results to save\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Cleanup (Optional)\n",
        "\n",
        "Delete resources when done:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to delete resources\n",
        "# kserve_client.delete(MODEL_NAME, namespace=NAMESPACE)\n",
        "# core_v1.delete_namespaced_config_map(CONFIGMAP_NAME, NAMESPACE)\n",
        "# print(f\"‚úÖ Deleted InferenceService and ConfigMap\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## ‚úÖ Pipeline Complete!\n",
        "\n",
        "### End-to-End Summary\n",
        "\n",
        "| Notebook | Component | Output |\n",
        "|----------|-----------|--------|\n",
        "| 01 | Feast | Features registered, online store populated |\n",
        "| 02 | Kubeflow | Model trained, logged to MLflow |\n",
        "| 03 | KServe | Model serving with Feast integration |\n",
        "\n",
        "### Key Benefits of This Architecture\n",
        "\n",
        "| Benefit | How |\n",
        "|---------|-----|\n",
        "| **No feature skew** | Same FeatureService for train & serve |\n",
        "| **Scalable training** | Ray + PyTorch DDP |\n",
        "| **Low latency serving** | Feast online store (<50ms) |\n",
        "| **Experiment tracking** | MLflow for all runs |\n",
        "| **Simple client** | Send entity IDs, get predictions |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
