{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Feast Feature Store Setup\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook sets up the **Feast Feature Store** for retail demand forecasting. It generates synthetic Walmart-style sales data, engineers predictive features, and registers them in Feast using a **remote KubeRay cluster** for distributed processing.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "| Step | Action | Why It Matters |\n",
    "|------|--------|----------------|\n",
    "| 1 | Generate synthetic sales data | Creates realistic retail patterns |\n",
    "| 2 | Engineer lag/rolling features | Most predictive features (63% importance) |\n",
    "| 3 | `feast apply` via Ray | Register features in PostgreSQL registry |\n",
    "| 4 | `feast materialize` via Ray | Populate online store for serving |\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        FEAST FEATURE STORE ARCHITECTURE                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                            â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚  â”‚   DATA SOURCES      â”‚         â”‚      ENTITIES       â”‚                   â”‚\n",
    "â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   â”‚\n",
    "â”‚  â”‚ sales_features.pq   â”‚â”€â”€â”€â”€â”    â”‚ store_id (1-45)     â”‚                   â”‚\n",
    "â”‚  â”‚ store_features.pq   â”‚â”€â”€â” â”‚    â”‚ dept_id  (1-14)     â”‚                   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚                           â”‚ â”‚              â”‚                               â”‚\n",
    "â”‚                           â–¼ â–¼              â–¼                               â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚                        FEATURE VIEWS                                â”‚   â”‚\n",
    "â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚\n",
    "â”‚  â”‚  sales_features (19)         â”‚  store_features (3)                  â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ weekly_sales (target)     â”‚  â€¢ store_type (A/B/C)                â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ lag_1, lag_2, lag_4, lag_8â”‚  â€¢ store_size (sqft)                 â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ rolling_mean_4w, std_4w   â”‚  â€¢ region                            â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ week_of_year, month, qtr  â”‚                                      â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ is_holiday, days_to_hol   â”‚                                      â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ temp, fuel, cpi, unemp    â”‚                                      â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                           â”‚                   â”‚                            â”‚\n",
    "â”‚                           â–¼                   â–¼                            â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚                      FEATURE SERVICES                               â”‚   â”‚\n",
    "â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚\n",
    "â”‚  â”‚  training_features             â”‚  inference_features                â”‚   â”‚\n",
    "â”‚  â”‚  22 features (incl. target)    â”‚  21 features (excl. target)        â”‚   â”‚\n",
    "â”‚  â”‚  â†’ Model training              â”‚  â†’ Real-time predictions           â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                                                                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Run `kubectl apply -k manifests/` first to deploy:\n",
    "- PostgreSQL (registry + online store)\n",
    "- RayCluster `feast-ray` (distributed processing)\n",
    "- Shared PVC (data storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Install Dependencies & Setup Auth\n",
    "\n",
    "Install required packages and configure CodeFlare SDK authentication for KubeRay access.\n",
    "\n",
    "**Why CodeFlare SDK?** It provides secure authentication to the KubeRay cluster using your Kubernetes service account token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"feast[postgres,ray]==0.59.0\" codeflare-sdk pandas pyarrow psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# Setup CodeFlare SDK auth from service account token (for KubeRay cluster access)\n",
    "token_path = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n",
    "if os.path.exists(token_path):\n",
    "    with open(token_path) as f:\n",
    "        os.environ[\"FEAST_RAY_AUTH_TOKEN\"] = f.read().strip()\n",
    "    k8s_host = os.environ.get(\"KUBERNETES_SERVICE_HOST\", \"\")\n",
    "    k8s_port = os.environ.get(\"KUBERNETES_SERVICE_PORT\", \"443\")\n",
    "    if k8s_host:\n",
    "        os.environ[\"FEAST_RAY_AUTH_SERVER\"] = f\"https://{k8s_host}:{k8s_port}\"\n",
    "    os.environ[\"FEAST_RAY_SKIP_TLS\"] = \"true\"\n",
    "    print(\"ğŸ” CodeFlare SDK auth configured for KubeRay access\")\n",
    "else:\n",
    "    print(\"âš ï¸ Not running in cluster - some features may not work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Configuration\n",
    "\n",
    "Define paths and data generation parameters.\n",
    "\n",
    "| Variable | Value | Purpose |\n",
    "|----------|-------|---------|\n",
    "| `SHARED_ROOT` | `/opt/app-root/src/shared` | PVC mount point in RHOAI workbench |\n",
    "| `WEEKS` | 104 | 2 years of historical data |\n",
    "| `STORES Ã— DEPTS` | 45 Ã— 14 | 630 unique store-department combinations |\n",
    "| **Total Records** | **65,520** | Weekly granularity (104 Ã— 45 Ã— 14) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PVC mounted at /opt/app-root/src/shared in RHOAI workbench\n",
    "SHARED_ROOT = Path(\"/opt/app-root/src/shared\")\n",
    "FEATURE_REPO = SHARED_ROOT / \"feature_repo\"\n",
    "DATA_DIR = SHARED_ROOT / \"data\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Data generation parameters\n",
    "START_DATE = \"2022-01-01\"\n",
    "WEEKS = 104      # 2 years\n",
    "STORES = 45      # Walmart-style store count\n",
    "DEPTS = 14       # Department count\n",
    "SEED = 42        # Reproducibility\n",
    "\n",
    "print(f\"ğŸ“Š Configuration:\")\n",
    "print(f\"   Data dir: {DATA_DIR}\")\n",
    "print(f\"   Feature repo: {FEATURE_REPO}\")\n",
    "print(f\"   Total records: {WEEKS * STORES * DEPTS:,}\")\n",
    "print(f\"   Date range: {START_DATE} to {(datetime.fromisoformat(START_DATE) + timedelta(weeks=WEEKS-1)).strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Generate Synthetic Sales Data\n",
    "\n",
    "Creates **Walmart-style retail data** with realistic patterns:\n",
    "\n",
    "| Feature | Logic | Purpose |\n",
    "|---------|-------|---------|\n",
    "| `weekly_sales` | Base Ã— Store Ã— Dept Ã— Season Ã— Holiday | Target variable |\n",
    "| `is_holiday` | Weeks 6, 27, 36, 47, 51 | Super Bowl, July 4th, Labor Day, Thanksgiving, Christmas |\n",
    "| `seasonal` | `sin(2Ï€ Ã— week/52)` | Summer peak, winter dip |\n",
    "| `temperature` | `60 + 20Ã—sin()` | Weather correlation |\n",
    "| `fuel_price`, `cpi` | Random walk | Economic indicators |\n",
    "\n",
    "### Why This Matters\n",
    "Real retail sales follow these patterns:\n",
    "- **Seasonality**: Summer/back-to-school peaks, post-holiday dips\n",
    "- **Holiday spikes**: 50%+ increase during major holidays\n",
    "- **Store/dept variation**: Larger stores with certain departments sell more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "base_date = datetime.fromisoformat(START_DATE).replace(tzinfo=timezone.utc)\n",
    "\n",
    "# Major US retail holiday weeks\n",
    "HOLIDAYS = {6, 27, 36, 47, 51}  # Super Bowl, July 4th, Labor Day, Thanksgiving, Christmas\n",
    "HOLIDAY_WEEKS = sorted(HOLIDAYS)\n",
    "\n",
    "records = []\n",
    "for week in range(WEEKS):\n",
    "    dt = base_date + timedelta(weeks=week)\n",
    "    woy, month = dt.isocalendar()[1], dt.month\n",
    "    day = dt.day\n",
    "    week_of_month = (day - 1) // 7 + 1\n",
    "    next_week = dt + timedelta(weeks=1)\n",
    "    is_month_end = 1 if next_week.month != month else 0\n",
    "    days_to_holiday = min([abs((h - woy) % 52) * 7 for h in HOLIDAY_WEEKS])\n",
    "    \n",
    "    # Seasonal pattern (peaks in summer, dips in winter)\n",
    "    seasonal = 1 + 0.3 * np.sin(2 * np.pi * woy / 52)\n",
    "    \n",
    "    for s in range(1, STORES + 1):\n",
    "        for d in range(1, DEPTS + 1):\n",
    "            # Sales = Base Ã— Store factor Ã— Dept factor Ã— Seasonal Ã— Holiday boost + noise\n",
    "            base_sales = 50000 + s * 5000  # Larger store IDs = larger stores\n",
    "            dept_factor = 0.5 + d * 0.2    # Higher dept IDs = more sales\n",
    "            holiday_boost = 1.5 if woy in HOLIDAYS else 1.0\n",
    "            noise = np.random.normal(0, 2000)\n",
    "            \n",
    "            sales = max(0, base_sales * dept_factor * seasonal * holiday_boost + noise)\n",
    "            \n",
    "            records.append({\n",
    "                \"store_id\": s, \"dept_id\": d, \"event_timestamp\": dt, \"weekly_sales\": round(sales, 2),\n",
    "                \"week_of_year\": woy, \"month\": month, \"quarter\": (month-1)//3+1, \n",
    "                \"week_of_month\": week_of_month, \"is_month_end\": is_month_end,\n",
    "                \"is_holiday\": int(woy in HOLIDAYS), \"days_to_holiday\": days_to_holiday,\n",
    "                \"temperature\": round(60 + 20*np.sin(2*np.pi*woy/52) + np.random.normal(0,5), 1),\n",
    "                \"fuel_price\": round(3 + 0.5*np.random.random(), 2), \n",
    "                \"cpi\": round(220 + week*0.1, 1), \n",
    "                \"unemployment\": round(5 + np.random.normal(0, 0.5), 1)\n",
    "            })\n",
    "\n",
    "# Sort by entity keys + timestamp (important for efficient PIT joins)\n",
    "sales_df = pd.DataFrame(records).sort_values([\"store_id\", \"dept_id\", \"event_timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… Generated {len(sales_df):,} rows\")\n",
    "print(f\"   Date range: {sales_df['event_timestamp'].min().date()} to {sales_df['event_timestamp'].max().date()}\")\n",
    "print(f\"   Stores: {sales_df['store_id'].nunique()}, Departments: {sales_df['dept_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š SAMPLE DATA: Raw sales data (before feature engineering)\n",
    "print(\"ğŸ“Š Sample: Raw sales data (first 5 rows)\")\n",
    "print(f\"   Columns: {list(sales_df.columns)}\")\n",
    "print()\n",
    "sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š SAMPLE DATA: Sales statistics\n",
    "print(\"ğŸ“Š Sales Distribution Statistics:\")\n",
    "print(sales_df[['weekly_sales', 'temperature', 'fuel_price', 'cpi', 'unemployment']].describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Feature Engineering\n",
    "\n",
    "Add **time-series features** that capture historical patterns. These are the most predictive features for demand forecasting.\n",
    "\n",
    "### Feature Importance Breakdown\n",
    "\n",
    "| Feature Group | Importance | Features | Why |\n",
    "|---------------|------------|----------|-----|\n",
    "| **Lag Features** | 35% | `lag_1`, `lag_2`, `lag_4`, `lag_8` | Recent history is most predictive |\n",
    "| **Rolling Stats** | 28% | `rolling_mean_4w`, `rolling_std_4w`, `sales_vs_avg` | Trend and volatility |\n",
    "| **Temporal** | 18% | `week_of_year`, `month`, `quarter` | Seasonality patterns |\n",
    "| **Holiday** | 10% | `is_holiday`, `days_to_holiday` | Holiday effects |\n",
    "| **Economic** | 7% | `temperature`, `fuel_price`, `cpi`, `unemployment` | External factors |\n",
    "| **Store** | 2% | `store_type`, `store_size`, `region` | Store context |\n",
    "\n",
    "### Key Insight\n",
    "**63% of predictive power comes from lag and rolling features** - this is why time-series feature engineering is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag features (most predictive - 35% importance)\n",
    "# These capture \"what happened recently\" which strongly predicts \"what will happen next\"\n",
    "for lag in [1, 2, 4, 8]:\n",
    "    sales_df[f\"lag_{lag}\"] = sales_df.groupby([\"store_id\", \"dept_id\"])[\"weekly_sales\"].shift(lag)\n",
    "\n",
    "# Rolling statistics (28% importance)\n",
    "# These capture trend (mean) and volatility (std)\n",
    "g = sales_df.groupby([\"store_id\", \"dept_id\"])[\"weekly_sales\"]\n",
    "sales_df[\"rolling_mean_4w\"] = g.transform(lambda x: x.rolling(4, min_periods=1).mean())\n",
    "sales_df[\"rolling_std_4w\"] = g.transform(lambda x: x.rolling(4, min_periods=2).std()).fillna(0)\n",
    "sales_df[\"sales_vs_avg\"] = (sales_df[\"weekly_sales\"] / sales_df[\"rolling_mean_4w\"].replace(0, 1)).fillna(1)\n",
    "\n",
    "# Fill NaN lags with rolling mean (more realistic than 0)\n",
    "for lag in [1, 2, 4, 8]:\n",
    "    sales_df[f\"lag_{lag}\"] = sales_df[f\"lag_{lag}\"].fillna(sales_df[\"rolling_mean_4w\"])\n",
    "sales_df = sales_df.fillna(0)\n",
    "\n",
    "print(f\"âœ… Features engineered: {len(sales_df.columns)} columns\")\n",
    "print(f\"   New features: lag_1, lag_2, lag_4, lag_8, rolling_mean_4w, rolling_std_4w, sales_vs_avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š SAMPLE DATA: After feature engineering (show lag and rolling features)\n",
    "print(\"ğŸ“Š Sample: Engineered features for Store 1, Dept 1\")\n",
    "print(\"   Notice how lag_1 = previous week's sales, rolling_mean_4w smooths the trend\")\n",
    "print()\n",
    "feature_cols = ['event_timestamp', 'weekly_sales', 'lag_1', 'lag_2', 'rolling_mean_4w', 'rolling_std_4w', 'sales_vs_avg']\n",
    "sales_df[(sales_df['store_id'] == 1) & (sales_df['dept_id'] == 1)][feature_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š SAMPLE DATA: Feature correlation with target\n",
    "print(\"ğŸ“Š Feature Correlation with weekly_sales:\")\n",
    "print(\"   (Higher = more predictive)\")\n",
    "print()\n",
    "numeric_cols = ['lag_1', 'lag_2', 'lag_4', 'lag_8', 'rolling_mean_4w', 'rolling_std_4w', \n",
    "                'week_of_year', 'is_holiday', 'temperature']\n",
    "correlations = sales_df[numeric_cols + ['weekly_sales']].corr()['weekly_sales'].drop('weekly_sales').sort_values(ascending=False)\n",
    "print(correlations.round(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Save to Parquet\n",
    "\n",
    "Save feature data to the shared PVC for Feast to read.\n",
    "\n",
    "```\n",
    "/opt/app-root/src/shared/data/\n",
    "â”œâ”€â”€ sales_features.parquet   # 65K rows, 22 cols - time-series features\n",
    "â””â”€â”€ store_features.parquet   # 630 rows - static store metadata\n",
    "```\n",
    "\n",
    "### Why Parquet?\n",
    "- **Columnar format**: Fast for analytical queries\n",
    "- **Compression**: 10x smaller than CSV\n",
    "- **Type preservation**: No type inference issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sales features\n",
    "sales_df.to_parquet(DATA_DIR / \"sales_features.parquet\", index=False)\n",
    "print(f\"âœ… Saved: {DATA_DIR / 'sales_features.parquet'}\")\n",
    "print(f\"   Shape: {sales_df.shape}\")\n",
    "print(f\"   Size: {(DATA_DIR / 'sales_features.parquet').stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Create and save store features (static metadata)\n",
    "stores = pd.DataFrame([\n",
    "    {\n",
    "        \"store_id\": s, \"dept_id\": d, \"event_timestamp\": base_date,\n",
    "        \"store_type\": [\"A\", \"B\", \"C\"][s % 3],  # A=small, B=medium, C=large\n",
    "        \"store_size\": 100000 + s * 10000,       # Square footage\n",
    "        \"region\": f\"region_{(s - 1) // 15 + 1}\" # 3 regions\n",
    "    }\n",
    "    for s in range(1, STORES + 1) for d in range(1, DEPTS + 1)\n",
    "])\n",
    "stores.to_parquet(DATA_DIR / \"store_features.parquet\", index=False)\n",
    "print(f\"\\nâœ… Saved: {DATA_DIR / 'store_features.parquet'}\")\n",
    "print(f\"   Shape: {stores.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š SAMPLE DATA: Store features\n",
    "print(\"ğŸ“Š Sample: Store features (first 5 rows)\")\n",
    "print(\"   Store types: A=small, B=medium, C=large\")\n",
    "print()\n",
    "stores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š SAMPLE DATA: Store type distribution\n",
    "print(\"ğŸ“Š Store Type Distribution:\")\n",
    "print(stores.groupby('store_type').agg({'store_id': 'nunique', 'store_size': 'mean'}).round(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Setup Feast Repository\n",
    "\n",
    "Copy feature definitions to the shared PVC. These files define how Feast manages features.\n",
    "\n",
    "| File | Purpose |\n",
    "|------|---------|\n",
    "| `feature_store.yaml` | Ray-enabled config (KubeRay + CodeFlare SDK auth) |\n",
    "| `features.py` | FeatureViews, Entities, FeatureServices + auto-auth |\n",
    "\n",
    "### Key Feature Services\n",
    "\n",
    "| Service | Features | Use Case |\n",
    "|---------|----------|----------|\n",
    "| `training_features` | 22 features (incl. target) | Model training |\n",
    "| `inference_features` | 21 features (excl. target) | Real-time serving |\n",
    "\n",
    "### Auto-Auth\n",
    "`features.py` automatically reads the service account token from `/var/run/secrets/kubernetes.io/serviceaccount/token` for CodeFlare SDK authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_REPO.mkdir(parents=True, exist_ok=True)\n",
    "(DATA_DIR / \"ray_storage\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Look for feature_repo in multiple possible locations\n",
    "possible_paths = [\n",
    "    Path(\"/opt/app-root/src/feature_repo\"),\n",
    "    Path(\"/opt/app-root/src/sales-demand-forecasting/feature_repo\"),\n",
    "    Path(\"../feature_repo\"),\n",
    "]\n",
    "\n",
    "src_dir = None\n",
    "for p in possible_paths:\n",
    "    if p.exists() and (p / \"features.py\").exists():\n",
    "        src_dir = p\n",
    "        print(f\"ğŸ“ Found feature_repo at: {src_dir}\")\n",
    "        break\n",
    "\n",
    "if src_dir is None:\n",
    "    raise FileNotFoundError(f\"feature_repo not found in: {possible_paths}\")\n",
    "\n",
    "# Copy features.py (includes auto-auth for CodeFlare SDK)\n",
    "shutil.copy(src_dir / \"features.py\", FEATURE_REPO / \"features.py\")\n",
    "print(\"âœ… Copied: features.py (with CodeFlare SDK auto-auth)\")\n",
    "\n",
    "# Use Ray config as main feature_store.yaml\n",
    "shutil.copy(src_dir / \"feature_store_ray.yaml\", FEATURE_REPO / \"feature_store.yaml\")\n",
    "print(\"âœ… Copied: feature_store.yaml (Ray + KubeRay enabled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š SAMPLE DATA: Show feature_store.yaml config\n",
    "print(\"ğŸ“Š Feast Config (feature_store.yaml):\")\n",
    "print(\"-\" * 60)\n",
    "with open(FEATURE_REPO / \"feature_store.yaml\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Feast Apply (via Remote KubeRay Cluster)\n",
    "\n",
    "Register feature definitions in PostgreSQL using the deployed Ray cluster.\n",
    "\n",
    "```\n",
    "feast apply\n",
    "    â”‚\n",
    "    â”œâ”€â”€ Reads features.py (auto-auth configures CodeFlare SDK)\n",
    "    â”œâ”€â”€ Connects to KubeRay cluster \"feast-ray\" via CodeFlare SDK\n",
    "    â”œâ”€â”€ Uses mTLS for secure communication\n",
    "    â””â”€â”€ Creates tables in PostgreSQL registry\n",
    "```\n",
    "\n",
    "### What Gets Created\n",
    "- **Entities**: `store_id`, `dept_id`\n",
    "- **Feature Views**: `sales_features`, `store_features`\n",
    "- **Feature Services**: `training_features`, `inference_features`\n",
    "\n",
    "**Note:** The Ray cluster must be running. Deploy with `kubectl apply -f manifests/03-raycluster.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(str(FEATURE_REPO))\n",
    "print(f\"ğŸ“ Working dir: {os.getcwd()}\")\n",
    "print(\"\\nğŸš€ Running: feast apply\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run([\"feast\", \"apply\"], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print(f\"âŒ ERROR: {result.stderr}\")\n",
    "else:\n",
    "    print(\"âœ… Features registered to PostgreSQL registry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Feast Materialize (via Remote KubeRay Cluster)\n",
    "\n",
    "Populate the **online store** using the deployed Ray cluster for distributed processing.\n",
    "\n",
    "```\n",
    "Offline Store (Parquet)     KubeRay Cluster     Online Store (PostgreSQL)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Full history       â”‚â”€â”€â”€â–¶â”‚  Distributed  â”‚â”€â”€â”€â–¶â”‚ Latest values only â”‚\n",
    "â”‚ 65K rows           â”‚    â”‚  Processing   â”‚    â”‚ 630 entities       â”‚\n",
    "â”‚ For training       â”‚    â”‚ (feast-ray)   â”‚    â”‚ For serving (<50ms)â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Why Ray for Materialize?\n",
    "- **Distributed processing**: Work spread across KubeRay workers\n",
    "- **Faster for large datasets**: 10x+ faster than local for >1M rows\n",
    "- **Automatic parallelization**: Ray handles data partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "print(f\"ğŸš€ Running: feast materialize {START_DATE}T00:00:00 {end_ts}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"feast\", \"materialize\", f\"{START_DATE}T00:00:00\", end_ts],\n",
    "    capture_output=True, text=True, cwd=str(FEATURE_REPO)\n",
    ")\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print(f\"âŒ ERROR: {result.stderr}\")\n",
    "else:\n",
    "    print(\"âœ… Features materialized to PostgreSQL online store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Verify Setup\n",
    "\n",
    "Test feature retrieval to ensure everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "\n",
    "store = FeatureStore(repo_path=str(FEATURE_REPO))\n",
    "\n",
    "print(\"ğŸ“‹ Registered Objects:\")\n",
    "print(f\"   Entities: {[e.name for e in store.list_entities()]}\")\n",
    "print(f\"   FeatureViews: {[fv.name for fv in store.list_feature_views()]}\")\n",
    "print(f\"   FeatureServices: {[fs.name for fs in store.list_feature_services()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š SAMPLE DATA: Online feature lookup (what serving will use)\n",
    "print(\"ğŸ“Š Sample: Online Feature Lookup for Store 1, Dept 1\")\n",
    "print(\"   This is what KServe will use for real-time predictions (<50ms)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "online_features = store.get_online_features(\n",
    "    features=[\n",
    "        \"sales_features:weekly_sales\",\n",
    "        \"sales_features:lag_1\",\n",
    "        \"sales_features:rolling_mean_4w\",\n",
    "        \"sales_features:is_holiday\",\n",
    "        \"store_features:store_type\",\n",
    "        \"store_features:store_size\",\n",
    "    ],\n",
    "    entity_rows=[{\"store_id\": 1, \"dept_id\": 1}]\n",
    ").to_dict()\n",
    "\n",
    "for k, v in online_features.items():\n",
    "    print(f\"   {k}: {v[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š SAMPLE DATA: Online features for multiple entities\n",
    "print(\"ğŸ“Š Sample: Online Features for Multiple Stores\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "entities = [\n",
    "    {\"store_id\": 1, \"dept_id\": 1},\n",
    "    {\"store_id\": 10, \"dept_id\": 5},\n",
    "    {\"store_id\": 25, \"dept_id\": 10},\n",
    "    {\"store_id\": 45, \"dept_id\": 14},\n",
    "]\n",
    "\n",
    "multi_features = store.get_online_features(\n",
    "    features=[\"sales_features:weekly_sales\", \"sales_features:lag_1\", \"store_features:store_type\"],\n",
    "    entity_rows=entities\n",
    ").to_df()\n",
    "\n",
    "multi_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š SAMPLE DATA: Historical features (what training will use via remote Ray)\n",
    "print(\"ğŸ“Š Sample: Historical Feature Retrieval via Remote KubeRay\")\n",
    "print(\"   This uses get_historical_features() which distributes PIT joins across KubeRay cluster\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Small entity DataFrame for demo\n",
    "entity_df = pd.DataFrame([\n",
    "    {\"store_id\": 1, \"dept_id\": 1, \"event_timestamp\": datetime(2023, 6, 1, tzinfo=timezone.utc)},\n",
    "    {\"store_id\": 1, \"dept_id\": 1, \"event_timestamp\": datetime(2023, 6, 15, tzinfo=timezone.utc)},\n",
    "    {\"store_id\": 10, \"dept_id\": 5, \"event_timestamp\": datetime(2023, 6, 1, tzinfo=timezone.utc)},\n",
    "    {\"store_id\": 25, \"dept_id\": 10, \"event_timestamp\": datetime(2023, 7, 1, tzinfo=timezone.utc)},\n",
    "])\n",
    "\n",
    "historical = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\"sales_features:weekly_sales\", \"sales_features:lag_1\", \"sales_features:rolling_mean_4w\", \"store_features:store_type\"]\n",
    ").to_df()\n",
    "\n",
    "print(f\"âœ… Retrieved {len(historical)} rows with {len(historical.columns)} columns\")\n",
    "print(f\"   Columns: {list(historical.columns)}\")\n",
    "print()\n",
    "historical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## âœ… Complete!\n",
    "\n",
    "### What We Built\n",
    "\n",
    "| Component | Count | Location |\n",
    "|-----------|-------|---------|\n",
    "| Sales records | 65,520 | `/shared/data/sales_features.parquet` |\n",
    "| Store records | 630 | `/shared/data/store_features.parquet` |\n",
    "| Features | 22 | Lags, rolling stats, temporal, economic |\n",
    "| Registry | PostgreSQL | Feature metadata |\n",
    "| Online Store | PostgreSQL | Latest values for serving |\n",
    "\n",
    "### Feature Importance Summary\n",
    "\n",
    "| Feature Group | Importance | Examples |\n",
    "|---------------|------------|----------|\n",
    "| **Lag features** | 35% | `lag_1`, `lag_2`, `lag_4`, `lag_8` |\n",
    "| **Rolling stats** | 28% | `rolling_mean_4w`, `rolling_std_4w` |\n",
    "| **Temporal** | 18% | `week_of_year`, `month`, `quarter` |\n",
    "| **Holiday** | 10% | `is_holiday`, `days_to_holiday` |\n",
    "| **Economic** | 7% | `temperature`, `fuel_price`, `cpi` |\n",
    "| **Store** | 2% | `store_type`, `store_size` |\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| `KeyError: 'auth_token'` | Ensure CodeFlare SDK env vars are set (Step 0) |\n",
    "| `FileNotFoundError` for parquet | Check DATA_DIR path and PVC mount |\n",
    "| Ray connection timeout | Verify RayCluster is running: `kubectl get raycluster -n feast-trainer-demo` |\n",
    "| PostgreSQL connection error | Check PostgreSQL pod: `kubectl get pods -n feast-trainer-demo` |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Option A: Use Manifests (Recommended for Production)**\n",
    "```bash\n",
    "kubectl apply -f manifests/05-dataprep-job.yaml   # Regenerate data\n",
    "kubectl apply -f manifests/06-trainjob.yaml       # Train model (multi-node DDP)\n",
    "```\n",
    "\n",
    "**Option B: Use Notebooks (Interactive Development)**\n",
    "- `02-training.ipynb` â†’ Train model with `get_historical_features()`\n",
    "- `03-inference.ipynb` â†’ Deploy model with KServe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
