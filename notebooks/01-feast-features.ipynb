{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 - Feast Feature Store Setup\n",
        "\n",
        "![Workflow](../docs/01-features-workflow.png)\n",
        "\n",
        "## What This Notebook Does\n",
        "\n",
        "| Step | Action | Output |\n",
        "|------|--------|--------|\n",
        "| 1 | Generate synthetic sales data | `sales_features.parquet` |\n",
        "| 2 | Engineer lag/rolling features | 22 total features |\n",
        "| 3 | `feast apply` | Register features in PostgreSQL registry |\n",
        "| 4 | `feast materialize` | Populate online store for serving |\n",
        "\n",
        "## Why Feast?\n",
        "\n",
        "```\n",
        "Without Feast:                    With Feast:\n",
        "┌─────────┐                       ┌─────────┐\n",
        "│Training │ ← manual features     │Training │ ← FeatureService\n",
        "└─────────┘                       └─────────┘\n",
        "┌─────────┐                       ┌─────────┐\n",
        "│ Serving │ ← duplicate logic     │ Serving │ ← Same FeatureService\n",
        "└─────────┘   (SKEW RISK!)        └─────────┘   (CONSISTENT)\n",
        "```\n",
        "\n",
        "**Prerequisites:** PostgreSQL, PVC (`shared`), RayCluster running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q \"feast[postgres]==0.59.0\" pandas pyarrow psycopg2-binary\n",
        "import os, shutil, subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta, timezone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "| Variable | Value | Purpose |\n",
        "|----------|-------|----------|\n",
        "| `SHARED_ROOT` | `/opt/app-root/src/shared` | PVC mount point |\n",
        "| `WEEKS` | 104 | 2 years of data |\n",
        "| `STORES × DEPTS` | 45 × 14 | 630 unique entities |\n",
        "| **Total Records** | **65,520** | Weekly granularity |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PVC mounted at /opt/app-root/src/shared in RHOAI workbench\n",
        "SHARED_ROOT = Path(\"/opt/app-root/src/shared\")\n",
        "FEATURE_REPO = SHARED_ROOT / \"feature_repo\"\n",
        "DATA_DIR = SHARED_ROOT / \"data\"\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "START_DATE, WEEKS, STORES, DEPTS, SEED = \"2022-01-01\", 104, 45, 14, 42\n",
        "print(f\"Records: {WEEKS * STORES * DEPTS:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Synthetic Sales Data\n",
        "\n",
        "Creates Walmart-style retail data with realistic patterns:\n",
        "\n",
        "| Feature | Logic | Purpose |\n",
        "|---------|-------|----------|\n",
        "| `weekly_sales` | Base × Store × Dept × Season × Holiday | Target variable |\n",
        "| `is_holiday` | Weeks 6,27,36,47,51 | Super Bowl, Memorial Day, etc. |\n",
        "| `seasonal` | `sin(2π × week/52)` | Summer peak, winter dip |\n",
        "| `temperature` | `60 + 20×sin()` | Weather correlation |\n",
        "| `fuel_price`, `cpi` | Random walk | Economic indicators |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(SEED)\n",
        "base_date = datetime.fromisoformat(START_DATE).replace(tzinfo=timezone.utc)\n",
        "HOLIDAYS = {6, 27, 36, 47, 51}  # Major holiday weeks\n",
        "HOLIDAY_WEEKS = sorted(HOLIDAYS)\n",
        "\n",
        "records = []\n",
        "for week in range(WEEKS):\n",
        "    dt = base_date + timedelta(weeks=week)\n",
        "    woy, month = dt.isocalendar()[1], dt.month\n",
        "    day = dt.day\n",
        "    week_of_month = (day - 1) // 7 + 1\n",
        "    # Check if last week of month\n",
        "    next_week = dt + timedelta(weeks=1)\n",
        "    is_month_end = 1 if next_week.month != month else 0\n",
        "    # Days to next holiday\n",
        "    days_to_holiday = min([abs((h - woy) % 52) * 7 for h in HOLIDAY_WEEKS])\n",
        "    \n",
        "    seasonal = 1 + 0.3 * np.sin(2 * np.pi * woy / 52)\n",
        "    for s in range(1, STORES + 1):\n",
        "        for d in range(1, DEPTS + 1):\n",
        "            sales = max(0, (50000 + s*5000) * (0.5 + d*0.2) * seasonal * (1.5 if woy in HOLIDAYS else 1) + np.random.normal(0, 2000))\n",
        "            records.append({\n",
        "                \"store_id\": s, \"dept_id\": d, \"event_timestamp\": dt, \"weekly_sales\": round(sales, 2),\n",
        "                \"week_of_year\": woy, \"month\": month, \"quarter\": (month-1)//3+1, \n",
        "                \"week_of_month\": week_of_month, \"is_month_end\": is_month_end,\n",
        "                \"is_holiday\": int(woy in HOLIDAYS), \"days_to_holiday\": days_to_holiday,\n",
        "                \"temperature\": round(60 + 20*np.sin(2*np.pi*woy/52) + np.random.normal(0,5), 1),\n",
        "                \"fuel_price\": round(3 + 0.5*np.random.random(), 2), \"cpi\": round(220 + week*0.1, 1), \n",
        "                \"unemployment\": round(5 + np.random.normal(0, 0.5), 1)\n",
        "            })\n",
        "\n",
        "sales_df = pd.DataFrame(records).sort_values([\"store_id\", \"dept_id\", \"event_timestamp\"]).reset_index(drop=True)\n",
        "print(f\"Generated {len(sales_df):,} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering\n",
        "\n",
        "Add time-series features that capture historical patterns:\n",
        "\n",
        "```\n",
        "lag_1:  Sales from 1 week ago  → Most predictive\n",
        "lag_2:  Sales from 2 weeks ago → Recent trend\n",
        "lag_4:  Sales from 4 weeks ago → Monthly pattern\n",
        "lag_8:  Sales from 8 weeks ago → Bi-monthly pattern\n",
        "\n",
        "rolling_mean_4w:  4-week moving average → Smoothed trend\n",
        "rolling_std_4w:   4-week std deviation → Volatility\n",
        "sales_vs_avg:     Current / Average    → Relative performance\n",
        "```\n",
        "\n",
        "**Note:** Lags are filled with `rolling_mean_4w` to avoid NaN at start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lag + rolling features\n",
        "for lag in [1, 2, 4, 8]:\n",
        "    sales_df[f\"lag_{lag}\"] = sales_df.groupby([\"store_id\", \"dept_id\"])[\"weekly_sales\"].shift(lag)\n",
        "\n",
        "g = sales_df.groupby([\"store_id\", \"dept_id\"])[\"weekly_sales\"]\n",
        "sales_df[\"rolling_mean_4w\"] = g.transform(lambda x: x.rolling(4, min_periods=1).mean())\n",
        "sales_df[\"rolling_std_4w\"] = g.transform(lambda x: x.rolling(4, min_periods=2).std()).fillna(0)\n",
        "sales_df[\"sales_vs_avg\"] = (sales_df[\"weekly_sales\"] / sales_df[\"rolling_mean_4w\"].replace(0, 1)).fillna(1)\n",
        "\n",
        "for lag in [1, 2, 4, 8]:\n",
        "    sales_df[f\"lag_{lag}\"] = sales_df[f\"lag_{lag}\"].fillna(sales_df[\"rolling_mean_4w\"])\n",
        "sales_df = sales_df.fillna(0)\n",
        "print(f\"Features: {len(sales_df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save to Parquet\n",
        "\n",
        "Save feature data to PVC for Feast to read:\n",
        "\n",
        "```\n",
        "/opt/app-root/src/shared/\n",
        "└── data/\n",
        "    ├── sales_features.parquet   # 65K rows, 22 cols\n",
        "    └── store_features.parquet   # Store metadata\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save\n",
        "sales_df.to_parquet(DATA_DIR / \"sales_features.parquet\", index=False)\n",
        "stores = pd.DataFrame([{\"store_id\": s, \"dept_id\": d, \"event_timestamp\": base_date, \"store_type\": [\"A\",\"B\",\"C\"][s%3], \"store_size\": 100000+s*10000, \"region\": f\"region_{(s-1)//15+1}\"} for s in range(1, STORES+1) for d in range(1, DEPTS+1)])\n",
        "stores.to_parquet(DATA_DIR / \"store_features.parquet\", index=False)\n",
        "print(f\"✅ Saved to {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Feast Repository\n",
        "\n",
        "Copy feature definitions to the shared PVC:\n",
        "\n",
        "| File | Purpose |\n",
        "|------|----------|\n",
        "| `feature_store.yaml` | Registry (PostgreSQL), offline/online stores |\n",
        "| `feature_store_ray.yaml` | Ray-enabled config for distributed retrieval |\n",
        "| `features.py` | FeatureViews, Entities, FeatureServices |\n",
        "\n",
        "**Key FeatureServices defined:**\n",
        "- `training_features` → All features for model training\n",
        "- `inference_features` → Subset for real-time serving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FEATURE_REPO.mkdir(parents=True, exist_ok=True)\n",
        "# Flat structure: feature_repo/ is in same directory as notebook\n",
        "src_dir = Path(\"/opt/app-root/src/feature_repo\")\n",
        "for f in [\"feature_store.yaml\", \"feature_store_ray.yaml\", \"features.py\"]:\n",
        "    src = src_dir / f\n",
        "    if src.exists():\n",
        "        shutil.copy(src, FEATURE_REPO / f)\n",
        "        print(f\"✅ {f}\")\n",
        "    else:\n",
        "        print(f\"❌ Missing: {src}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feast Apply\n",
        "\n",
        "Register feature definitions in PostgreSQL:\n",
        "\n",
        "```\n",
        "feast apply\n",
        "    │\n",
        "    ├── Reads features.py\n",
        "    ├── Creates tables in PostgreSQL registry\n",
        "    └── Stores feature metadata (schemas, timestamps)\n",
        "```\n",
        "\n",
        "**Output:** FeatureViews (`store_features`, `sales_features`) registered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(str(FEATURE_REPO))\n",
        "result = subprocess.run([\"feast\", \"apply\"], capture_output=True, text=True)\n",
        "print(result.stdout)\n",
        "if result.returncode != 0:\n",
        "    print(f\"ERROR: {result.stderr}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feast Materialize\n",
        "\n",
        "Populate the **online store** for low-latency serving:\n",
        "\n",
        "```\n",
        "Offline Store (Parquet)          Online Store (PostgreSQL)\n",
        "┌────────────────────┐           ┌────────────────────┐\n",
        "│ Full history       │  ──────▶  │ Latest values only │\n",
        "│ 65K rows           │ materialize│ 630 entities       │\n",
        "│ For training       │           │ For serving        │\n",
        "└────────────────────┘           └────────────────────┘\n",
        "```\n",
        "\n",
        "**Why:** Training needs history; serving needs current values fast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "end_ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
        "result = subprocess.run([\"feast\", \"materialize\", f\"{START_DATE}T00:00:00\", end_ts], capture_output=True, text=True, cwd=str(FEATURE_REPO))\n",
        "print(result.stdout)\n",
        "if result.returncode != 0:\n",
        "    print(f\"ERROR: {result.stderr}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify Setup\n",
        "\n",
        "Test online feature retrieval (what serving will use):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feast import FeatureStore\n",
        "store = FeatureStore(repo_path=str(FEATURE_REPO))\n",
        "print(f\"Views: {[fv.name for fv in store.list_feature_views()]}\")\n",
        "print(f\"Services: {[fs.name for fs in store.list_feature_services()]}\")\n",
        "\n",
        "features = store.get_online_features(features=[\"sales_features:weekly_sales\", \"sales_features:lag_1\"], entity_rows=[{\"store_id\": 1, \"dept_id\": 1}]).to_dict()\n",
        "print(f\"\\n✅ Online: {features}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## ✅ Complete!\n",
        "\n",
        "**What we built:**\n",
        "- 65K rows of synthetic sales data\n",
        "- 22 engineered features (lags, rolling stats)\n",
        "- Feast registry in PostgreSQL\n",
        "- Online store populated for serving\n",
        "\n",
        "**Next:** `02-training.ipynb` → Train model with distributed feature retrieval"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
