{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 - Feast Feature Store Setup\n",
        "\n",
        "![Workflow](../docs/01-features-workflow.png)\n",
        "\n",
        "## What This Notebook Does\n",
        "\n",
        "| Step | Action | Output |\n",
        "|------|--------|--------|\n",
        "| 1 | Generate synthetic sales data | `sales_features.parquet` |\n",
        "| 2 | Engineer lag/rolling features | 22 total features |\n",
        "| 3 | `feast apply` via Ray | Register features in PostgreSQL |\n",
        "| 4 | `feast materialize` via Ray | Populate online store |\n",
        "\n",
        "## Architecture (KubeRay + CodeFlare SDK)\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Notebook   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  KubeRay    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  PostgreSQL ‚îÇ\n",
        "‚îÇ  (Feast)    ‚îÇ     ‚îÇ  Cluster    ‚îÇ     ‚îÇ  Registry   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "       ‚îÇ                   ‚îÇ                   ‚îÇ\n",
        "       ‚îÇ CodeFlare SDK     ‚îÇ Distributed       ‚îÇ Online Store\n",
        "       ‚îÇ Auto-Auth         ‚îÇ Materialize       ‚îÇ (Low-latency)\n",
        "```\n",
        "\n",
        "**Prerequisites:** Run `kubectl apply -k manifests/` first to deploy:\n",
        "- PostgreSQL (registry + online store)\n",
        "- RayCluster `feast-ray`\n",
        "- Shared PVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install -q \"feast[postgres,ray]==0.59.0\" codeflare-sdk pandas pyarrow psycopg2-binary\n",
        "import os, shutil, subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "# Setup CodeFlare SDK auth from service account token (for Ray cluster access)\n",
        "token_path = \"/var/run/secrets/kubernetes.io/serviceaccount/token\"\n",
        "if os.path.exists(token_path):\n",
        "    with open(token_path) as f:\n",
        "        os.environ[\"FEAST_RAY_AUTH_TOKEN\"] = f.read().strip()\n",
        "    k8s_host = os.environ.get(\"KUBERNETES_SERVICE_HOST\", \"\")\n",
        "    k8s_port = os.environ.get(\"KUBERNETES_SERVICE_PORT\", \"443\")\n",
        "    if k8s_host:\n",
        "        os.environ[\"FEAST_RAY_AUTH_SERVER\"] = f\"https://{k8s_host}:{k8s_port}\"\n",
        "    os.environ[\"FEAST_RAY_SKIP_TLS\"] = \"true\"\n",
        "    print(\"üîê CodeFlare SDK auth configured for KubeRay access\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "| Variable | Value | Purpose |\n",
        "|----------|-------|----------|\n",
        "| `SHARED_ROOT` | `/opt/app-root/src/shared` | PVC mount point |\n",
        "| `WEEKS` | 104 | 2 years of data |\n",
        "| `STORES √ó DEPTS` | 45 √ó 14 | 630 unique entities |\n",
        "| **Total Records** | **65,520** | Weekly granularity |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# PVC mounted at /opt/app-root/src/shared in RHOAI workbench\n",
        "SHARED_ROOT = Path(\"/opt/app-root/src/shared\")\n",
        "FEATURE_REPO = SHARED_ROOT / \"feature_repo\"\n",
        "DATA_DIR = SHARED_ROOT / \"data\"\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "START_DATE, WEEKS, STORES, DEPTS, SEED = \"2022-01-01\", 104, 45, 14, 42\n",
        "print(f\"üìä Config:\")\n",
        "print(f\"   Data dir: {DATA_DIR}\")\n",
        "print(f\"   Feature repo: {FEATURE_REPO}\")\n",
        "print(f\"   Total records: {WEEKS * STORES * DEPTS:,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Generate Synthetic Sales Data\n",
        "\n",
        "Creates Walmart-style retail data with realistic patterns:\n",
        "\n",
        "| Feature | Logic | Purpose |\n",
        "|---------|-------|----------|\n",
        "| `weekly_sales` | Base √ó Store √ó Dept √ó Season √ó Holiday | Target variable |\n",
        "| `is_holiday` | Weeks 6,27,36,47,51 | Super Bowl, July 4th, etc. |\n",
        "| `seasonal` | `sin(2œÄ √ó week/52)` | Summer peak, winter dip |\n",
        "| `temperature` | `60 + 20√ósin()` | Weather correlation |\n",
        "| `fuel_price`, `cpi` | Random walk | Economic indicators |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.random.seed(SEED)\n",
        "base_date = datetime.fromisoformat(START_DATE).replace(tzinfo=timezone.utc)\n",
        "HOLIDAYS = {6, 27, 36, 47, 51}  # Major holiday weeks\n",
        "HOLIDAY_WEEKS = sorted(HOLIDAYS)\n",
        "\n",
        "records = []\n",
        "for week in range(WEEKS):\n",
        "    dt = base_date + timedelta(weeks=week)\n",
        "    woy, month = dt.isocalendar()[1], dt.month\n",
        "    day = dt.day\n",
        "    week_of_month = (day - 1) // 7 + 1\n",
        "    next_week = dt + timedelta(weeks=1)\n",
        "    is_month_end = 1 if next_week.month != month else 0\n",
        "    days_to_holiday = min([abs((h - woy) % 52) * 7 for h in HOLIDAY_WEEKS])\n",
        "    \n",
        "    seasonal = 1 + 0.3 * np.sin(2 * np.pi * woy / 52)\n",
        "    for s in range(1, STORES + 1):\n",
        "        for d in range(1, DEPTS + 1):\n",
        "            sales = max(0, (50000 + s*5000) * (0.5 + d*0.2) * seasonal * (1.5 if woy in HOLIDAYS else 1) + np.random.normal(0, 2000))\n",
        "            records.append({\n",
        "                \"store_id\": s, \"dept_id\": d, \"event_timestamp\": dt, \"weekly_sales\": round(sales, 2),\n",
        "                \"week_of_year\": woy, \"month\": month, \"quarter\": (month-1)//3+1, \n",
        "                \"week_of_month\": week_of_month, \"is_month_end\": is_month_end,\n",
        "                \"is_holiday\": int(woy in HOLIDAYS), \"days_to_holiday\": days_to_holiday,\n",
        "                \"temperature\": round(60 + 20*np.sin(2*np.pi*woy/52) + np.random.normal(0,5), 1),\n",
        "                \"fuel_price\": round(3 + 0.5*np.random.random(), 2), \"cpi\": round(220 + week*0.1, 1), \n",
        "                \"unemployment\": round(5 + np.random.normal(0, 0.5), 1)\n",
        "            })\n",
        "\n",
        "sales_df = pd.DataFrame(records).sort_values([\"store_id\", \"dept_id\", \"event_timestamp\"]).reset_index(drop=True)\n",
        "print(f\"‚úÖ Generated {len(sales_df):,} rows\")\n",
        "print(f\"   Date range: {sales_df['event_timestamp'].min().date()} to {sales_df['event_timestamp'].max().date()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üìä SAMPLE DATA: Raw sales data (before feature engineering)\n",
        "print(\"üìä Sample: Raw sales data (5 rows)\")\n",
        "print(f\"   Columns: {list(sales_df.columns)}\")\n",
        "print()\n",
        "sales_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üìä SAMPLE DATA: Sales statistics by store\n",
        "print(\"üìä Sample: Sales distribution\")\n",
        "print(sales_df[['weekly_sales', 'temperature', 'fuel_price', 'cpi']].describe().round(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Feature Engineering\n",
        "\n",
        "Add time-series features that capture historical patterns:\n",
        "\n",
        "```\n",
        "lag_1:  Sales from 1 week ago  ‚Üí Most predictive (35% importance)\n",
        "lag_2:  Sales from 2 weeks ago ‚Üí Recent trend\n",
        "lag_4:  Sales from 4 weeks ago ‚Üí Monthly pattern\n",
        "lag_8:  Sales from 8 weeks ago ‚Üí Bi-monthly pattern\n",
        "\n",
        "rolling_mean_4w:  4-week moving average ‚Üí Smoothed trend (28% importance)\n",
        "rolling_std_4w:   4-week std deviation ‚Üí Volatility\n",
        "sales_vs_avg:     Current / Average    ‚Üí Relative performance\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Lag features (most predictive - 35% importance)\n",
        "for lag in [1, 2, 4, 8]:\n",
        "    sales_df[f\"lag_{lag}\"] = sales_df.groupby([\"store_id\", \"dept_id\"])[\"weekly_sales\"].shift(lag)\n",
        "\n",
        "# Rolling statistics (28% importance)\n",
        "g = sales_df.groupby([\"store_id\", \"dept_id\"])[\"weekly_sales\"]\n",
        "sales_df[\"rolling_mean_4w\"] = g.transform(lambda x: x.rolling(4, min_periods=1).mean())\n",
        "sales_df[\"rolling_std_4w\"] = g.transform(lambda x: x.rolling(4, min_periods=2).std()).fillna(0)\n",
        "sales_df[\"sales_vs_avg\"] = (sales_df[\"weekly_sales\"] / sales_df[\"rolling_mean_4w\"].replace(0, 1)).fillna(1)\n",
        "\n",
        "# Fill NaN lags with rolling mean (more realistic than 0)\n",
        "for lag in [1, 2, 4, 8]:\n",
        "    sales_df[f\"lag_{lag}\"] = sales_df[f\"lag_{lag}\"].fillna(sales_df[\"rolling_mean_4w\"])\n",
        "sales_df = sales_df.fillna(0)\n",
        "\n",
        "print(f\"‚úÖ Features engineered: {len(sales_df.columns)} columns\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üìä SAMPLE DATA: After feature engineering (show lag and rolling features)\n",
        "print(\"üìä Sample: Engineered features for Store 1, Dept 1\")\n",
        "feature_cols = ['event_timestamp', 'weekly_sales', 'lag_1', 'lag_2', 'rolling_mean_4w', 'rolling_std_4w', 'sales_vs_avg']\n",
        "sales_df[(sales_df['store_id'] == 1) & (sales_df['dept_id'] == 1)][feature_cols].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üìä SAMPLE DATA: Feature correlation with target\n",
        "print(\"üìä Feature correlation with weekly_sales:\")\n",
        "numeric_cols = ['lag_1', 'lag_2', 'lag_4', 'lag_8', 'rolling_mean_4w', 'rolling_std_4w', \n",
        "                'week_of_year', 'is_holiday', 'temperature']\n",
        "correlations = sales_df[numeric_cols + ['weekly_sales']].corr()['weekly_sales'].drop('weekly_sales').sort_values(ascending=False)\n",
        "print(correlations.round(3).to_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Save to Parquet\n",
        "\n",
        "Save feature data to PVC for Feast to read:\n",
        "\n",
        "```\n",
        "/opt/app-root/src/shared/data/\n",
        "‚îú‚îÄ‚îÄ sales_features.parquet   # 65K rows, 22 cols\n",
        "‚îî‚îÄ‚îÄ store_features.parquet   # Store metadata\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save sales features\n",
        "sales_df.to_parquet(DATA_DIR / \"sales_features.parquet\", index=False)\n",
        "print(f\"‚úÖ Saved: {DATA_DIR / 'sales_features.parquet'}\")\n",
        "print(f\"   Shape: {sales_df.shape}\")\n",
        "\n",
        "# Create and save store features (static metadata)\n",
        "stores = pd.DataFrame([\n",
        "    {\n",
        "        \"store_id\": s, \"dept_id\": d, \"event_timestamp\": base_date,\n",
        "        \"store_type\": [\"A\", \"B\", \"C\"][s % 3],\n",
        "        \"store_size\": 100000 + s * 10000,\n",
        "        \"region\": f\"region_{(s - 1) // 15 + 1}\"\n",
        "    }\n",
        "    for s in range(1, STORES + 1) for d in range(1, DEPTS + 1)\n",
        "])\n",
        "stores.to_parquet(DATA_DIR / \"store_features.parquet\", index=False)\n",
        "print(f\"‚úÖ Saved: {DATA_DIR / 'store_features.parquet'}\")\n",
        "print(f\"   Shape: {stores.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üìä SAMPLE DATA: Store features\n",
        "print(\"üìä Sample: Store features (5 rows)\")\n",
        "stores.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üìä SAMPLE DATA: Store type distribution\n",
        "print(\"üìä Store type distribution:\")\n",
        "print(stores.groupby('store_type')['store_id'].nunique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Setup Feast Repository\n",
        "\n",
        "Copy feature definitions to the shared PVC:\n",
        "\n",
        "| File | Purpose |\n",
        "|------|----------|\n",
        "| `feature_store.yaml` | Ray-enabled config (KubeRay + CodeFlare SDK) |\n",
        "| `features.py` | FeatureViews, Entities, FeatureServices + **auto-auth** |\n",
        "\n",
        "**Key Features:**\n",
        "- `training_features` ‚Üí All features for model training\n",
        "- `inference_features` ‚Üí Subset for real-time serving\n",
        "\n",
        "**Auto-Auth:** `features.py` reads service account token from `/var/run/secrets/kubernetes.io/serviceaccount/token`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "FEATURE_REPO.mkdir(parents=True, exist_ok=True)\n",
        "(DATA_DIR / \"ray_storage\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Look for feature_repo in multiple possible locations\n",
        "possible_paths = [\n",
        "    Path(\"/opt/app-root/src/feature_repo\"),\n",
        "    Path(\"/opt/app-root/src/sales-demand-forecasting/feature_repo\"),\n",
        "    Path(\"../feature_repo\"),\n",
        "]\n",
        "\n",
        "src_dir = None\n",
        "for p in possible_paths:\n",
        "    if p.exists() and (p / \"features.py\").exists():\n",
        "        src_dir = p\n",
        "        print(f\"üìÅ Found feature_repo at: {src_dir}\")\n",
        "        break\n",
        "\n",
        "if src_dir is None:\n",
        "    raise FileNotFoundError(f\"feature_repo not found in: {possible_paths}\")\n",
        "\n",
        "# Copy features.py (includes auto-auth for CodeFlare SDK)\n",
        "shutil.copy(src_dir / \"features.py\", FEATURE_REPO / \"features.py\")\n",
        "print(\"‚úÖ features.py (with CodeFlare SDK auto-auth)\")\n",
        "\n",
        "# Use Ray config as main feature_store.yaml\n",
        "shutil.copy(src_dir / \"feature_store_ray.yaml\", FEATURE_REPO / \"feature_store.yaml\")\n",
        "print(\"‚úÖ feature_store.yaml (Ray + KubeRay)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üìä SAMPLE DATA: Show feature_store.yaml config\n",
        "print(\"üìä Feast Config (feature_store.yaml):\")\n",
        "print(\"-\" * 50)\n",
        "with open(FEATURE_REPO / \"feature_store.yaml\") as f:\n",
        "    print(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Feast Apply (via Remote KubeRay Cluster)\n",
        "\n",
        "Register feature definitions in PostgreSQL using the deployed Ray cluster:\n",
        "\n",
        "```\n",
        "feast apply\n",
        "    ‚îÇ\n",
        "    ‚îú‚îÄ‚îÄ Reads features.py (auto-auth configures CodeFlare SDK)\n",
        "    ‚îú‚îÄ‚îÄ Connects to KubeRay cluster \"feast-ray\" via CodeFlare SDK\n",
        "    ‚îú‚îÄ‚îÄ Uses mTLS for secure communication\n",
        "    ‚îî‚îÄ‚îÄ Creates tables in PostgreSQL registry\n",
        "```\n",
        "\n",
        "**Note:** The Ray cluster must be running before this step. Deploy with `kubectl apply -f manifests/03-raycluster.yaml`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "os.chdir(str(FEATURE_REPO))\n",
        "print(f\"üìç Working dir: {os.getcwd()}\")\n",
        "print(\"\\nüöÄ Running: feast apply\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "result = subprocess.run([\"feast\", \"apply\"], capture_output=True, text=True)\n",
        "print(result.stdout)\n",
        "if result.returncode != 0:\n",
        "    print(f\"‚ùå ERROR: {result.stderr}\")\n",
        "else:\n",
        "    print(\"‚úÖ Features registered to PostgreSQL\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Feast Materialize (via Remote KubeRay Cluster)\n",
        "\n",
        "Populate the **online store** using the deployed Ray cluster for distributed processing:\n",
        "\n",
        "```\n",
        "Offline Store (Parquet)     KubeRay Cluster     Online Store (PostgreSQL)\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Full history       ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Distributed  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Latest values only ‚îÇ\n",
        "‚îÇ 65K rows           ‚îÇ    ‚îÇ  Processing   ‚îÇ    ‚îÇ 630 entities       ‚îÇ\n",
        "‚îÇ For training       ‚îÇ    ‚îÇ (feast-ray)   ‚îÇ    ‚îÇ For serving        ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "**Why Ray for Materialize:**\n",
        "- Distributes work across KubeRay cluster `feast-ray`\n",
        "- Faster for large datasets (>1M rows)\n",
        "- Uses `batch_engine: ray.engine` in feature_store_ray.yaml\n",
        "- CodeFlare SDK handles mTLS authentication automatically"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "end_ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
        "print(f\"üöÄ Running: feast materialize {START_DATE}T00:00:00 {end_ts}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "result = subprocess.run(\n",
        "    [\"feast\", \"materialize\", f\"{START_DATE}T00:00:00\", end_ts],\n",
        "    capture_output=True, text=True, cwd=str(FEATURE_REPO)\n",
        ")\n",
        "print(result.stdout)\n",
        "if result.returncode != 0:\n",
        "    print(f\"‚ùå ERROR: {result.stderr}\")\n",
        "else:\n",
        "    print(\"‚úÖ Features materialized to PostgreSQL online store\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Verify Setup\n",
        "\n",
        "Test feature retrieval to ensure everything is working:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from feast import FeatureStore\n",
        "\n",
        "store = FeatureStore(repo_path=str(FEATURE_REPO))\n",
        "\n",
        "print(\"üìã Registered Objects:\")\n",
        "print(f\"   Entities: {[e.name for e in store.list_entities()]}\")\n",
        "print(f\"   FeatureViews: {[fv.name for fv in store.list_feature_views()]}\")\n",
        "print(f\"   FeatureServices: {[fs.name for fs in store.list_feature_services()]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üìä SAMPLE DATA: Online feature lookup (what serving will use)\n",
        "print(\"üìä Sample: Online feature lookup for Store 1, Dept 1\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "online_features = store.get_online_features(\n",
        "    features=[\n",
        "        \"sales_features:weekly_sales\",\n",
        "        \"sales_features:lag_1\",\n",
        "        \"sales_features:rolling_mean_4w\",\n",
        "        \"sales_features:is_holiday\",\n",
        "        \"store_features:store_type\",\n",
        "        \"store_features:store_size\",\n",
        "    ],\n",
        "    entity_rows=[{\"store_id\": 1, \"dept_id\": 1}]\n",
        ").to_dict()\n",
        "\n",
        "for k, v in online_features.items():\n",
        "    print(f\"   {k}: {v[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üìä SAMPLE DATA: Online features for multiple entities\n",
        "print(\"üìä Sample: Online features for multiple stores\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "entities = [\n",
        "    {\"store_id\": 1, \"dept_id\": 1},\n",
        "    {\"store_id\": 10, \"dept_id\": 5},\n",
        "    {\"store_id\": 25, \"dept_id\": 10},\n",
        "    {\"store_id\": 45, \"dept_id\": 14},\n",
        "]\n",
        "\n",
        "multi_features = store.get_online_features(\n",
        "    features=[\"sales_features:weekly_sales\", \"sales_features:lag_1\", \"store_features:store_type\"],\n",
        "    entity_rows=entities\n",
        ").to_df()\n",
        "\n",
        "multi_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# üìä SAMPLE DATA: Historical features (what training will use via remote Ray)\n",
        "print(\"üìä Sample: Historical feature retrieval via Remote KubeRay\")\n",
        "print(\"-\" * 50)\n",
        "print(\"This uses get_historical_features() which distributes PIT joins across KubeRay cluster\")\n",
        "print()\n",
        "\n",
        "# Small entity DataFrame for demo\n",
        "entity_df = pd.DataFrame([\n",
        "    {\"store_id\": 1, \"dept_id\": 1, \"event_timestamp\": datetime(2023, 6, 1, tzinfo=timezone.utc)},\n",
        "    {\"store_id\": 1, \"dept_id\": 1, \"event_timestamp\": datetime(2023, 6, 15, tzinfo=timezone.utc)},\n",
        "    {\"store_id\": 10, \"dept_id\": 5, \"event_timestamp\": datetime(2023, 6, 1, tzinfo=timezone.utc)},\n",
        "    {\"store_id\": 25, \"dept_id\": 10, \"event_timestamp\": datetime(2023, 7, 1, tzinfo=timezone.utc)},\n",
        "])\n",
        "\n",
        "historical = store.get_historical_features(\n",
        "    entity_df=entity_df,\n",
        "    features=[\"sales_features:weekly_sales\", \"sales_features:lag_1\", \"sales_features:rolling_mean_4w\", \"store_features:store_type\"]\n",
        ").to_df()\n",
        "\n",
        "print(f\"‚úÖ Retrieved {len(historical)} rows with {len(historical.columns)} columns\")\n",
        "print(f\"   Columns: {list(historical.columns)}\")\n",
        "print()\n",
        "historical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Complete!\n",
        "\n",
        "### What We Built\n",
        "\n",
        "| Component | Count | Location |\n",
        "|-----------|-------|----------|\n",
        "| Sales records | 65,520 | `/shared/data/sales_features.parquet` |\n",
        "| Store records | 630 | `/shared/data/store_features.parquet` |\n",
        "| Features | 22 | Lags, rolling stats, temporal, economic |\n",
        "| Registry | PostgreSQL | Feature metadata |\n",
        "| Online Store | PostgreSQL | Latest values for serving |\n",
        "\n",
        "### Feature Importance (typical retail forecasting)\n",
        "\n",
        "| Feature Group | Importance | Examples |\n",
        "|---------------|------------|----------|\n",
        "| Lag features | 35% | `lag_1`, `lag_2`, `lag_4`, `lag_8` |\n",
        "| Rolling stats | 28% | `rolling_mean_4w`, `rolling_std_4w` |\n",
        "| Temporal | 18% | `week_of_year`, `month`, `quarter` |\n",
        "| Holiday | 10% | `is_holiday`, `days_to_holiday` |\n",
        "| Economic | 7% | `temperature`, `fuel_price`, `cpi` |\n",
        "| Store | 2% | `store_type`, `store_size` |\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "**Option A: Use Manifests (Recommended)**\n",
        "```bash\n",
        "kubectl apply -f manifests/05-dataprep-job.yaml   # Regenerate data\n",
        "kubectl apply -f manifests/06-trainjob.yaml       # Train model\n",
        "```\n",
        "\n",
        "**Option B: Use Notebooks**\n",
        "- `02-training.ipynb` ‚Üí Train model with `get_historical_features()`\n",
        "- `03-inference.ipynb` ‚Üí Deploy model with KServe"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}