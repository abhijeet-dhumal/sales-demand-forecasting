# Data Preparation Job with Feast Integration
# Creates data, registers features, materializes to online store
apiVersion: batch/v1
kind: Job
metadata:
  name: feast-data-prep
  namespace: feast-trainer-demo
spec:
  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - name: data-prep
        image: registry.redhat.io/rhoai/odh-training-cuda128-torch28-py312-rhel9@sha256:bdc8cb781f005c11534a959fd57b8ba5133522e3bb4756d409e3111eeaf2e8ee
        env:
        - name: DATA_DIR
          value: /shared/data
        - name: FEATURE_REPO
          value: /shared/feature_repo
        # PostgreSQL connection for Feast
        - name: POSTGRES_HOST
          value: feast-postgres.feast-trainer-demo.svc.cluster.local
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef:
              name: feast-postgres-secret
              key: username
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: feast-postgres-secret
              key: password
        volumeMounts:
        - name: shared-storage
          mountPath: /shared
        command:
        - bash
        - -c
        - |
          set -e
          pip install --quiet "feast[postgres]==0.40.0" psycopg2-binary
          
          mkdir -p $DATA_DIR $FEATURE_REPO
          
          # =================================================================
          # STEP 1: Create Feature Store Config
          # =================================================================
          cat > $FEATURE_REPO/feature_store.yaml << EOF
          project: sales_forecasting
          provider: local
          registry:
            registry_type: sql
            path: postgresql+psycopg://$POSTGRES_USER:$POSTGRES_PASSWORD@$POSTGRES_HOST:5432/feast
          offline_store:
            type: file
          online_store:
            type: postgres
            host: $POSTGRES_HOST
            port: 5432
            database: feast
            user: $POSTGRES_USER
            password: $POSTGRES_PASSWORD
          entity_key_serialization_version: 2
          EOF
          
          echo "âœ… Feature store config created"
          
          # =================================================================
          # STEP 2: Create Feature Definitions
          # =================================================================
          cat > $FEATURE_REPO/features.py << 'FEATURES_PY'
          from datetime import timedelta
          from feast import Entity, FeatureView, Field, FileSource, FeatureService
          from feast.types import Float64, Int64
          import os
          
          DATA_DIR = os.environ.get("DATA_DIR", "/shared/data")
          
          # Entities
          store = Entity(name="store_id", join_keys=["store_id"])
          dept = Entity(name="dept_id", join_keys=["dept_id"])
          
          # Sales features source
          sales_source = FileSource(
              name="sales_source",
              path=f"{DATA_DIR}/sales_features.parquet",
              timestamp_field="date",
              created_timestamp_column="created_timestamp",
          )
          
          # Store features source
          store_source = FileSource(
              name="store_source",
              path=f"{DATA_DIR}/store_features.parquet",
              timestamp_field="date",
              created_timestamp_column="created_timestamp",
          )
          
          # Sales feature view (lag features)
          sales_features = FeatureView(
              name="sales_features",
              entities=[store, dept],
              ttl=timedelta(days=365*3),
              schema=[
                  Field(name="lag_1", dtype=Float64),
                  Field(name="lag_2", dtype=Float64),
                  Field(name="lag_4", dtype=Float64),
                  Field(name="lag_8", dtype=Float64),
                  Field(name="lag_52", dtype=Float64),
                  Field(name="rolling_mean_4w", dtype=Float64),
                  Field(name="rolling_std_4w", dtype=Float64),
                  Field(name="rolling_mean_8w", dtype=Float64),
                  Field(name="rolling_std_8w", dtype=Float64),
                  Field(name="rolling_mean_52w", dtype=Float64),
              ],
              source=sales_source,
              online=True,
          )
          
          # Store feature view
          store_features = FeatureView(
              name="store_features",
              entities=[store],
              ttl=timedelta(days=365*3),
              schema=[
                  Field(name="store_size", dtype=Int64),
                  Field(name="temperature", dtype=Float64),
                  Field(name="fuel_price", dtype=Float64),
                  Field(name="cpi", dtype=Float64),
                  Field(name="unemployment", dtype=Float64),
                  Field(name="markdown1", dtype=Float64),
                  Field(name="markdown2", dtype=Float64),
                  Field(name="markdown3", dtype=Float64),
                  Field(name="markdown4", dtype=Float64),
                  Field(name="markdown5", dtype=Float64),
                  Field(name="is_holiday", dtype=Int64),
                  Field(name="week_of_year", dtype=Int64),
                  Field(name="month", dtype=Int64),
              ],
              source=store_source,
              online=True,
          )
          
          # Feature service
          sales_forecasting_service = FeatureService(
              name="sales_forecasting_features",
              features=[sales_features, store_features],
          )
          FEATURES_PY
          
          echo "âœ… Feature definitions created"
          
          # =================================================================
          # STEP 3: Generate Synthetic Data
          # =================================================================
          python << 'DATA_SCRIPT'
          import os
          import pandas as pd
          import numpy as np
          from datetime import datetime
          
          DATA_DIR = os.environ.get("DATA_DIR", "/shared/data")
          
          print("=" * 60)
          print("STEP 3: Generating synthetic sales data")
          print("=" * 60)
          
          np.random.seed(42)
          n_stores, n_depts, n_weeks = 10, 20, 150
          
          dates = pd.date_range("2010-02-05", periods=n_weeks, freq="W-FRI")
          
          # Generate raw sales data
          rows = []
          for store in range(1, n_stores + 1):
              store_base = np.random.uniform(0.8, 1.2)  # Store-level multiplier
              for dept in range(1, n_depts + 1):
                  dept_base = np.random.uniform(5000, 50000)
                  for date in dates:
                      week = date.isocalendar()[1]
                      # Seasonality + trend + noise
                      seasonality = 1 + 0.3 * np.sin(2 * np.pi * week / 52)
                      trend = 1 + 0.001 * (date - dates[0]).days
                      noise = np.random.normal(0, 0.1)
                      sales = dept_base * store_base * seasonality * trend * (1 + noise)
                      
                      rows.append({
                          "store_id": store,
                          "dept_id": dept,
                          "date": date,
                          "weekly_sales": max(0, sales),
                          "is_holiday": int(week in [6, 36, 47, 52]),
                          "store_size": int(np.random.randint(50000, 200000)),
                          "temperature": np.random.uniform(30, 100),
                          "fuel_price": np.random.uniform(2.5, 4.5),
                          "cpi": np.random.uniform(200, 230),
                          "unemployment": np.random.uniform(5, 12),
                          "markdown1": np.random.uniform(0, 10000) if np.random.random() > 0.5 else 0,
                          "markdown2": np.random.uniform(0, 5000) if np.random.random() > 0.5 else 0,
                          "markdown3": np.random.uniform(0, 2000) if np.random.random() > 0.5 else 0,
                          "markdown4": np.random.uniform(0, 3000) if np.random.random() > 0.5 else 0,
                          "markdown5": np.random.uniform(0, 5000) if np.random.random() > 0.5 else 0,
                      })
          
          df = pd.DataFrame(rows)
          df = df.sort_values(["store_id", "dept_id", "date"]).reset_index(drop=True)
          print(f"  Raw data: {len(df):,} rows")
          
          # Create lag features (NO LEAKAGE - shift before rolling)
          print("  Creating lag features...")
          def create_lag_features(group):
              group = group.sort_values("date")
              group["lag_1"] = group["weekly_sales"].shift(1)
              group["lag_2"] = group["weekly_sales"].shift(2)
              group["lag_4"] = group["weekly_sales"].shift(4)
              group["lag_8"] = group["weekly_sales"].shift(8)
              group["lag_52"] = group["weekly_sales"].shift(52)
              group["rolling_mean_4w"] = group["weekly_sales"].shift(1).rolling(4, min_periods=1).mean()
              group["rolling_std_4w"] = group["weekly_sales"].shift(1).rolling(4, min_periods=1).std()
              group["rolling_mean_8w"] = group["weekly_sales"].shift(1).rolling(8, min_periods=1).mean()
              group["rolling_std_8w"] = group["weekly_sales"].shift(1).rolling(8, min_periods=1).std()
              group["rolling_mean_52w"] = group["weekly_sales"].shift(1).rolling(52, min_periods=4).mean()
              return group
          
          df = df.groupby(["store_id", "dept_id"], group_keys=False).apply(create_lag_features)
          
          # Calendar features
          df["week_of_year"] = df["date"].dt.isocalendar().week.astype(int)
          df["month"] = df["date"].dt.month
          
          # Drop rows with NaN lags
          df_clean = df.dropna(subset=["lag_1", "lag_2", "rolling_mean_4w"]).copy()
          df_clean["created_timestamp"] = datetime.now()
          
          print(f"  Clean data: {len(df_clean):,} rows")
          
          # Save for Feast sources
          sales_cols = ["store_id", "dept_id", "date", "created_timestamp",
                        "lag_1", "lag_2", "lag_4", "lag_8", "lag_52",
                        "rolling_mean_4w", "rolling_std_4w", "rolling_mean_8w", 
                        "rolling_std_8w", "rolling_mean_52w"]
          df_clean[sales_cols].to_parquet(f"{DATA_DIR}/sales_features.parquet", index=False)
          
          store_cols = ["store_id", "date", "created_timestamp", "store_size",
                        "temperature", "fuel_price", "cpi", "unemployment",
                        "markdown1", "markdown2", "markdown3", "markdown4", "markdown5",
                        "is_holiday", "week_of_year", "month"]
          df_clean[store_cols].drop_duplicates(["store_id", "date"]).to_parquet(
              f"{DATA_DIR}/store_features.parquet", index=False
          )
          
          # Save full dataset with target for training
          df_clean.to_parquet(f"{DATA_DIR}/features.parquet", index=False)
          
          # Save entity dataframe for Feast queries
          df_clean[["store_id", "dept_id", "date"]].to_parquet(
              f"{DATA_DIR}/entities.parquet", index=False
          )
          
          print(f"âœ… Data saved to {DATA_DIR}")
          DATA_SCRIPT
          
          # =================================================================
          # STEP 4: Apply Feast Feature Definitions
          # =================================================================
          echo ""
          echo "============================================================"
          echo "STEP 4: Applying Feast feature definitions"
          echo "============================================================"
          cd $FEATURE_REPO
          feast apply
          
          echo ""
          echo "ðŸ“‹ Registered Feature Views:"
          feast feature-views list
          
          # =================================================================
          # STEP 5: Materialize Features to Online Store
          # =================================================================
          echo ""
          echo "============================================================"
          echo "STEP 5: Materializing features to online store"
          echo "============================================================"
          
          # Materialize last 30 days for online serving
          START_DATE=$(date -d "30 days ago" +%Y-%m-%dT00:00:00)
          END_DATE=$(date +%Y-%m-%dT23:59:59)
          
          echo "Materializing from $START_DATE to $END_DATE"
          feast materialize $START_DATE $END_DATE
          
          echo ""
          echo "============================================================"
          echo "âœ… FEAST DATA PREPARATION COMPLETE"
          echo "============================================================"
          echo ""
          echo "Feature Store: PostgreSQL @ $POSTGRES_HOST"
          echo "Registry: feast_registry"
          echo "Offline Store: feast_offline"
          echo "Online Store: feast_online"
          echo ""
          echo "Feature Views:"
          echo "  - sales_features (lag/rolling stats)"
          echo "  - store_features (external factors)"
          echo ""
          echo "Next: Run training job to fetch features via Feast SDK"
      volumes:
      - name: shared-storage
        persistentVolumeClaim:
          claimName: shared-storage
