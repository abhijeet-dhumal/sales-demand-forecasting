# Feast Training Job - Connects to KubeRay for distributed training
# Fetches features from Feast and trains PyTorch model
#
# Prerequisites:
#   - RayCluster running (08-kuberay-cluster.yaml)
#   - Feast data prepared (10-feast-dataprep-job.yaml)
#   - feast-pvc exists (09-feast-prereqs.yaml)
#
# Monitor: Ray Dashboard -> Jobs tab
#
apiVersion: batch/v1
kind: Job
metadata:
  name: feast-train
  namespace: feast-trainer-demo
  labels:
    app: feast-train
    pipeline: sales-forecasting
spec:
  template:
    spec:
      serviceAccountName: feast-sa
      restartPolicy: Never
      containers:
        - name: feast-train
          # Must match RayCluster version
          image: quay.io/modh/ray:2.52.1-py312-cu128
          command: ["/bin/bash", "-c"]
          args:
            - |
              echo "============================================================"
              echo "FEAST MODEL TRAINING - NATIVE RAY + KUBERAY"
              echo "============================================================"
              
              echo "Installing dependencies to /tmp..."
              pip install --target=/tmp/pylibs -q feast[postgres,ray]==0.59.0 psycopg2-binary codeflare-sdk torch
              
              export PYTHONPATH=/tmp/pylibs:$PYTHONPATH
              
              echo "Running training script..."
              python /scripts/train.py
          env:
            - name: PYTHONUNBUFFERED
              value: "1"
            - name: FEAST_RAY_USE_KUBERAY
              value: "true"
            - name: FEAST_RAY_CLUSTER_NAME
              value: "feast-ray"
            - name: FEAST_RAY_NAMESPACE
              value: "feast-trainer-demo"
            - name: FEAST_RAY_AUTH_SERVER
              value: "https://kubernetes.default.svc"
            - name: FEAST_RAY_SKIP_TLS
              value: "true"
          volumeMounts:
            - name: scripts
              mountPath: /scripts
            - name: feast-data
              mountPath: /shared
          resources:
            limits:
              cpu: "2"
              memory: "4Gi"
            requests:
              cpu: "1"
              memory: "2Gi"
      volumes:
        - name: scripts
          configMap:
            name: feast-train-scripts
        - name: feast-data
          persistentVolumeClaim:
            claimName: feast-pvc
  backoffLimit: 2
  ttlSecondsAfterFinished: 3600

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: feast-train-scripts
  namespace: feast-trainer-demo
data:
  train.py: |
    #!/usr/bin/env python3
    """
    Sales Forecasting Training - Uses Feast + KubeRay
    
    Features:
    - Fetches training data from Feast feature store via Ray
    - Trains PyTorch neural network
    - Saves model to shared PVC
    """
    import os
    import sys
    import torch
    import torch.nn as nn
    import pandas as pd
    import numpy as np
    from datetime import datetime, timedelta
    from pathlib import Path
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import train_test_split
    
    print("=" * 70)
    print("SALES FORECASTING TRAINING")
    print("=" * 70)
    
    # Paths
    REPO_DIR = Path("/shared/feature_repo")
    MODEL_DIR = Path("/shared/models")
    MODEL_DIR.mkdir(parents=True, exist_ok=True)
    
    # =========================================================================
    # STEP 1: Setup Feast with KubeRay
    # =========================================================================
    print("\n" + "=" * 70)
    print("STEP 1: Connecting to Feast + KubeRay")
    print("=" * 70)
    
    # Set auth token from SA
    try:
        with open("/var/run/secrets/kubernetes.io/serviceaccount/token", "r") as f:
            os.environ["FEAST_RAY_AUTH_TOKEN"] = f.read().strip()
        print("   ✅ Set FEAST_RAY_AUTH_TOKEN from SA token")
    except Exception as e:
        print(f"   ⚠️  Could not read SA token: {e}")
    
    os.environ["FEAST_RAY_USE_KUBERAY"] = "true"
    os.environ["FEAST_RAY_CLUSTER_NAME"] = "feast-ray"
    os.environ["FEAST_RAY_NAMESPACE"] = "feast-trainer-demo"
    os.environ["FEAST_RAY_AUTH_SERVER"] = "https://kubernetes.default.svc"
    os.environ["FEAST_RAY_SKIP_TLS"] = "true"
    
    from feast import FeatureStore
    
    store = FeatureStore(repo_path=str(REPO_DIR))
    print(f"   Project: {store.project}")
    print(f"   Feature Views: {[fv.name for fv in store.list_feature_views()]}")
    
    # =========================================================================
    # STEP 2: Fetch Features from Feast
    # =========================================================================
    print("\n" + "=" * 70)
    print("STEP 2: Fetching Features from Feast")
    print("=" * 70)
    
    num_samples = 50000
    np.random.seed(42)
    
    entity_df = pd.DataFrame({
        "store_id": np.random.randint(1, 51, num_samples),
        "dept_id": np.random.randint(1, 13, num_samples),
        "event_timestamp": [
            datetime.now() - timedelta(days=np.random.randint(1, 365))
            for _ in range(num_samples)
        ]
    })
    
    print(f"   Fetching features for {len(entity_df):,} training samples...")
    
    training_df = store.get_historical_features(
        entity_df=entity_df,
        features=[
            "sales_features:weekly_sales",
            "sales_features:lag_1",
            "sales_features:lag_2",
            "sales_features:lag_4",
            "sales_features:rolling_mean_4w",
            "store_features:store_size",
            "store_features:temperature",
            "store_features:fuel_price",
            "store_features:cpi",
            "store_features:unemployment",
        ]
    ).to_df()
    
    print(f"✅ Retrieved {len(training_df):,} rows")
    
    # =========================================================================
    # STEP 3: Prepare Data
    # =========================================================================
    print("\n" + "=" * 70)
    print("STEP 3: Preparing Training Data")
    print("=" * 70)
    
    training_df = training_df.dropna()
    print(f"   Samples after dropping NaN: {len(training_df):,}")
    
    feature_cols = [
        "lag_1", "lag_2", "lag_4", "rolling_mean_4w",
        "store_size", "temperature", "fuel_price", "cpi", "unemployment"
    ]
    target_col = "weekly_sales"
    
    X = training_df[feature_cols].values
    y = training_df[target_col].values
    
    scaler_X = StandardScaler()
    scaler_y = StandardScaler()
    
    X_scaled = scaler_X.fit_transform(X)
    y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_scaled, test_size=0.2, random_state=42
    )
    
    print(f"   Training: {len(X_train):,} | Test: {len(X_test):,}")
    
    # =========================================================================
    # STEP 4: Define and Train Model
    # =========================================================================
    print("\n" + "=" * 70)
    print("STEP 4: Training Neural Network")
    print("=" * 70)
    
    class SalesForecastModel(nn.Module):
        def __init__(self, input_dim=9):
            super().__init__()
            self.network = nn.Sequential(
                nn.Linear(input_dim, 128),
                nn.BatchNorm1d(128),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(128, 64),
                nn.BatchNorm1d(64),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(64, 32),
                nn.ReLU(),
                nn.Linear(32, 1)
            )
        
        def forward(self, x):
            return self.network(x).squeeze(-1)
    
    model = SalesForecastModel(input_dim=len(feature_cols))
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"   Device: {device}")
    print(f"   Parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    model = model.to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # Convert to tensors
    X_train_t = torch.FloatTensor(X_train)
    y_train_t = torch.FloatTensor(y_train)
    X_test_t = torch.FloatTensor(X_test)
    y_test_t = torch.FloatTensor(y_test)
    
    train_dataset = torch.utils.data.TensorDataset(X_train_t, y_train_t)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)
    
    EPOCHS = 50
    best_loss = float('inf')
    
    print(f"\n   Training for {EPOCHS} epochs...")
    
    for epoch in range(EPOCHS):
        model.train()
        epoch_loss = 0.0
        
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        
        avg_loss = epoch_loss / len(train_loader)
        
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_test_t.to(device))
            val_loss = criterion(val_outputs, y_test_t.to(device)).item()
        
        if val_loss < best_loss:
            best_loss = val_loss
            torch.save({
                "model_state_dict": model.state_dict(),
                "scaler_X": scaler_X,
                "scaler_y": scaler_y,
                "feature_cols": feature_cols,
            }, MODEL_DIR / "best_model.pt")
        
        if (epoch + 1) % 10 == 0:
            print(f"   Epoch {epoch+1:3d}: train={avg_loss:.4f}, val={val_loss:.4f}")
    
    # =========================================================================
    # STEP 5: Evaluate
    # =========================================================================
    print("\n" + "=" * 70)
    print("STEP 5: Evaluation")
    print("=" * 70)
    
    checkpoint = torch.load(MODEL_DIR / "best_model.pt")
    model.load_state_dict(checkpoint["model_state_dict"])
    
    model.eval()
    with torch.no_grad():
        predictions = model(X_test_t.to(device)).cpu().numpy()
    
    predictions_orig = scaler_y.inverse_transform(predictions.reshape(-1, 1)).flatten()
    actuals_orig = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()
    
    rmse = np.sqrt(np.mean((predictions_orig - actuals_orig) ** 2))
    mae = np.mean(np.abs(predictions_orig - actuals_orig))
    
    print(f"   RMSE: ${rmse:,.2f}")
    print(f"   MAE:  ${mae:,.2f}")
    
    # Save final model
    final_path = MODEL_DIR / f"sales_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pt"
    torch.save({
        "model_state_dict": model.state_dict(),
        "scaler_X": scaler_X,
        "scaler_y": scaler_y,
        "feature_cols": feature_cols,
        "metrics": {"rmse": float(rmse), "mae": float(mae)},
    }, final_path)
    
    print(f"\n✅ Model saved: {final_path}")
    print("\n" + "=" * 70)
    print("TRAINING COMPLETE")
    print("=" * 70)

