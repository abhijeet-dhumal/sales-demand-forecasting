{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training with Kubeflow and MLflow\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. **Fetch features** from Feast feature store\n",
        "2. **Train locally** for quick validation\n",
        "3. **Submit Kubeflow TrainJob** for distributed training\n",
        "4. **Track experiments** with MLflow\n",
        "5. **Register model** in MLflow Model Registry\n",
        "\n",
        "## Prerequisites\n",
        "- Completed `02-feast-features.ipynb` (features registered in Feast)\n",
        "- MLflow server running\n",
        "- Kubeflow Training Operator installed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration - aligned with example manifests\n",
        "NAMESPACE = os.environ.get(\"NAMESPACE\", \"feast-mlops-demo\")\n",
        "MLFLOW_TRACKING_URI = f\"http://mlflow.{NAMESPACE}.svc.cluster.local:5000\"\n",
        "SHARED_DIR = os.environ.get(\"SHARED_DIR\", \"/shared\")\n",
        "\n",
        "# Set MLflow tracking\n",
        "os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_TRACKING_URI\n",
        "\n",
        "print(f\"\"\"\n",
        "Configuration:\n",
        "  Namespace: {NAMESPACE}\n",
        "  MLflow: {MLFLOW_TRACKING_URI}\n",
        "  Shared Storage: {SHARED_DIR}\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Fetch Features from Feast\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta, timezone\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from feast import FeatureStore\n",
        "\n",
        "# Initialize Feast\n",
        "REPO_DIR = Path(SHARED_DIR) / \"feature_repo\"\n",
        "fs = FeatureStore(repo_path=str(REPO_DIR))\n",
        "\n",
        "print(f\"Feast project: {fs.project}\")\n",
        "print(f\"Feature views: {[fv.name for fv in fs.list_feature_views()]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create training entity dataframe\n",
        "num_samples = 10000\n",
        "np.random.seed(42)\n",
        "\n",
        "entity_df = pd.DataFrame({\n",
        "    \"store_id\": np.random.randint(1, 51, num_samples),\n",
        "    \"dept_id\": np.random.randint(1, 13, num_samples),\n",
        "    \"event_timestamp\": [\n",
        "        datetime.now(timezone.utc) - timedelta(days=np.random.randint(1, 365))\n",
        "        for _ in range(num_samples)\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Fetch historical features\n",
        "print(f\"üöÄ Fetching features for {len(entity_df):,} training samples...\")\n",
        "\n",
        "training_df = fs.get_historical_features(\n",
        "    entity_df=entity_df,\n",
        "    features=[\n",
        "        \"sales_features:weekly_sales\",\n",
        "        \"sales_features:lag_1\",\n",
        "        \"sales_features:lag_2\",\n",
        "        \"sales_features:lag_4\",\n",
        "        \"sales_features:rolling_mean_4w\",\n",
        "        \"store_features:store_size\",\n",
        "        \"store_features:temperature\",\n",
        "        \"store_features:fuel_price\",\n",
        "        \"store_features:cpi\",\n",
        "        \"store_features:unemployment\",\n",
        "    ]\n",
        ").to_df()\n",
        "\n",
        "print(f\"‚úÖ Retrieved {len(training_df):,} rows\")\n",
        "training_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Local Training with MLflow Tracking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepare data\n",
        "training_df_clean = training_df.dropna()\n",
        "feature_cols = [\"lag_1\", \"lag_2\", \"lag_4\", \"rolling_mean_4w\",\n",
        "                \"store_size\", \"temperature\", \"fuel_price\", \"cpi\", \"unemployment\"]\n",
        "target_col = \"weekly_sales\"\n",
        "\n",
        "X = training_df_clean[feature_cols].values\n",
        "y = training_df_clean[target_col].values\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define model\n",
        "class SalesForecastModel(nn.Module):\n",
        "    def __init__(self, input_dim=9):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x).squeeze(-1)\n",
        "\n",
        "model = SalesForecastModel(input_dim=len(feature_cols))\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training function with MLflow logging\n",
        "def train_model(model, X_train, y_train, X_test, y_test, epochs=50, lr=0.001, batch_size=256):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    \n",
        "    X_train_t = torch.FloatTensor(X_train).to(device)\n",
        "    y_train_t = torch.FloatTensor(y_train).to(device)\n",
        "    X_test_t = torch.FloatTensor(X_test).to(device)\n",
        "    y_test_t = torch.FloatTensor(y_test).to(device)\n",
        "    \n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "    \n",
        "    train_dataset = torch.utils.data.TensorDataset(X_train_t, y_train_t)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "    history = {\"train_loss\": [], \"test_loss\": []}\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(X_batch)\n",
        "            loss = criterion(pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        scheduler.step()\n",
        "        \n",
        "        # Eval\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_pred = model(X_test_t)\n",
        "            test_loss = criterion(test_pred, y_test_t).item()\n",
        "        \n",
        "        train_loss = epoch_loss / len(train_loader)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"test_loss\"].append(test_loss)\n",
        "        \n",
        "        # Log to MLflow\n",
        "        mlflow.log_metrics({\n",
        "            \"train_loss\": train_loss,\n",
        "            \"test_loss\": test_loss,\n",
        "            \"lr\": scheduler.get_last_lr()[0]\n",
        "        }, step=epoch)\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1:3d} | Train: {train_loss:.4f} | Test: {test_loss:.4f}\")\n",
        "    \n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run training with MLflow tracking\n",
        "mlflow.set_experiment(\"sales-forecasting\")\n",
        "\n",
        "with mlflow.start_run(run_name=\"local-pytorch-training\"):\n",
        "    # Log hyperparameters\n",
        "    mlflow.log_params({\n",
        "        \"model_type\": \"SalesForecastModel\",\n",
        "        \"epochs\": 50,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"batch_size\": 256,\n",
        "        \"optimizer\": \"AdamW\",\n",
        "        \"features\": feature_cols,\n",
        "        \"train_samples\": len(X_train),\n",
        "        \"test_samples\": len(X_test)\n",
        "    })\n",
        "    \n",
        "    # Train\n",
        "    trained_model, history = train_model(model, X_train, y_train, X_test, y_test)\n",
        "    \n",
        "    # Log model\n",
        "    mlflow.pytorch.log_model(trained_model, \"model\")\n",
        "    \n",
        "    # Save locally too\n",
        "    model_path = Path(SHARED_DIR) / \"models\" / \"sales_forecast_model.pt\"\n",
        "    model_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    torch.save(trained_model.state_dict(), model_path)\n",
        "    mlflow.log_artifact(str(model_path))\n",
        "    \n",
        "    # Save scalers\n",
        "    import joblib\n",
        "    scalers_path = Path(SHARED_DIR) / \"models\" / \"scalers.joblib\"\n",
        "    joblib.dump({\"scaler_X\": scaler_X, \"scaler_y\": scaler_y}, scalers_path)\n",
        "    mlflow.log_artifact(str(scalers_path))\n",
        "    \n",
        "    run_id = mlflow.active_run().info.run_id\n",
        "    print(f\"\\n‚úÖ Training complete! MLflow run_id: {run_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "ax.plot(history[\"train_loss\"], label=\"Train Loss\", linewidth=2)\n",
        "ax.plot(history[\"test_loss\"], label=\"Test Loss\", linewidth=2, linestyle=\"--\")\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Loss (MSE)\")\n",
        "ax.set_title(\"Training Progress\")\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Distributed Training with Kubeflow\n",
        "\n",
        "For larger datasets or models, use Kubeflow Training Operator for distributed training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create training script for Kubeflow\n",
        "training_script = '''\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data import DataLoader, TensorDataset, DistributedSampler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import mlflow\n",
        "\n",
        "# Setup distributed\n",
        "def setup_distributed():\n",
        "    if \"WORLD_SIZE\" in os.environ:\n",
        "        dist.init_process_group(backend=\"nccl\" if torch.cuda.is_available() else \"gloo\")\n",
        "        return dist.get_rank(), dist.get_world_size()\n",
        "    return 0, 1\n",
        "\n",
        "def cleanup_distributed():\n",
        "    if dist.is_initialized():\n",
        "        dist.destroy_process_group()\n",
        "\n",
        "class SalesForecastModel(nn.Module):\n",
        "    def __init__(self, input_dim=9):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x).squeeze(-1)\n",
        "\n",
        "def train():\n",
        "    rank, world_size = setup_distributed()\n",
        "    device = torch.device(f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    # Load data\n",
        "    data_path = os.environ.get(\"DATA_PATH\", \"/mnt/shared/data\")\n",
        "    df = pd.read_parquet(f\"{data_path}/training_data.parquet\")\n",
        "    \n",
        "    feature_cols = [\"lag_1\", \"lag_2\", \"lag_4\", \"rolling_mean_4w\",\n",
        "                    \"store_size\", \"temperature\", \"fuel_price\", \"cpi\", \"unemployment\"]\n",
        "    X = df[feature_cols].values\n",
        "    y = df[\"weekly_sales\"].values\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    \n",
        "    dataset = TensorDataset(torch.FloatTensor(X), torch.FloatTensor(y))\n",
        "    sampler = DistributedSampler(dataset) if world_size > 1 else None\n",
        "    loader = DataLoader(dataset, batch_size=256, sampler=sampler, shuffle=(sampler is None))\n",
        "    \n",
        "    model = SalesForecastModel().to(device)\n",
        "    if world_size > 1:\n",
        "        model = DDP(model, device_ids=[rank] if torch.cuda.is_available() else None)\n",
        "    \n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "    \n",
        "    # MLflow on rank 0 only\n",
        "    if rank == 0:\n",
        "        mlflow.set_tracking_uri(os.environ.get(\"MLFLOW_TRACKING_URI\"))\n",
        "        mlflow.set_experiment(\"sales-forecasting\")\n",
        "        mlflow.start_run(run_name=\"kubeflow-distributed-training\")\n",
        "        mlflow.log_params({\"world_size\": world_size, \"epochs\": 50})\n",
        "    \n",
        "    for epoch in range(50):\n",
        "        if sampler:\n",
        "            sampler.set_epoch(epoch)\n",
        "        \n",
        "        epoch_loss = 0\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X_batch), y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        if rank == 0 and (epoch + 1) % 10 == 0:\n",
        "            avg_loss = epoch_loss / len(loader)\n",
        "            mlflow.log_metric(\"train_loss\", avg_loss, step=epoch)\n",
        "            print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n",
        "    \n",
        "    if rank == 0:\n",
        "        model_to_save = model.module if hasattr(model, \"module\") else model\n",
        "        torch.save(model_to_save.state_dict(), \"/mnt/shared/models/distributed_model.pt\")\n",
        "        mlflow.pytorch.log_model(model_to_save, \"model\")\n",
        "        mlflow.end_run()\n",
        "        print(\"Training complete!\")\n",
        "    \n",
        "    cleanup_distributed()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n",
        "'''\n",
        "\n",
        "# Save script\n",
        "script_path = Path(SHARED_DIR) / \"scripts\" / \"train_distributed.py\"\n",
        "script_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "script_path.write_text(training_script)\n",
        "print(f\"‚úÖ Training script saved to {script_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Submit TrainJob using Kubeflow SDK\n",
        "from kubeflow.training import TrainerClient\n",
        "\n",
        "trainer = TrainerClient(namespace=NAMESPACE)\n",
        "\n",
        "# Submit training job\n",
        "job_name = trainer.train(\n",
        "    script_code=training_script,\n",
        "    packages_to_install=[\"torch\", \"pandas\", \"pyarrow\", \"scikit-learn\", \"mlflow\"],\n",
        "    num_nodes=2,\n",
        "    gpus_per_node=0,  # Set to 1+ for GPU training\n",
        "    env_vars={\n",
        "        \"MLFLOW_TRACKING_URI\": MLFLOW_TRACKING_URI,\n",
        "        \"DATA_PATH\": f\"{SHARED_DIR}/data\"\n",
        "    },\n",
        "    confirmed=True\n",
        ")\n",
        "\n",
        "print(f\"üöÄ Submitted TrainJob: {job_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor training progress\n",
        "import time\n",
        "\n",
        "for _ in range(30):  # Check for 5 minutes\n",
        "    status = trainer.get_job(job_name)\n",
        "    print(f\"Job status: {status.get('status', 'Unknown')}\")\n",
        "    \n",
        "    if status.get('status') in ['Complete', 'Failed']:\n",
        "        break\n",
        "    time.sleep(10)\n",
        "\n",
        "# Get logs\n",
        "logs = trainer.get_training_logs(job_name)\n",
        "print(\"\\n--- Training Logs ---\")\n",
        "print(logs.get('logs', 'No logs available')[-2000:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Register Model in MLflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register best model\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "client = MlflowClient()\n",
        "\n",
        "# Get the best run from experiment\n",
        "experiment = client.get_experiment_by_name(\"sales-forecasting\")\n",
        "runs = client.search_runs(\n",
        "    experiment_ids=[experiment.experiment_id],\n",
        "    order_by=[\"metrics.test_loss ASC\"],\n",
        "    max_results=1\n",
        ")\n",
        "\n",
        "if runs:\n",
        "    best_run = runs[0]\n",
        "    print(f\"Best run: {best_run.info.run_id}\")\n",
        "    print(f\"  Test Loss: {best_run.data.metrics.get('test_loss', 'N/A'):.4f}\")\n",
        "    \n",
        "    # Register model\n",
        "    model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
        "    model_version = mlflow.register_model(model_uri, \"sales-forecast-model\")\n",
        "    \n",
        "    # Transition to production\n",
        "    client.transition_model_version_stage(\n",
        "        name=\"sales-forecast-model\",\n",
        "        version=model_version.version,\n",
        "        stage=\"Production\"\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n‚úÖ Model registered: sales-forecast-model v{model_version.version} (Production)\")\n",
        "else:\n",
        "    print(\"‚ùå No runs found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "‚úÖ **What we accomplished:**\n",
        "1. Fetched historical features from Feast\n",
        "2. Trained a PyTorch model locally with MLflow tracking\n",
        "3. Submitted distributed training via Kubeflow\n",
        "4. Registered the best model in MLflow Model Registry\n",
        "\n",
        "**Next:** `04-inference.ipynb` - Use online features for real-time predictions\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
