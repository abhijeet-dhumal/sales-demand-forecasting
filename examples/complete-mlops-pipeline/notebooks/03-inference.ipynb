{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Inference with Online Features\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. **Load model** from MLflow Model Registry\n",
        "2. **Get online features** from Feast for real-time prediction\n",
        "3. **Make predictions** and visualize results\n",
        "\n",
        "## Prerequisites\n",
        "- Completed `02-feast-features.ipynb` (features materialized)\n",
        "- Completed `03-training.ipynb` (model registered in MLflow)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from feast import FeatureStore\n",
        "import joblib\n",
        "\n",
        "# Configuration - aligned with example manifests\n",
        "NAMESPACE = os.environ.get(\"NAMESPACE\", \"feast-mlops-demo\")\n",
        "MLFLOW_TRACKING_URI = f\"http://mlflow.{NAMESPACE}.svc.cluster.local:5000\"\n",
        "SHARED_DIR = os.environ.get(\"SHARED_DIR\", \"/shared\")\n",
        "os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_TRACKING_URI\n",
        "\n",
        "print(f\"\"\"\n",
        "Configuration:\n",
        "  Namespace: {NAMESPACE}\n",
        "  MLflow: {MLFLOW_TRACKING_URI}\n",
        "  Shared Storage: {SHARED_DIR}\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Load Model from MLflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load production model from MLflow\n",
        "model_name = \"sales-forecast-model\"\n",
        "model_uri = f\"models:/{model_name}/Production\"\n",
        "\n",
        "try:\n",
        "    model = mlflow.pytorch.load_model(model_uri)\n",
        "    print(f\"‚úÖ Loaded model: {model_name} (Production)\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not load from MLflow: {e}\")\n",
        "    print(\"Loading from local file...\")\n",
        "    \n",
        "    # Define model architecture\n",
        "    class SalesForecastModel(nn.Module):\n",
        "        def __init__(self, input_dim=9):\n",
        "            super().__init__()\n",
        "            self.network = nn.Sequential(\n",
        "                nn.Linear(input_dim, 128),\n",
        "                nn.BatchNorm1d(128),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.2),\n",
        "                nn.Linear(128, 64),\n",
        "                nn.BatchNorm1d(64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, 32),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(32, 1)\n",
        "            )\n",
        "        def forward(self, x):\n",
        "            return self.network(x).squeeze(-1)\n",
        "    \n",
        "    model = SalesForecastModel()\n",
        "    model.load_state_dict(torch.load(f\"{SHARED_DIR}/models/sales_forecast_model.pt\"))\n",
        "    print(\"‚úÖ Loaded model from local file\")\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load scalers\n",
        "scalers = joblib.load(f\"{SHARED_DIR}/models/scalers.joblib\")\n",
        "scaler_X = scalers[\"scaler_X\"]\n",
        "scaler_y = scalers[\"scaler_y\"]\n",
        "print(\"‚úÖ Loaded scalers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Get Online Features from Feast\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Feast\n",
        "REPO_DIR = Path(SHARED_DIR) / \"feature_repo\"\n",
        "fs = FeatureStore(repo_path=str(REPO_DIR))\n",
        "print(f\"Feast project: {fs.project}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate real-time prediction request\n",
        "# In production, these would come from incoming requests\n",
        "entity_rows = [\n",
        "    {\"store_id\": 1, \"dept_id\": 1},\n",
        "    {\"store_id\": 1, \"dept_id\": 5},\n",
        "    {\"store_id\": 10, \"dept_id\": 3},\n",
        "    {\"store_id\": 25, \"dept_id\": 7},\n",
        "    {\"store_id\": 45, \"dept_id\": 10},\n",
        "]\n",
        "\n",
        "# Get online features (low-latency from PostgreSQL)\n",
        "print(\"üöÄ Fetching online features...\")\n",
        "feature_vector = fs.get_online_features(\n",
        "    features=[\n",
        "        \"sales_features:lag_1\",\n",
        "        \"sales_features:lag_2\",\n",
        "        \"sales_features:lag_4\",\n",
        "        \"sales_features:rolling_mean_4w\",\n",
        "        \"store_features:store_size\",\n",
        "        \"store_features:temperature\",\n",
        "        \"store_features:fuel_price\",\n",
        "        \"store_features:cpi\",\n",
        "        \"store_features:unemployment\",\n",
        "    ],\n",
        "    entity_rows=entity_rows\n",
        ").to_df()\n",
        "\n",
        "print(f\"‚úÖ Retrieved features for {len(feature_vector)} entities\")\n",
        "feature_vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Make Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features for prediction\n",
        "feature_cols = [\"lag_1\", \"lag_2\", \"lag_4\", \"rolling_mean_4w\",\n",
        "                \"store_size\", \"temperature\", \"fuel_price\", \"cpi\", \"unemployment\"]\n",
        "\n",
        "# Handle missing values (use median imputation)\n",
        "X = feature_vector[feature_cols].fillna(feature_vector[feature_cols].median()).values\n",
        "\n",
        "# Scale features\n",
        "X_scaled = scaler_X.transform(X)\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    X_tensor = torch.FloatTensor(X_scaled)\n",
        "    predictions_scaled = model(X_tensor).numpy()\n",
        "\n",
        "# Inverse transform to get actual values\n",
        "predictions = scaler_y.inverse_transform(predictions_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Create results dataframe\n",
        "results = pd.DataFrame({\n",
        "    \"store_id\": [e[\"store_id\"] for e in entity_rows],\n",
        "    \"dept_id\": [e[\"dept_id\"] for e in entity_rows],\n",
        "    \"predicted_weekly_sales\": predictions.round(2)\n",
        "})\n",
        "\n",
        "print(\"üìä Predictions:\")\n",
        "results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "# Create labels\n",
        "labels = [f\"Store {r['store_id']}\\nDept {r['dept_id']}\" for _, r in results.iterrows()]\n",
        "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(results)))\n",
        "\n",
        "bars = ax.barh(labels, results[\"predicted_weekly_sales\"], color=colors)\n",
        "ax.set_xlabel(\"Predicted Weekly Sales ($)\")\n",
        "ax.set_title(\"Real-Time Sales Forecasts\")\n",
        "\n",
        "# Add value labels\n",
        "for bar, val in zip(bars, results[\"predicted_weekly_sales\"]):\n",
        "    ax.text(val + 50, bar.get_y() + bar.get_height()/2, \n",
        "            f\"${val:,.0f}\", va='center', fontweight='bold')\n",
        "\n",
        "ax.set_xlim(0, max(results[\"predicted_weekly_sales\"]) * 1.2)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Batch Inference (Optional)\n",
        "\n",
        "For large-scale batch predictions, use the offline store.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime, timezone\n",
        "\n",
        "# Create entity DataFrame for batch inference\n",
        "# All store-dept combinations for current week\n",
        "stores = list(range(1, 51))\n",
        "depts = list(range(1, 13))\n",
        "now = datetime.now(timezone.utc)\n",
        "\n",
        "batch_entities = pd.DataFrame([\n",
        "    {\"store_id\": s, \"dept_id\": d, \"event_timestamp\": now}\n",
        "    for s in stores for d in depts\n",
        "])\n",
        "\n",
        "print(f\"Batch inference for {len(batch_entities):,} entity combinations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch batch features (via Ray for distributed processing)\n",
        "print(\"üöÄ Fetching batch features via Ray...\")\n",
        "\n",
        "batch_features = fs.get_historical_features(\n",
        "    entity_df=batch_entities,\n",
        "    features=[\n",
        "        \"sales_features:lag_1\",\n",
        "        \"sales_features:lag_2\",\n",
        "        \"sales_features:lag_4\",\n",
        "        \"sales_features:rolling_mean_4w\",\n",
        "        \"store_features:store_size\",\n",
        "        \"store_features:temperature\",\n",
        "        \"store_features:fuel_price\",\n",
        "        \"store_features:cpi\",\n",
        "        \"store_features:unemployment\",\n",
        "    ]\n",
        ").to_df()\n",
        "\n",
        "print(f\"‚úÖ Retrieved {len(batch_features):,} feature rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch predictions\n",
        "X_batch = batch_features[feature_cols].fillna(batch_features[feature_cols].median()).values\n",
        "X_batch_scaled = scaler_X.transform(X_batch)\n",
        "\n",
        "with torch.no_grad():\n",
        "    batch_preds_scaled = model(torch.FloatTensor(X_batch_scaled)).numpy()\n",
        "\n",
        "batch_preds = scaler_y.inverse_transform(batch_preds_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "batch_features[\"predicted_sales\"] = batch_preds\n",
        "\n",
        "# Aggregate by store\n",
        "store_forecast = batch_features.groupby(\"store_id\")[\"predicted_sales\"].sum().reset_index()\n",
        "store_forecast.columns = [\"store_id\", \"total_predicted_sales\"]\n",
        "store_forecast = store_forecast.sort_values(\"total_predicted_sales\", ascending=False)\n",
        "\n",
        "print(\"üè™ Top 10 Stores by Predicted Weekly Sales:\")\n",
        "store_forecast.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "‚úÖ **What we accomplished:**\n",
        "1. Loaded production model from MLflow Model Registry\n",
        "2. Fetched online features from Feast (low-latency PostgreSQL)\n",
        "3. Made real-time predictions for specific store-department combinations\n",
        "4. Performed batch inference using historical features (distributed via Ray)\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ End-to-End Pipeline Complete!\n",
        "\n",
        "| Stage | Component | What Happened |\n",
        "|-------|-----------|---------------|\n",
        "| **Features** | Feast + Ray | Distributed feature computation |\n",
        "| **Training** | Kubeflow + MLflow | Distributed training, experiment tracking |\n",
        "| **Serving** | MLflow Registry | Model versioning, deployment staging |\n",
        "| **Inference** | Feast Online Store | Low-latency feature serving |\n",
        "\n",
        "**Production considerations:**\n",
        "- Deploy Feast feature server for HTTP-based feature serving\n",
        "- Use KServe for model serving with autoscaling\n",
        "- Set up MLflow Model Registry webhooks for CI/CD\n",
        "- Configure monitoring dashboards for feature drift detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
