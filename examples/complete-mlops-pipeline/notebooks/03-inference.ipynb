{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sales Forecasting - Inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q kserve requests numpy yamlmagic\n",
        "%load_ext yamlmagic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%yaml parameters\n",
        "\n",
        "# =============================================================================\n",
        "# Cluster Configuration\n",
        "# =============================================================================\n",
        "namespace: feast-trainer-demo\n",
        "shared_pvc: feast-pvc\n",
        "\n",
        "# =============================================================================\n",
        "# Model Configuration\n",
        "# =============================================================================\n",
        "model_name: sales-forecast\n",
        "\n",
        "# =============================================================================\n",
        "# Serving Configuration (use training image - has PyTorch)\n",
        "# =============================================================================\n",
        "serving_image: quay.io/modh/training:py311-cuda124-torch251\n",
        "\n",
        "resources:\n",
        "  cpu_request: \"500m\"\n",
        "  cpu_limit: \"2\"\n",
        "  memory_request: 1Gi\n",
        "  memory_limit: 4Gi\n",
        "\n",
        "# =============================================================================\n",
        "# Data Paths (must match training - PVC mounted at /shared)\n",
        "# =============================================================================\n",
        "paths:\n",
        "  model_dir: /shared/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract parameters\n",
        "NAMESPACE = parameters['namespace']\n",
        "MODEL_NAME = parameters['model_name']\n",
        "SHARED_PVC = parameters['shared_pvc']\n",
        "SERVING_IMAGE = parameters['serving_image']\n",
        "CPU_LIMIT = parameters['resources']['cpu_limit']\n",
        "MEMORY_LIMIT = parameters['resources']['memory_limit']\n",
        "CPU_REQUEST = parameters['resources']['cpu_request']\n",
        "MEMORY_REQUEST = parameters['resources']['memory_request']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Authentication\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, time, urllib3\n",
        "from datetime import datetime\n",
        "\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "K8S_TOKEN = os.getenv(\"K8S_TOKEN\", \"<YOUR_TOKEN>\")\n",
        "K8S_API_SERVER = os.getenv(\"K8S_API_SERVER\", \"<YOUR_API_SERVER>\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kubernetes import client as k8s\n",
        "from kserve import KServeClient, V1beta1InferenceService, V1beta1InferenceServiceSpec, V1beta1PredictorSpec\n",
        "\n",
        "cfg = k8s.Configuration()\n",
        "cfg.host = K8S_API_SERVER\n",
        "cfg.verify_ssl = False\n",
        "cfg.api_key = {\"authorization\": f\"Bearer {K8S_TOKEN}\"}\n",
        "k8s.Configuration.set_default(cfg)\n",
        "\n",
        "core_api = k8s.CoreV1Api()\n",
        "custom_api = k8s.CustomObjectsApi()\n",
        "kserve_client = KServeClient()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Serving Script\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SERVE_SCRIPT = '''#!/usr/bin/env python3\n",
        "\"\"\"Sales forecasting inference server - loads model architecture from metadata\"\"\"\n",
        "import os, json, torch, torch.nn as nn, joblib, numpy as np\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "model, scalers, feature_cols, metadata = None, None, None, None\n",
        "\n",
        "\n",
        "def build_model(input_dim, hidden_dims, dropout=0.2):\n",
        "    \"\"\"Build model dynamically from metadata\"\"\"\n",
        "    layers = []\n",
        "    prev_dim = input_dim\n",
        "    for h_dim in hidden_dims:\n",
        "        layers.extend([nn.Linear(prev_dim, h_dim), nn.BatchNorm1d(h_dim), nn.ReLU(), nn.Dropout(dropout)])\n",
        "        prev_dim = h_dim\n",
        "    layers.append(nn.Linear(prev_dim, 1))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class SalesMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims):\n",
        "        super().__init__()\n",
        "        self.net = build_model(input_dim, hidden_dims)\n",
        "    def forward(self, x): return self.net(x).squeeze(-1)\n",
        "\n",
        "\n",
        "def load_model():\n",
        "    global model, scalers, feature_cols, metadata\n",
        "    model_dir = os.getenv(\"MODEL_DIR\", \"/shared/models\")\n",
        "    \n",
        "    with open(f\"{model_dir}/model_metadata.json\") as f:\n",
        "        metadata = json.load(f)\n",
        "    \n",
        "    hidden_dims = metadata.get(\"hidden_dims\", [256, 128, 64])\n",
        "    model = SalesMLP(metadata[\"input_dim\"], hidden_dims)\n",
        "    model.load_state_dict(torch.load(f\"{model_dir}/best_model.pt\", map_location=\"cpu\", weights_only=True))\n",
        "    model.eval()\n",
        "    \n",
        "    scalers = joblib.load(f\"{model_dir}/scalers.joblib\")\n",
        "    feature_cols = metadata[\"feature_columns\"]\n",
        "    print(f\"Loaded: {len(feature_cols)} features, arch={hidden_dims}\")\n",
        "\n",
        "\n",
        "@app.route(\"/health\", methods=[\"GET\"])\n",
        "def health():\n",
        "    return jsonify({\"status\": \"healthy\", \"model\": \"sales-forecast\"})\n",
        "\n",
        "\n",
        "@app.route(\"/v1/models/sales-forecast\", methods=[\"GET\"])\n",
        "def model_info():\n",
        "    return jsonify({\n",
        "        \"name\": \"sales-forecast\",\n",
        "        \"features\": feature_cols,\n",
        "        \"hidden_dims\": metadata.get(\"hidden_dims\"),\n",
        "        \"best_loss\": metadata.get(\"best_loss\"),\n",
        "        \"device_trained\": metadata.get(\"device_type\", \"unknown\")\n",
        "    })\n",
        "\n",
        "\n",
        "@app.route(\"/v1/models/sales-forecast:predict\", methods=[\"POST\"])\n",
        "def predict():\n",
        "    instances = request.json.get(\"instances\", [])\n",
        "    X = np.array([[inst.get(c, 0) if isinstance(inst, dict) else inst[i] for i, c in enumerate(feature_cols)] for inst in instances])\n",
        "    X_scaled = scalers[\"scaler_X\"].transform(X)\n",
        "    with torch.no_grad():\n",
        "        preds = model(torch.FloatTensor(X_scaled)).numpy()\n",
        "    return jsonify({\"predictions\": scalers[\"scaler_y\"].inverse_transform(preds.reshape(-1, 1)).flatten().tolist()})\n",
        "\n",
        "\n",
        "@app.route(\"/v1/models/sales-forecast:explain\", methods=[\"POST\"])\n",
        "def explain():\n",
        "    weights = model.net[0].weight.abs().mean(dim=0).detach().numpy()\n",
        "    importance = {f: float(w)/weights.sum() for f, w in zip(feature_cols, weights)}\n",
        "    return jsonify({\"feature_importance\": dict(sorted(importance.items(), key=lambda x: -x[1]))})\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    load_model()\n",
        "    app.run(host=\"0.0.0.0\", port=8080)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "deploy_id = datetime.now().strftime(\"%m%d-%H%M\")\n",
        "labels = {\"app\": \"sales-forecasting\", \"deploy-id\": deploy_id}\n",
        "cm_name = f\"{MODEL_NAME}-serve\"\n",
        "\n",
        "try:\n",
        "    core_api.delete_namespaced_config_map(cm_name, NAMESPACE)\n",
        "except: pass\n",
        "\n",
        "core_api.create_namespaced_config_map(\n",
        "    NAMESPACE,\n",
        "    k8s.V1ConfigMap(\n",
        "        metadata=k8s.V1ObjectMeta(name=cm_name, labels=labels),\n",
        "        data={\"serve.py\": SERVE_SCRIPT}\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    kserve_client.delete(MODEL_NAME, namespace=NAMESPACE)\n",
        "    for _ in range(30):\n",
        "        try: kserve_client.get(MODEL_NAME, namespace=NAMESPACE); time.sleep(2)\n",
        "        except: break\n",
        "except: pass\n",
        "\n",
        "isvc = V1beta1InferenceService(\n",
        "    api_version=\"serving.kserve.io/v1beta1\",\n",
        "    kind=\"InferenceService\",\n",
        "    metadata=k8s.V1ObjectMeta(name=MODEL_NAME, namespace=NAMESPACE, labels=labels),\n",
        "    spec=V1beta1InferenceServiceSpec(\n",
        "        predictor=V1beta1PredictorSpec(\n",
        "            containers=[k8s.V1Container(\n",
        "                name=\"kserve-container\",\n",
        "                image=SERVING_IMAGE,\n",
        "                command=[\"/bin/bash\", \"-c\", \"pip install -q flask joblib numpy scikit-learn && python /scripts/serve.py\"],\n",
        "                env=[k8s.V1EnvVar(name=\"MODEL_DIR\", value=\"/shared/models\")],\n",
        "                ports=[k8s.V1ContainerPort(container_port=8080, protocol=\"TCP\")],\n",
        "                volume_mounts=[\n",
        "                    k8s.V1VolumeMount(name=\"model-storage\", mount_path=\"/shared\"),\n",
        "                    k8s.V1VolumeMount(name=\"serve-script\", mount_path=\"/scripts\")\n",
        "                ],\n",
        "                resources=k8s.V1ResourceRequirements(\n",
        "                    limits={\"cpu\": CPU_LIMIT, \"memory\": MEMORY_LIMIT},\n",
        "                    requests={\"cpu\": CPU_REQUEST, \"memory\": MEMORY_REQUEST}\n",
        "                )\n",
        "            )],\n",
        "            volumes=[\n",
        "                k8s.V1Volume(name=\"model-storage\", persistent_volume_claim=k8s.V1PersistentVolumeClaimVolumeSource(claim_name=SHARED_PVC)),\n",
        "                k8s.V1Volume(name=\"serve-script\", config_map=k8s.V1ConfigMapVolumeSource(name=cm_name))\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        ")\n",
        "kserve_client.create(isvc, namespace=NAMESPACE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kserve_client.wait_isvc_ready(MODEL_NAME, namespace=NAMESPACE, timeout_seconds=300)\n",
        "isvc_status = kserve_client.get(MODEL_NAME, namespace=NAMESPACE)\n",
        "internal_url = isvc_status.get(\"status\", {}).get(\"url\", \"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create non-headless service for route (KServe creates headless service by default)\n",
        "external_svc_name = f\"{MODEL_NAME}-external\"\n",
        "try: core_api.delete_namespaced_service(external_svc_name, NAMESPACE)\n",
        "except: pass\n",
        "\n",
        "core_api.create_namespaced_service(\n",
        "    NAMESPACE,\n",
        "    k8s.V1Service(\n",
        "        metadata=k8s.V1ObjectMeta(name=external_svc_name, labels=labels),\n",
        "        spec=k8s.V1ServiceSpec(\n",
        "            selector={\"app\": f\"isvc.{MODEL_NAME}-predictor\"},\n",
        "            ports=[k8s.V1ServicePort(name=\"http\", port=8080, target_port=8080)],\n",
        "            type=\"ClusterIP\"\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "route = {\n",
        "    \"apiVersion\": \"route.openshift.io/v1\",\n",
        "    \"kind\": \"Route\",\n",
        "    \"metadata\": {\"name\": MODEL_NAME, \"namespace\": NAMESPACE, \"labels\": labels},\n",
        "    \"spec\": {\n",
        "        \"to\": {\"kind\": \"Service\", \"name\": external_svc_name, \"weight\": 100},\n",
        "        \"port\": {\"targetPort\": \"http\"},\n",
        "        \"tls\": {\"termination\": \"edge\", \"insecureEdgeTerminationPolicy\": \"Redirect\"}\n",
        "    }\n",
        "}\n",
        "\n",
        "try: custom_api.delete_namespaced_custom_object(\"route.openshift.io\", \"v1\", NAMESPACE, \"routes\", MODEL_NAME)\n",
        "except: pass\n",
        "\n",
        "custom_api.create_namespaced_custom_object(\"route.openshift.io\", \"v1\", NAMESPACE, \"routes\", route)\n",
        "route_info = custom_api.get_namespaced_custom_object(\"route.openshift.io\", \"v1\", NAMESPACE, \"routes\", MODEL_NAME)\n",
        "EXTERNAL_URL = f\"https://{route_info.get('spec', {}).get('host', '')}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests, numpy as np\n",
        "\n",
        "class InferenceClient:\n",
        "    def __init__(self, url, token=None):\n",
        "        self.url = url.rstrip(\"/\")\n",
        "        self.session = requests.Session()\n",
        "        self.session.verify = False\n",
        "        if token: self.session.headers[\"Authorization\"] = f\"Bearer {token}\"\n",
        "    \n",
        "    def health(self): return self.session.get(f\"{self.url}/health\", timeout=10).json()\n",
        "    def info(self): return self.session.get(f\"{self.url}/v1/models/{MODEL_NAME}\", timeout=10).json()\n",
        "    def predict(self, instances): return self.session.post(f\"{self.url}/v1/models/{MODEL_NAME}:predict\", json={\"instances\": instances}, timeout=30).json()\n",
        "    def explain(self): return self.session.post(f\"{self.url}/v1/models/{MODEL_NAME}:explain\", json={}, timeout=30).json()\n",
        "\n",
        "client = InferenceClient(EXTERNAL_URL, K8S_TOKEN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time.sleep(10)  # Wait for server startup\n",
        "client.health()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample prediction\n",
        "sample = {\n",
        "    \"lag_1\": 25000, \"lag_2\": 24000, \"lag_4\": 23000, \"lag_8\": 22000, \"lag_52\": 20000,\n",
        "    \"rolling_mean_4w\": 24500, \"store_size\": 150000, \"temperature\": 65.0,\n",
        "    \"fuel_price\": 2.8, \"cpi\": 215.0, \"unemployment\": 5.5\n",
        "}\n",
        "client.predict([sample])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Latency test (10 requests)\n",
        "import time\n",
        "times = []\n",
        "for _ in range(10):\n",
        "    t0 = time.time()\n",
        "    client.predict([sample])\n",
        "    times.append((time.time() - t0) * 1000)\n",
        "f\"Mean: {np.mean(times):.0f}ms, P95: {np.percentile(times, 95):.0f}ms\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to delete:\n",
        "# kserve_client.delete(MODEL_NAME, namespace=NAMESPACE)\n",
        "# custom_api.delete_namespaced_custom_object(\"route.openshift.io\", \"v1\", NAMESPACE, \"routes\", MODEL_NAME)\n",
        "# core_api.delete_namespaced_service(external_svc_name, NAMESPACE)\n",
        "# core_api.delete_namespaced_config_map(cm_name, NAMESPACE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f\"Model: {MODEL_NAME} | URL: {EXTERNAL_URL}\"\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
