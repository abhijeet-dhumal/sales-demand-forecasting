{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feast Feature Engineering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q \"feast[postgres,ray]==0.56.0\" pandas pyarrow psycopg2-binary yamlmagic\n",
        "%load_ext yamlmagic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%yaml parameters\n",
        "\n",
        "# =============================================================================\n",
        "# Paths (must match training notebook: /shared/data)\n",
        "# =============================================================================\n",
        "feature_repo: ../feature_repo\n",
        "data_dir: /shared/data\n",
        "\n",
        "# =============================================================================\n",
        "# Data Generation\n",
        "# =============================================================================\n",
        "data:\n",
        "  start_date: \"2010-02-05\"\n",
        "  weeks: 143\n",
        "  stores: 45\n",
        "  departments: 99\n",
        "  seed: 42\n",
        "\n",
        "# =============================================================================\n",
        "# Feast Materialize Window\n",
        "# =============================================================================\n",
        "materialize:\n",
        "  start: \"2010-02-05T00:00:00\"\n",
        "  end: \"2013-01-01T00:00:00\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from pathlib import Path\n",
        "\n",
        "FEATURE_REPO = Path(parameters['feature_repo']).resolve()\n",
        "DATA_DIR = Path(parameters['data_dir'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Sales Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(parameters['data']['seed'])\n",
        "base_date = datetime.fromisoformat(parameters['data']['start_date']).replace(tzinfo=timezone.utc)\n",
        "weeks = parameters['data']['weeks']\n",
        "stores = parameters['data']['stores']\n",
        "depts = parameters['data']['departments']\n",
        "\n",
        "records = []\n",
        "for week in range(weeks):\n",
        "    week_date = base_date + timedelta(weeks=week)\n",
        "    week_of_year = week_date.isocalendar()[1]\n",
        "    month = week_date.month\n",
        "    quarter = (month - 1) // 3 + 1\n",
        "    \n",
        "    seasonal = 1 + 0.3 * np.sin(2 * np.pi * week_of_year / 52)\n",
        "    is_holiday = 1 if week_of_year in [6, 36, 47, 51, 52] else 0\n",
        "    holiday_factor = 1.5 if is_holiday else 1.0\n",
        "    \n",
        "    for store_id in range(1, stores + 1):\n",
        "        store_base = 15000 + store_id * 500\n",
        "        store_type = [\"A\", \"B\", \"C\"][store_id % 3]\n",
        "        store_size = {\"A\": 200000, \"B\": 150000, \"C\": 100000}[store_type]\n",
        "        \n",
        "        for dept_id in range(1, depts + 1):\n",
        "            dept_factor = 0.2 + (dept_id % 10) * 0.1\n",
        "            weekly_sales = max(0, store_base * dept_factor * seasonal * holiday_factor + np.random.normal(0, 1000))\n",
        "            \n",
        "            has_markdown = 1 if np.random.random() > 0.7 else 0\n",
        "            total_markdown = sum([np.random.uniform(0, x) if has_markdown else 0 for x in [5000, 2000, 500, 1000, 3000]])\n",
        "            \n",
        "            records.append({\n",
        "                \"store\": store_id, \"dept\": dept_id, \"date\": week_date,\n",
        "                \"weekly_sales\": round(weekly_sales, 2), \"is_holiday\": is_holiday,\n",
        "                \"week_of_year\": week_of_year, \"month\": month, \"quarter\": quarter,\n",
        "                \"temperature\": round(50 + 30 * np.sin(2 * np.pi * week_of_year / 52) + np.random.normal(0, 5), 1),\n",
        "                \"fuel_price\": round(2.5 + 0.5 * np.sin(2 * np.pi * week / 52) + np.random.normal(0, 0.1), 2),\n",
        "                \"cpi\": round(210 + week * 0.05 + np.random.normal(0, 1), 2),\n",
        "                \"unemployment\": round(max(4, 8 - week * 0.02 + np.random.normal(0, 0.3)), 1),\n",
        "                \"total_markdown\": round(total_markdown, 2), \"has_markdown\": has_markdown,\n",
        "                \"store_type\": store_type, \"store_size\": store_size\n",
        "            })\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "df = df.sort_values([\"store\", \"dept\", \"date\"]).reset_index(drop=True)\n",
        "\n",
        "# Add lag features\n",
        "for lag in [1, 2, 4]:\n",
        "    df[f\"sales_lag_{lag}\"] = df.groupby([\"store\", \"dept\"])[\"weekly_sales\"].shift(lag)\n",
        "df[\"sales_rolling_mean_4\"] = df.groupby([\"store\", \"dept\"])[\"weekly_sales\"].transform(lambda x: x.rolling(4, min_periods=1).mean())\n",
        "df[\"sales_rolling_mean_12\"] = df.groupby([\"store\", \"dept\"])[\"weekly_sales\"].transform(lambda x: x.rolling(12, min_periods=1).mean())\n",
        "df[\"sales_rolling_std_4\"] = df.groupby([\"store\", \"dept\"])[\"weekly_sales\"].transform(lambda x: x.rolling(4, min_periods=1).std())\n",
        "\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "sales_cols = [\"store\", \"dept\", \"date\", \"weekly_sales\", \"is_holiday\", \"week_of_year\", \"month\", \"quarter\",\n",
        "              \"sales_lag_1\", \"sales_lag_2\", \"sales_lag_4\", \"sales_rolling_mean_4\", \"sales_rolling_mean_12\", \"sales_rolling_std_4\"]\n",
        "store_cols = [\"store\", \"dept\", \"date\", \"temperature\", \"fuel_price\", \"cpi\", \"unemployment\",\n",
        "              \"total_markdown\", \"has_markdown\", \"store_type\", \"store_size\"]\n",
        "\n",
        "df[sales_cols].to_parquet(DATA_DIR / \"sales_features.parquet\", index=False)\n",
        "df[store_cols].to_parquet(DATA_DIR / \"store_features.parquet\", index=False)\n",
        "df.to_parquet(DATA_DIR / \"features.parquet\", index=False)\n",
        "\n",
        "!ls -lh {DATA_DIR}/*.parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feast Apply\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash -s \"$FEATURE_REPO\"\n",
        "cd $1 && feast apply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feast Materialize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.run([\n",
        "    \"feast\", \"materialize\",\n",
        "    parameters['materialize']['start'],\n",
        "    parameters['materialize']['end']\n",
        "], cwd=str(FEATURE_REPO), check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Query Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feast import FeatureStore\n",
        "\n",
        "store = FeatureStore(repo_path=str(FEATURE_REPO))\n",
        "store.list_feature_views()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Online features (low-latency serving)\n",
        "online = store.get_online_features(\n",
        "    features=[\"sales_features:weekly_sales\", \"sales_features:sales_lag_1\", \"store_features:temperature\"],\n",
        "    entity_rows=[{\"store\": 1, \"dept\": 1}]\n",
        ").to_dict()\n",
        "online\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Historical features (training data)\n",
        "entity_df = pd.DataFrame({\n",
        "    \"store\": [1, 1, 2],\n",
        "    \"dept\": [1, 2, 1],\n",
        "    \"event_timestamp\": pd.to_datetime([\"2012-06-01\", \"2012-06-01\", \"2012-06-01\"], utc=True)\n",
        "})\n",
        "\n",
        "historical = store.get_historical_features(\n",
        "    entity_df=entity_df,\n",
        "    features=[\"sales_features:weekly_sales\", \"sales_features:sales_lag_1\", \"store_features:store_size\"]\n",
        ").to_df()\n",
        "historical\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to teardown:\n",
        "# !cd {FEATURE_REPO} && feast teardown\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
