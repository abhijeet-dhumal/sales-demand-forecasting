{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Feast Feature Store Setup\n",
    "\n",
    "**Data Generation ‚Üí Feast Apply ‚Üí Materialize ‚Üí Verify**\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "| Toggle | Operation | `false` (default) | `true` |\n",
    "|--------|-----------|-------------------|--------|\n",
    "| `use_ray` | Training `get_historical_features()` | File-based (single node) | KubeRay (distributed PIT joins) |\n",
    "| `use_ray_dataprep` | DataPrep `feast materialize` | Single-node | Ray batch_engine (big data) |\n",
    "\n",
    "### When to Enable Each\n",
    "\n",
    "| Data Size | `use_ray` | `use_ray_dataprep` | Notes |\n",
    "|-----------|-----------|-------------------|-------|\n",
    "| **<1M rows** | `false` | `false` | Quick start, no Ray needed |\n",
    "| **1-10M rows** | `true` | `false` | Ray for training PIT joins only |\n",
    "| **>10M rows** | `true` | `true` | Full Ray: training + materialization |\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    FEAST CONFIGURATION                      ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  feature_store.yaml       (File offline + optional Ray)     ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ feast apply           ‚úÖ Always single-node             ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ feast materialize     üîÄ Ray batch_engine if enabled    ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ batch_engine: ray     ‚Üí Only if use_ray_dataprep=true   ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  feature_store_ray.yaml   (Ray/KubeRay offline store)       ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ get_historical_features() ‚Üí Only if use_ray=true        ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"feast[postgres]==0.59.0\" pandas pyarrow psycopg2-binary pyyaml yamlmagic\n",
    "%load_ext yamlmagic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%yaml config\n",
    "\n",
    "# =============================================================================\n",
    "# üîÄ MODE SELECTION - Set this based on your cluster and data size\n",
    "# =============================================================================\n",
    "# Training: get_historical_features() - point-in-time joins\n",
    "use_ray: true           # true = KubeRay for distributed PIT joins, false = file-based\n",
    "\n",
    "# DataPrep: feast materialize - sync offline ‚Üí online store\n",
    "use_ray_dataprep: false # true = Ray batch_engine for big data (>1M rows), false = single-node\n",
    "\n",
    "# =============================================================================\n",
    "# Paths (shared PVC)\n",
    "# =============================================================================\n",
    "feature_repo: /shared/feature_repo\n",
    "data_dir: /shared/data\n",
    "\n",
    "# =============================================================================\n",
    "# Data Generation\n",
    "# =============================================================================\n",
    "data:\n",
    "  start_date: \"2022-01-01\"\n",
    "  weeks: 104          # 2 years of weekly data\n",
    "  stores: 10\n",
    "  departments: 5\n",
    "  seed: 42\n",
    "\n",
    "# =============================================================================\n",
    "# Feast (PostgreSQL backend)\n",
    "# =============================================================================\n",
    "feast:\n",
    "  project: sales_forecasting\n",
    "  postgres_host: postgres.feast-trainer-demo.svc.cluster.local\n",
    "  postgres_port: 5432\n",
    "  postgres_db: feast\n",
    "  postgres_user: feast\n",
    "  postgres_password: feast123\n",
    "\n",
    "# =============================================================================\n",
    "# KubeRay (only used if use_ray: true)\n",
    "# =============================================================================\n",
    "kuberay:\n",
    "  cluster_name: feast-ray\n",
    "  namespace: feast-trainer-demo\n",
    "  skip_tls: true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "FEATURE_REPO = Path(config['feature_repo'])\n",
    "DATA_DIR = Path(config['data_dir'])\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Feature Repo: {FEATURE_REPO}\")\n",
    "print(f\"Data Dir: {DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sales Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic sales data with lag features\n",
    "np.random.seed(config['data']['seed'])\n",
    "base_date = datetime.fromisoformat(config['data']['start_date']).replace(tzinfo=timezone.utc)\n",
    "weeks = config['data']['weeks']\n",
    "stores = config['data']['stores']\n",
    "depts = config['data']['departments']\n",
    "\n",
    "print(f\"Generating {weeks} weeks √ó {stores} stores √ó {depts} depts = {weeks*stores*depts:,} records...\")\n",
    "\n",
    "sales_records = []\n",
    "for week in range(weeks):\n",
    "    week_date = base_date + timedelta(weeks=week)\n",
    "    week_of_year = week_date.isocalendar()[1]\n",
    "    \n",
    "    # Seasonality\n",
    "    seasonal = 1 + 0.3 * np.sin(2 * np.pi * week_of_year / 52)\n",
    "    is_holiday = 1 if week_of_year in [6, 36, 47, 51, 52] else 0  # Super Bowl, Labor Day, Thanksgiving, Christmas\n",
    "    holiday_factor = 1.5 if is_holiday else 1.0\n",
    "    \n",
    "    for store_id in range(1, stores + 1):\n",
    "        store_base = 15000 + store_id * 500\n",
    "        for dept_id in range(1, depts + 1):\n",
    "            dept_factor = 0.5 + (dept_id % 5) * 0.2\n",
    "            weekly_sales = max(0, store_base * dept_factor * seasonal * holiday_factor + np.random.normal(0, 2000))\n",
    "            \n",
    "            sales_records.append({\n",
    "                \"store_id\": store_id,\n",
    "                \"dept_id\": dept_id,\n",
    "                \"event_timestamp\": week_date,\n",
    "                \"weekly_sales\": round(weekly_sales, 2),\n",
    "                \"is_holiday\": is_holiday,\n",
    "                \"temperature\": round(50 + 30 * np.sin(2 * np.pi * week_of_year / 52) + np.random.normal(0, 5), 1),\n",
    "                \"fuel_price\": round(2.5 + 0.5 * np.sin(2 * np.pi * week / 52) + np.random.normal(0, 0.1), 2),\n",
    "                \"cpi\": round(210 + week * 0.05 + np.random.normal(0, 1), 2),\n",
    "                \"unemployment\": round(max(4, 8 - week * 0.02 + np.random.normal(0, 0.3)), 1),\n",
    "            })\n",
    "\n",
    "sales_df = pd.DataFrame(sales_records)\n",
    "sales_df = sales_df.sort_values([\"store_id\", \"dept_id\", \"event_timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "# Add lag features\n",
    "for lag in [1, 2, 4, 8, 52]:\n",
    "    sales_df[f\"lag_{lag}\"] = sales_df.groupby([\"store_id\", \"dept_id\"])[\"weekly_sales\"].shift(lag)\n",
    "\n",
    "sales_df[\"rolling_mean_4w\"] = sales_df.groupby([\"store_id\", \"dept_id\"])[\"weekly_sales\"].transform(\n",
    "    lambda x: x.rolling(4, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "# Drop rows with NaN lags\n",
    "sales_df = sales_df.dropna()\n",
    "print(f\"‚úÖ Sales features: {len(sales_df):,} rows\")\n",
    "sales_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate store metadata\n",
    "store_records = []\n",
    "for store_id in range(1, stores + 1):\n",
    "    store_type = [\"A\", \"B\", \"C\"][store_id % 3]\n",
    "    store_size = {\"A\": 200000, \"B\": 150000, \"C\": 100000}[store_type]\n",
    "    region = [\"West\", \"South\", \"Midwest\", \"Northeast\"][store_id % 4]\n",
    "    \n",
    "    for dept_id in range(1, depts + 1):\n",
    "        store_records.append({\n",
    "            \"store_id\": store_id,\n",
    "            \"dept_id\": dept_id,\n",
    "            \"event_timestamp\": base_date,\n",
    "            \"store_type\": store_type,\n",
    "            \"store_size\": store_size,\n",
    "            \"region\": region,\n",
    "        })\n",
    "\n",
    "store_df = pd.DataFrame(store_records)\n",
    "print(f\"‚úÖ Store features: {len(store_df):,} rows\")\n",
    "\n",
    "# Save parquet files\n",
    "sales_df.to_parquet(DATA_DIR / \"sales_features.parquet\", index=False)\n",
    "store_df.to_parquet(DATA_DIR / \"store_features.parquet\", index=False)\n",
    "print(f\"\\n‚úÖ Saved to {DATA_DIR}:\")\n",
    "!ls -lh {DATA_DIR}/*.parquet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feast Configs\n",
    "\n",
    "Generate **two** configs:\n",
    "- `feature_store.yaml` - File-based (for apply/materialize)\n",
    "- `feature_store_ray.yaml` - KubeRay (for training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import yaml\n",
    "\n",
    "USE_RAY = config.get('use_ray', False)\n",
    "USE_RAY_DATAPREP = config.get('use_ray_dataprep', False)\n",
    "FEATURE_REPO.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîÄ Configuration:\")\n",
    "print(f\"   Training (get_historical_features): {'KubeRay' if USE_RAY else 'File-based'}\")\n",
    "print(f\"   DataPrep (materialize):             {'Ray batch_engine' if USE_RAY_DATAPREP else 'Single-node'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Source configs (version-controlled in repo)\n",
    "REPO_CONFIG_DIR = Path(\"../feature_repo\")\n",
    "\n",
    "# Copy static configs from repo to PVC\n",
    "shutil.copy(REPO_CONFIG_DIR / \"feature_store.yaml\", FEATURE_REPO / \"feature_store.yaml\")\n",
    "shutil.copy(REPO_CONFIG_DIR / \"features.py\", FEATURE_REPO / \"features.py\")\n",
    "print(f\"‚úÖ Copied feature_store.yaml and features.py from {REPO_CONFIG_DIR}\")\n",
    "\n",
    "# Add Ray batch_engine for big data (if enabled)\n",
    "if USE_RAY_DATAPREP:\n",
    "    with open(FEATURE_REPO / \"feature_store.yaml\") as f:\n",
    "        fs_config = yaml.safe_load(f)\n",
    "    \n",
    "    kr = config['kuberay']\n",
    "    fs_config[\"batch_engine\"] = {\n",
    "        \"type\": \"ray.engine\",\n",
    "        \"ray_address\": f\"ray://feast-ray-head-svc.{kr['namespace']}.svc.cluster.local:10001\",\n",
    "    }\n",
    "    \n",
    "    with open(FEATURE_REPO / \"feature_store.yaml\", \"w\") as f:\n",
    "        f.write(\"# File-based offline store + Ray batch_engine (big data)\\n\")\n",
    "        yaml.dump(fs_config, f, default_flow_style=False)\n",
    "    print(\"   + Added batch_engine: ray.engine (for distributed materialize)\")\n",
    "\n",
    "# Copy Ray config for training (if enabled)\n",
    "if USE_RAY:\n",
    "    shutil.copy(REPO_CONFIG_DIR / \"feature_store_ray.yaml\", FEATURE_REPO / \"feature_store_ray.yaml\")\n",
    "    print(f\"‚úÖ Copied feature_store_ray.yaml (KubeRay for training)\")\n",
    "else:\n",
    "    ray_yaml = FEATURE_REPO / \"feature_store_ray.yaml\"\n",
    "    if ray_yaml.exists():\n",
    "        ray_yaml.unlink()\n",
    "    print(\"‚ÑπÔ∏è  Skipped feature_store_ray.yaml (use_ray=false)\")\n",
    "\n",
    "print()\n",
    "!ls -la {FEATURE_REPO}/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feast Apply\n",
    "\n",
    "Register feature definitions to PostgreSQL registry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$FEATURE_REPO\"\n",
    "cd $1 && feast apply\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feast Materialize\n",
    "\n",
    "Populate online store (PostgreSQL) for low-latency inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize: offline store ‚Üí online store\n",
    "start_ts = config['data']['start_date'] + \"T00:00:00\"\n",
    "end_ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "!cd {FEATURE_REPO} && feast materialize {start_ts} {end_ts}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "\n",
    "store = FeatureStore(repo_path=str(FEATURE_REPO))\n",
    "print(f\"Project: {store.project}\")\n",
    "print(f\"\\nFeature Views:\")\n",
    "for fv in store.list_feature_views():\n",
    "    print(f\"  - {fv.name}: {len(fv.features)} features, ttl={fv.ttl}\")\n",
    "print(f\"\\nFeature Services:\")\n",
    "for fs in store.list_feature_services():\n",
    "    print(f\"  - {fs.name}: {fs.description}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test online feature retrieval (low-latency)\n",
    "import time\n",
    "entity_rows = [{\"store_id\": 1, \"dept_id\": 1}, {\"store_id\": 5, \"dept_id\": 3}]\n",
    "\n",
    "t0 = time.time()\n",
    "online_features = store.get_online_features(\n",
    "    features=[\n",
    "        \"sales_features:weekly_sales\",\n",
    "        \"sales_features:lag_1\",\n",
    "        \"sales_features:rolling_mean_4w\",\n",
    "        \"store_features:store_type\",\n",
    "        \"store_features:store_size\",\n",
    "    ],\n",
    "    entity_rows=entity_rows,\n",
    ").to_dict()\n",
    "latency_ms = (time.time() - t0) * 1000\n",
    "\n",
    "print(f\"‚úÖ Online features retrieved in {latency_ms:.1f}ms\")\n",
    "pd.DataFrame(online_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we created:**\n",
    "\n",
    "| Component | Location | Purpose |\n",
    "|-----------|----------|---------|\n",
    "| `sales_features.parquet` | `/shared/data/` | Weekly sales + lag features |\n",
    "| `store_features.parquet` | `/shared/data/` | Store metadata |\n",
    "| `feature_store.yaml` | `/shared/feature_repo/` | File-based (apply/materialize) |\n",
    "| `feature_store_ray.yaml` | `/shared/feature_repo/` | KubeRay (training) |\n",
    "\n",
    "**Next:** Run `02-training.ipynb` to train with `get_historical_features()` via KubeRay.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
