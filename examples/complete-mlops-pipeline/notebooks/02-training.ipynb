{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - Kubeflow Training with Feast Integration\n",
        "\n",
        "![Workflow](../docs/02-training-workflow.png)\n",
        "\n",
        "**Entity DF \u2192 Feast get_historical_features \u2192 PyTorch DDP \u2192 MLflow**\n",
        "\n",
        "**Prerequisites:** `01-feast-features.ipynb` completed, MLflow running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q kubeflow-training kubernetes \"mlflow>=3.0\"\n",
        "from datetime import datetime\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NAMESPACE = \"feast-trainer-demo\"\n",
        "PVC = \"feast-pvc\"\n",
        "RUNTIME = \"torch-distributed\"\n",
        "MLFLOW_URI = f\"http://mlflow.{NAMESPACE}.svc.cluster.local:5000\"\n",
        "EPOCHS = 50\n",
        "USE_RAY = True\n",
        "RAY_CLUSTER = \"feast-ray\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Auth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "K8S_TOKEN = os.getenv(\"K8S_TOKEN\", \"\")\n",
        "K8S_API = os.getenv(\"K8S_API_SERVER\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kubernetes import client as k8s\n",
        "from kubeflow.training import TrainerClient, CustomTrainer\n",
        "from kubeflow.training.types import KubernetesBackendConfig, PodTemplateOverrides, PodTemplateOverride, PodSpecOverride, ContainerOverride, Labels\n",
        "\n",
        "cfg = k8s.Configuration()\n",
        "if K8S_TOKEN:\n",
        "    cfg.host = K8S_API\n",
        "    cfg.verify_ssl = False\n",
        "    cfg.api_key = {\"authorization\": f\"Bearer {K8S_TOKEN}\"}\n",
        "\n",
        "trainer = TrainerClient(KubernetesBackendConfig(namespace=NAMESPACE, client_configuration=cfg if K8S_TOKEN else None))\n",
        "runtime = trainer.get_runtime(RUNTIME)\n",
        "print(f\"Runtime: {RUNTIME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_fn(params):\n",
        "    import os, json, numpy as np, pandas as pd, torch, torch.nn as nn, torch.distributed as dist, joblib, mlflow, shutil, time\n",
        "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "    from torch.utils.data import DataLoader, Dataset, DistributedSampler\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from datetime import datetime, timezone, timedelta\n",
        "    \n",
        "    dist.init_process_group(backend='nccl' if torch.cuda.is_available() else 'gloo')\n",
        "    rank, world = dist.get_rank(), dist.get_world_size()\n",
        "    device = torch.device(f\"cuda:{int(os.environ.get('LOCAL_RANK', 0))}\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"[Rank {rank}] Device: {device}\")\n",
        "    \n",
        "    OUT = params.get('output_dir', '/shared/models')\n",
        "    REPO = params.get('feature_repo', '/shared/feature_repo')\n",
        "    EPOCHS = params.get('epochs', 50)\n",
        "    \n",
        "    class MLP(nn.Module):\n",
        "        def __init__(self, inp, hidden=[512, 256, 128, 64], drop=0.3):\n",
        "            super().__init__()\n",
        "            layers = []\n",
        "            for h in hidden:\n",
        "                layers.extend([nn.Linear(inp, h), nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(drop)])\n",
        "                inp = h\n",
        "            layers.append(nn.Linear(inp, 1))\n",
        "            self.net = nn.Sequential(*layers)\n",
        "        def forward(self, x):\n",
        "            return self.net(x).squeeze(-1)\n",
        "    \n",
        "    class DS(Dataset):\n",
        "        def __init__(self, X, y):\n",
        "            self.X = torch.tensor(X, dtype=torch.float32)\n",
        "            self.y = torch.tensor(y, dtype=torch.float32)\n",
        "        def __len__(self): return len(self.X)\n",
        "        def __getitem__(self, i): return self.X[i], self.y[i]\n",
        "    \n",
        "    if rank == 0:\n",
        "        os.makedirs(OUT, exist_ok=True)\n",
        "        mlflow.set_tracking_uri(os.getenv('MLFLOW_TRACKING_URI', 'http://mlflow:5000'))\n",
        "        mlflow.set_experiment('sales-forecasting')\n",
        "        mlflow.start_run(run_name=f\"train-{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
        "        \n",
        "        # Use Ray config for Feast\n",
        "        if params.get('use_ray'):\n",
        "            os.environ['FEAST_RAY_USE_KUBERAY'] = 'true'\n",
        "            os.environ['FEAST_RAY_CLUSTER_NAME'] = params.get('ray_cluster', 'feast-ray')\n",
        "            os.environ['FEAST_RAY_NAMESPACE'] = params.get('namespace', 'feast-trainer-demo')\n",
        "            os.environ['FEAST_RAY_SKIP_TLS'] = 'true'\n",
        "            token_path = '/var/run/secrets/kubernetes.io/serviceaccount/token'\n",
        "            if os.path.exists(token_path):\n",
        "                with open(token_path) as f:\n",
        "                    os.environ['FEAST_RAY_AUTH_TOKEN'] = f.read()\n",
        "                os.environ['FEAST_RAY_AUTH_SERVER'] = f\"https://{os.environ.get('KUBERNETES_SERVICE_HOST')}:{os.environ.get('KUBERNETES_SERVICE_PORT')}\"\n",
        "            ray_cfg = f\"{REPO}/feature_store_ray.yaml\"\n",
        "            if os.path.exists(ray_cfg):\n",
        "                shutil.copy(ray_cfg, f\"{REPO}/feature_store.yaml\")\n",
        "        \n",
        "        from feast import FeatureStore\n",
        "        store = FeatureStore(repo_path=REPO)\n",
        "        \n",
        "        # Entity DF\n",
        "        entity_rows = [{'store_id': s, 'dept_id': d, 'event_timestamp': datetime(2022,1,1,tzinfo=timezone.utc) + timedelta(weeks=w)}\n",
        "            for w in range(104) for s in range(1, 46) for d in range(1, 15)]\n",
        "        entity_df = pd.DataFrame(entity_rows)\n",
        "        print(f\"Entity DF: {len(entity_df):,} rows\")\n",
        "        \n",
        "        t0 = time.time()\n",
        "        df = store.get_historical_features(entity_df=entity_df, features=store.get_feature_service('training_features')).to_df()\n",
        "        print(f\"\u2705 Feast: {len(df):,} rows in {time.time()-t0:.1f}s\")\n",
        "        \n",
        "        df = df.dropna(subset=['weekly_sales']).sort_values('event_timestamp')\n",
        "        split = int(len(df) * 0.8)\n",
        "        train_df, val_df = df.iloc[:split], df.iloc[split:]\n",
        "        \n",
        "        exclude = ['store_id', 'dept_id', 'event_timestamp', 'date', 'weekly_sales']\n",
        "        feat_cols = [c for c in df.columns if c not in exclude and df[c].dtype in [np.float64, np.int64, np.float32, np.int32]]\n",
        "        print(f\"Features ({len(feat_cols)}): {feat_cols[:5]}...\")\n",
        "        \n",
        "        X_train, y_train = train_df[feat_cols].fillna(0).values, train_df['weekly_sales'].values\n",
        "        X_val, y_val = val_df[feat_cols].fillna(0).values, val_df['weekly_sales'].values\n",
        "        \n",
        "        scaler, y_scaler = StandardScaler(), StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_val = scaler.transform(X_val)\n",
        "        y_train_s = y_scaler.fit_transform(np.log1p(y_train).reshape(-1,1)).flatten()\n",
        "        y_val_s = y_scaler.transform(np.log1p(y_val).reshape(-1,1)).flatten()\n",
        "        \n",
        "        joblib.dump({'scaler_X': scaler, 'scaler_y': y_scaler, 'use_log_transform': True}, f'{OUT}/scalers.joblib')\n",
        "        joblib.dump(feat_cols, f'{OUT}/feature_cols.pkl')\n",
        "        np.savez(f'{OUT}/.data.npz', X_train=X_train, y_train=y_train_s, X_val=X_val, y_val=y_val_s, y_val_orig=y_val)\n",
        "        np.save(f'{OUT}/.dim.npy', [X_train.shape[1]])\n",
        "        mlflow.log_params({'epochs': EPOCHS, 'train_rows': len(train_df), 'val_rows': len(val_df), 'features': len(feat_cols)})\n",
        "    \n",
        "    dist.barrier()\n",
        "    data = np.load(f'{OUT}/.data.npz')\n",
        "    X_train, y_train, X_val, y_val, y_val_orig = data['X_train'], data['y_train'], data['X_val'], data['y_val'], data['y_val_orig']\n",
        "    inp_dim = int(np.load(f'{OUT}/.dim.npy')[0])\n",
        "    dist.barrier()\n",
        "    \n",
        "    train_ds = DS(X_train, y_train)\n",
        "    val_ds = DS(X_val, y_val)\n",
        "    sampler = DistributedSampler(train_ds, num_replicas=world, rank=rank)\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, sampler=sampler)\n",
        "    val_loader = DataLoader(val_ds, batch_size=64)\n",
        "    \n",
        "    model = DDP(MLP(inp_dim).to(device), device_ids=[int(os.environ.get('LOCAL_RANK',0))] if torch.cuda.is_available() else None)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-3)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=3)\n",
        "    crit = nn.MSELoss()\n",
        "    \n",
        "    best_loss, best_mape = float('inf'), float('inf')\n",
        "    for ep in range(EPOCHS):\n",
        "        sampler.set_epoch(ep)\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for X_b, y_b in train_loader:\n",
        "            X_b, y_b = X_b.to(device), y_b.to(device)\n",
        "            opt.zero_grad()\n",
        "            loss = crit(model(X_b), y_b)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(train_loader)\n",
        "        \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            preds = np.concatenate([model(X.to(device)).cpu().numpy() for X, _ in val_loader])\n",
        "        val_loss = np.mean((preds - y_val)**2)\n",
        "        sched.step(val_loss)\n",
        "        \n",
        "        if rank == 0:\n",
        "            y_sc = joblib.load(f'{OUT}/scalers.joblib')['scaler_y']\n",
        "            pred_orig = np.expm1(y_sc.inverse_transform(preds.reshape(-1,1)).flatten())\n",
        "            mask = y_val_orig > 1000\n",
        "            mape = np.mean(np.abs((y_val_orig[mask] - pred_orig[mask]) / y_val_orig[mask])) * 100\n",
        "            mlflow.log_metrics({'train_loss': train_loss, 'val_loss': val_loss, 'mape': mape, 'lr': opt.param_groups[0]['lr']}, step=ep)\n",
        "            if (ep + 1) % 10 == 0:\n",
        "                print(f\"Ep {ep+1}/{EPOCHS} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | MAPE: {mape:.1f}%\")\n",
        "            if val_loss < best_loss:\n",
        "                best_loss, best_mape = val_loss, mape\n",
        "                torch.save(model.module.state_dict(), f'{OUT}/best_model.pt')\n",
        "        dist.barrier()\n",
        "    \n",
        "    if rank == 0:\n",
        "        print(f\"\\n\u2705 DONE: MAPE {best_mape:.1f}%\")\n",
        "        mlflow.log_metrics({'best_mape': best_mape, 'best_val_loss': best_loss})\n",
        "        m = MLP(inp_dim)\n",
        "        m.load_state_dict(torch.load(f'{OUT}/best_model.pt'))\n",
        "        m.eval()\n",
        "        mlflow.pytorch.log_model(m, 'model')\n",
        "        feat_cols = joblib.load(f'{OUT}/feature_cols.pkl')\n",
        "        json.dump({'model_type': 'SalesMLP', 'input_dim': inp_dim, 'hidden_dims': [512,256,128,64], 'dropout': 0.3, 'best_mape': float(best_mape), 'feature_columns': feat_cols}, open(f'{OUT}/model_metadata.json', 'w'))\n",
        "        mlflow.end_run()\n",
        "        for f in ['.data.npz', '.dim.npy']:\n",
        "            try: os.remove(f'{OUT}/{f}')\n",
        "            except: pass\n",
        "    dist.destroy_process_group()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submit Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_id = datetime.now().strftime('%m%d-%H%M')\n",
        "params = {'epochs': EPOCHS, 'use_ray': USE_RAY, 'ray_cluster': RAY_CLUSTER, 'namespace': NAMESPACE, 'output_dir': '/shared/models', 'feature_repo': '/shared/feature_repo'}\n",
        "\n",
        "job = trainer.train(\n",
        "    trainer=CustomTrainer(\n",
        "        func=train_fn,\n",
        "        num_nodes=1,\n",
        "        resources_per_node={'cpu': 4, 'memory': '8Gi', 'nvidia.com/gpu': 1},\n",
        "        packages_to_install=['feast[ray,postgres]==0.59.0', 'codeflare-sdk', 'psycopg2-binary', 'scikit-learn', 'pandas', 'pyarrow', 'joblib', 'mlflow>=3.0'],\n",
        "        env={'MLFLOW_TRACKING_URI': MLFLOW_URI}\n",
        "    ),\n",
        "    runtime=runtime,\n",
        "    parameters=params,\n",
        "    options=[\n",
        "        Labels({'app': 'sales-forecasting'}),\n",
        "        PodTemplateOverrides(PodTemplateOverride(\n",
        "            target_jobs=['node'],\n",
        "            spec=PodSpecOverride(\n",
        "                volumes=[{'name': 'shared', 'persistentVolumeClaim': {'claimName': PVC}}],\n",
        "                containers=[ContainerOverride(name='node', volume_mounts=[{'name': 'shared', 'mountPath': '/shared'}])]\n",
        "            )\n",
        "        ))\n",
        "    ]\n",
        ")\n",
        "print(f\"\u2705 Submitted: {job}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.wait_for_job_status(name=job, status={'Running'}, timeout=300)\n",
        "_ = trainer.get_job_logs(name=job, follow=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.wait_for_job_status(name=job, status={'Complete', 'Failed'}, timeout=3600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MLflow Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "mlflow.set_tracking_uri(MLFLOW_URI)\n",
        "exp = mlflow.get_experiment_by_name('sales-forecasting')\n",
        "if exp:\n",
        "    runs = mlflow.search_runs([exp.experiment_id], max_results=5, order_by=['start_time DESC'])\n",
        "    display(runs[['tags.mlflow.runName', 'metrics.best_mape']])\n",
        "else:\n",
        "    print(\"Experiment not found yet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Next:** `03-inference.ipynb` (deploy KServe first: `kubectl apply -f ../manifests/08-kserve-inference.yaml`)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}