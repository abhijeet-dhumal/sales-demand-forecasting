# Training Job with Feast + Ray Integration + MLflow
# 
# This TrainJob demonstrates FULL Feast integration:
# 1. Calls get_historical_features() directly (not pre-computed)
# 2. Uses KubeRay cluster via CodeFlare SDK for distributed PIT joins
# 3. Trains PyTorch model with DDP
# 4. Logs to MLflow with model registry
#
# Architecture:
# - Feast Feature Service: "training_features" (ensures consistency)
# - Ray Compute Engine: KubeRay cluster (distributed processing)
# - MLflow: Experiment tracking + Model Registry
#
apiVersion: trainer.kubeflow.org/v1alpha1
kind: TrainJob
metadata:
  name: sales-forecasting-mlflow
  namespace: feast-trainer-demo
  labels:
    app.kubernetes.io/name: sales-forecasting-mlflow
spec:
  suspend: false
  runtimeRef:
    name: torch-with-storage  # Custom runtime with PVC already mounted at /shared
  trainer:
    numNodes: 2  # Distributed training across 2 nodes
    resourcesPerNode:
      requests:
        cpu: "4"
        memory: "8Gi"
        nvidia.com/gpu: "1"  # 1 GPU per node
      limits:
        cpu: "8"
        memory: "16Gi"
        nvidia.com/gpu: "1"
    env:
    # Data paths
    - name: DATA_PATH
      value: /shared/data
    - name: OUTPUT_DIR
      value: /shared/models
    - name: FEATURE_REPO
      value: /shared/feature_repo
    - name: NUM_EPOCHS
      value: "50"  # More epochs for better convergence
    # MLflow configuration
    - name: MLFLOW_TRACKING_URI
      value: "http://mlflow.feast-trainer-demo.svc.cluster.local:5000"
    - name: MLFLOW_EXPERIMENT_NAME
      value: "sales-forecasting"
    - name: MLFLOW_ARTIFACT_ROOT
      value: "/shared/mlflow-artifacts"
    # Model Registry configuration
    - name: MODEL_REGISTRY_URI
      value: "sales-model-registry.feast-trainer-demo.svc.cluster.local:8080"
    # KubeRay/CodeFlare SDK configuration (same as dataprep job)
    - name: FEAST_RAY_USE_KUBERAY
      value: "true"
    - name: FEAST_RAY_CLUSTER_NAME
      value: "feast-ray"
    - name: FEAST_RAY_NAMESPACE
      value: "feast-trainer-demo"
    - name: FEAST_RAY_SKIP_TLS
      value: "true"
    command:
    - bash
    - -c
    - |
      set -e
      echo "============================================================"
      echo "ðŸš€ TRAINING WITH FEAST + RAY INTEGRATION"
      echo "============================================================"
      
      # Install dependencies (Feast + Ray + CodeFlare + MLflow)
      # Use mlflow 3.x to match server (supports logged-models API)
      echo "ðŸ“¦ Installing dependencies..."
      pip install --quiet --target=/tmp/pylibs \
        "feast[postgres,ray]==0.59.0" codeflare-sdk psycopg2-binary \
        scikit-learn joblib pyarrow "mlflow>=3.0"
      
      export PYTHONPATH=/tmp/pylibs:$PYTHONPATH
      export PATH=/tmp/pylibs/bin:$PATH
      
      # Setup in-cluster auth for CodeFlare SDK
      export FEAST_RAY_AUTH_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
      export FEAST_RAY_AUTH_SERVER="https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}"
      echo "ðŸ” CodeFlare SDK auth configured"

      cat > /tmp/train_mlflow.py << 'TRAIN_SCRIPT'
      import os
      import sys
      import json
      import logging
      import numpy as np
      import pandas as pd
      import torch
      import torch.nn as nn
      import torch.distributed as dist
      from torch.nn.parallel import DistributedDataParallel as DDP
      from torch.utils.data import DataLoader, Dataset, DistributedSampler
      from sklearn.preprocessing import StandardScaler
      import joblib
      import mlflow
      from datetime import datetime, timezone, timedelta
      import time

      logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")
      logger = logging.getLogger(__name__)

      # Configuration
      DATA_PATH = os.environ.get("DATA_PATH", "/shared/data")
      OUTPUT_DIR = os.environ.get("OUTPUT_DIR", "/shared/models")
      FEATURE_REPO = os.environ.get("FEATURE_REPO", "/shared/feature_repo")
      NUM_EPOCHS = int(os.environ.get("NUM_EPOCHS", 15))
      MLFLOW_TRACKING_URI = os.environ.get("MLFLOW_TRACKING_URI", "http://mlflow:5000")
      MLFLOW_EXPERIMENT_NAME = os.environ.get("MLFLOW_EXPERIMENT_NAME", "sales-forecasting")

      # =================================================================
      # MODEL DEFINITION
      # =================================================================
      class SalesMLP(nn.Module):
          def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout=0.2):
              super().__init__()
              layers = []
              prev_dim = input_dim
              for dim in hidden_dims:
                  layers.extend([
                      nn.Linear(prev_dim, dim),
                      nn.BatchNorm1d(dim),
                      nn.ReLU(),
                      nn.Dropout(dropout)
                  ])
                  prev_dim = dim
              layers.append(nn.Linear(prev_dim, 1))
              self.net = nn.Sequential(*layers)
              self.hidden_dims = hidden_dims
              self.dropout = dropout
              
          def forward(self, x):
              return self.net(x).squeeze(-1)

      class SalesDataset(Dataset):
          def __init__(self, X, y):
              self.X = torch.tensor(X, dtype=torch.float32)
              self.y = torch.tensor(y, dtype=torch.float32)
          def __len__(self): return len(self.X)
          def __getitem__(self, i): return self.X[i], self.y[i]

      # =================================================================
      # SETUP
      # =================================================================
      backend = 'nccl' if torch.cuda.is_available() else 'gloo'
      dist.init_process_group(backend=backend)
      rank, world_size = dist.get_rank(), dist.get_world_size()
      local_rank = int(os.environ.get("LOCAL_RANK", 0))
      device = torch.device(f"cuda:{local_rank}" if torch.cuda.is_available() else "cpu")
      logger.info(f"DDP: rank={rank}/{world_size}, device={device}")

      # =================================================================
      # MLFLOW SETUP (Rank 0 only)
      # =================================================================
      if rank == 0:
          mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
          mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)
          
          # Start MLflow run
          run = mlflow.start_run(run_name=f"train-{datetime.now().strftime('%Y%m%d-%H%M%S')}")
          logger.info(f"MLflow tracking URI: {MLFLOW_TRACKING_URI}")
          logger.info(f"MLflow run ID: {run.info.run_id}")
          
          # Log initial parameters
          mlflow.log_params({
              "num_epochs": NUM_EPOCHS,
              "world_size": world_size,
              "device": str(device),
          })

      # =================================================================
      # FEAST + RAY: GET HISTORICAL FEATURES (Production Architecture)
      # =================================================================
      # WHY USE THIS OVER DIRECT PARQUET?
      # 
      # | Data Size | Direct Parquet | Feast + Ray | Winner      |
      # |-----------|----------------|-------------|-------------|
      # | 100K rows | 1 sec          | ~3 min      | Parquet     |
      # | 10M rows  | 30 min (OOM)   | ~10 min     | Feast+Ray   |
      # | 100M rows | Impossible     | ~30 min     | Feast+Ray   |
      # | 1B rows   | Impossible     | ~2-4 hours  | Feast+Ray   |
      #
      # PRODUCTION BENEFITS:
      # 1. Train-Serve Consistency: Same FeatureService in training & inference
      # 2. Point-in-Time Correctness: Automatic temporal joins, no data leakage
      # 3. Feature Versioning: Feast tracks all feature definitions
      # 4. Scalability: Ray distributes PIT joins across cluster
      # 5. Reproducibility: Can recreate exact training dataset anytime
      #
      if rank == 0:
          os.makedirs(OUTPUT_DIR, exist_ok=True)
          
          logger.info("=" * 60)
          logger.info("FEAST + RAY: GET_HISTORICAL_FEATURES")
          logger.info("=" * 60)
          logger.info("Production architecture for train-serve consistency")
          logger.info("Ray distributes PIT joins across KubeRay cluster")
          logger.info("")
          
          from feast import FeatureStore
          import shutil
          
          # Use Ray-enabled config (connects to KubeRay via CodeFlare SDK)
          ray_config_path = f"{FEATURE_REPO}/feature_store_ray.yaml"
          logger.info(f"Using Ray config: {ray_config_path}")
          shutil.copy(ray_config_path, f"{FEATURE_REPO}/feature_store.yaml")
          
          store = FeatureStore(repo_path=FEATURE_REPO)
          
          # Create entity DataFrame (what we want features for)
          logger.info("Creating entity DataFrame...")
          entity_rows = []
          base_date = datetime(2022, 1, 1, tzinfo=timezone.utc)
          
          # Scale parameters (matches DataPrep)
          NUM_WEEKS = int(os.getenv("NUM_WEEKS", "104"))
          NUM_STORES = int(os.getenv("NUM_STORES", "45"))
          NUM_DEPTS = int(os.getenv("NUM_DEPTS", "14"))
          
          for week in range(NUM_WEEKS):
              event_ts = base_date + timedelta(weeks=week)
              for store_id in range(1, NUM_STORES + 1):
                  for dept_id in range(1, NUM_DEPTS + 1):
                      entity_rows.append({
                          "store_id": store_id,
                          "dept_id": dept_id,
                          "event_timestamp": event_ts
                      })
          
          entity_df = pd.DataFrame(entity_rows)
          logger.info(f"Entity DataFrame: {len(entity_df):,} rows")
          logger.info(f"   ({NUM_WEEKS} weeks Ã— {NUM_STORES} stores Ã— {NUM_DEPTS} depts)")
          
          # Get historical features via Ray (distributed PIT joins)
          logger.info("")
          logger.info("Calling get_historical_features() via KubeRay...")
          logger.info("   Feature Service: training_features")
          logger.info("   This ensures SAME features used in training & inference!")
          logger.info("")
          
          start_time = time.time()
          training_data = store.get_historical_features(
              entity_df=entity_df,
              features=store.get_feature_service("training_features"),
          )
          training_df = training_data.to_df()
          elapsed = time.time() - start_time
          
          throughput = len(entity_df) / elapsed if elapsed > 0 else 0
          logger.info(f"âœ… Features retrieved in {elapsed:.1f}s")
          logger.info(f"   Throughput: {throughput:,.0f} rows/sec")
          logger.info(f"   Rows: {len(training_df):,}, Columns: {len(training_df.columns)}")
          
          # =================================================================
          # SAVE VERSIONED TRAINING DATASET (Reproducibility)
          # =================================================================
          logger.info("")
          logger.info("Saving versioned training dataset...")
          dataset_version = datetime.now().strftime("%Y%m%d_%H%M%S")
          dataset_path = f"{DATA_PATH}/saved_datasets"
          os.makedirs(dataset_path, exist_ok=True)
          
          dataset_file = f"{dataset_path}/training_v{dataset_version}.parquet"
          training_df.to_parquet(dataset_file, index=False)
          training_df.to_parquet(f"{dataset_path}/training_latest.parquet", index=False)
          
          # Metadata for lineage tracking
          dataset_metadata = {
              "version": dataset_version,
              "source": "feast_get_historical_features",
              "feature_service": "training_features",
              "entity_count": len(entity_df),
              "row_count": len(training_df),
              "columns": list(training_df.columns),
              "retrieval_time_sec": round(elapsed, 2),
              "throughput_rows_sec": round(throughput),
              "created_at": datetime.now(timezone.utc).isoformat(),
              "mlflow_run_id": mlflow.active_run().info.run_id,
          }
          with open(f"{dataset_path}/training_latest_metadata.json", "w") as f:
              json.dump(dataset_metadata, f, indent=2)
          
          logger.info(f"   Saved: {dataset_file}")
          
          # Log to MLflow
          mlflow.log_artifact(f"{dataset_path}/training_latest_metadata.json", artifact_path="datasets")
          mlflow.log_params({
              "data_source": "feast_ray_get_historical_features",
              "feature_service": "training_features",
              "feast_retrieval_time_sec": round(elapsed, 2),
              "feast_throughput_rows_sec": round(throughput),
              "dataset_version": dataset_version,
          })
          
          # Drop rows with missing target
          training_df = training_df.dropna(subset=["weekly_sales"])
          
          # Handle timestamp column naming (DataPrep saves as 'date')
          if "date" in training_df.columns and "event_timestamp" not in training_df.columns:
              training_df = training_df.rename(columns={"date": "event_timestamp"})
          
          # Temporal split (80/20 based on time)
          training_df = training_df.sort_values("event_timestamp")
          split_idx = int(len(training_df) * 0.8)
          train_df = training_df.iloc[:split_idx]
          val_df = training_df.iloc[split_idx:]
          
          logger.info(f"Train: {len(train_df):,}, Val: {len(val_df):,}")
          
          # Log dataset info to MLflow
          mlflow.log_params({
              "total_rows": len(training_df),
              "train_rows": len(train_df),
              "val_rows": len(val_df),
          })
          
          # Feature columns (exclude identifiers and target)
          exclude_cols = ["store_id", "dept_id", "date", "event_timestamp", "weekly_sales"]
          feature_cols = [c for c in training_df.columns 
                          if c not in exclude_cols
                          and training_df[c].dtype in [np.float64, np.int64, np.float32, np.int32]]
          
          logger.info(f"Feature columns ({len(feature_cols)}): {feature_cols}")
          
          X_train = train_df[feature_cols].fillna(0).values
          y_train = train_df["weekly_sales"].values
          X_val = val_df[feature_cols].fillna(0).values
          y_val = val_df["weekly_sales"].values
          
          # Scale features
          scaler = StandardScaler()
          X_train = scaler.fit_transform(X_train)
          X_val = scaler.transform(X_val)
          
          # LOG TRANSFORM + Scale target (handles heteroscedasticity in sales data)
          # This ensures model optimizes for % error not absolute error
          USE_LOG_TRANSFORM = True
          y_scaler = StandardScaler()
          
          if USE_LOG_TRANSFORM:
              # log1p handles zeros: log(1 + y)
              y_train_log = np.log1p(y_train)
              y_val_log = np.log1p(y_val)
              y_train_scaled = y_scaler.fit_transform(y_train_log.reshape(-1, 1)).flatten()
              y_val_scaled = y_scaler.transform(y_val_log.reshape(-1, 1)).flatten()
              logger.info("âœ… Using LOG TRANSFORM for target (better for % error optimization)")
          else:
              y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()
              y_val_scaled = y_scaler.transform(y_val.reshape(-1, 1)).flatten()
              logger.info("Using direct target scaling (no log transform)")
          
          # Save scalers (include log transform flag)
          joblib.dump(scaler, f"{OUTPUT_DIR}/scaler.pkl")
          joblib.dump(y_scaler, f"{OUTPUT_DIR}/y_scaler.pkl")
          joblib.dump(feature_cols, f"{OUTPUT_DIR}/feature_cols.pkl")
          # Combined format for serve.py with log transform flag
          joblib.dump({
              "scaler_X": scaler, 
              "scaler_y": y_scaler,
              "use_log_transform": USE_LOG_TRANSFORM
          }, f"{OUTPUT_DIR}/scalers.joblib")
          
          logger.info("Data loading and preprocessing complete")
          
          # =================================================================
          # ENHANCED LINEAGE: Feature Statistics & Data Quality
          # =================================================================
          logger.info("Logging feature statistics for lineage...")
          
          # Feature statistics (before scaling)
          feature_stats = {}
          raw_train = train_df[feature_cols].fillna(0)
          for col in feature_cols:
              feature_stats[col] = {
                  "mean": float(raw_train[col].mean()),
                  "std": float(raw_train[col].std()),
                  "min": float(raw_train[col].min()),
                  "max": float(raw_train[col].max()),
                  "null_count": int(train_df[col].isnull().sum()),
                  "unique_count": int(train_df[col].nunique()),
              }
          
          # Feature stats saved to artifact only (not metrics - too cluttered)
          # Individual feature metrics removed to keep MLflow UI clean
          
          # Target distribution
          target_stats = {
              "mean": float(train_df["weekly_sales"].mean()),
              "std": float(train_df["weekly_sales"].std()),
              "min": float(train_df["weekly_sales"].min()),
              "max": float(train_df["weekly_sales"].max()),
              "median": float(train_df["weekly_sales"].median()),
              "p95": float(train_df["weekly_sales"].quantile(0.95)),
          }
          mlflow.log_params({
              "target_mean": round(target_stats["mean"], 2),
              "target_std": round(target_stats["std"], 2),
              "target_min": round(target_stats["min"], 2),
              "target_max": round(target_stats["max"], 2),
          })
          
          # Data quality metrics
          data_quality = {
              "total_nulls": int(training_df.isnull().sum().sum()),
              "null_percentage": round(training_df.isnull().sum().sum() / (len(training_df) * len(training_df.columns)) * 100, 2),
              "feature_count": len(feature_cols),
              "entity_columns": ["store_id", "dept_id"],
              "temporal_range_days": (training_df["event_timestamp"].max() - training_df["event_timestamp"].min()).days,
          }
          mlflow.log_params({
              "data_quality_nulls": data_quality["total_nulls"],
              "data_quality_null_pct": data_quality["null_percentage"],
              "temporal_range_days": data_quality["temporal_range_days"],
          })
          
          # Save comprehensive lineage artifact
          lineage_artifact = {
              "feature_statistics": feature_stats,
              "target_statistics": target_stats,
              "data_quality": data_quality,
              "feature_columns": feature_cols,
              "feast_config": {
                  "feature_service": "training_features",
                  "offline_store": "ray",
                  "online_store": "postgres",
                  "registry": "postgres",
              },
              "ray_cluster": {
                  "cluster_name": os.environ.get("FEAST_RAY_CLUSTER_NAME", "feast-ray"),
                  "namespace": os.environ.get("FEAST_RAY_NAMESPACE", "feast-trainer-demo"),
              },
              "environment": {
                  "python_version": sys.version,
                  "torch_version": torch.__version__,
                  "feast_version": "0.59.0",
                  "mlflow_version": mlflow.__version__,
              },
              "training_config": {
                  "train_split": 0.8,
                  "val_split": 0.2,
                  "scaling": "StandardScaler",
              },
          }
          
          lineage_path = f"{OUTPUT_DIR}/lineage_metadata.json"
          with open(lineage_path, "w") as f:
              json.dump(lineage_artifact, f, indent=2, default=str)
          mlflow.log_artifact(lineage_path, artifact_path="lineage")
          
          logger.info(f"   Logged {len(feature_cols)} feature statistics")
          logger.info(f"   Data quality: {data_quality['null_percentage']}% nulls")
          
          # Save for other ranks
          np.save(f"{OUTPUT_DIR}/.X_train.npy", X_train)
          np.save(f"{OUTPUT_DIR}/.y_train.npy", y_train_scaled)
          np.save(f"{OUTPUT_DIR}/.X_val.npy", X_val)
          np.save(f"{OUTPUT_DIR}/.y_val.npy", y_val_scaled)
          np.save(f"{OUTPUT_DIR}/.y_val_orig.npy", y_val)
          
          input_dim = X_train.shape[1]
          np.save(f"{OUTPUT_DIR}/.input_dim.npy", np.array([input_dim]))
          
          logger.info("Features preprocessed and saved")

      dist.barrier()

      # Load data on all ranks
      X_train = np.load(f"{OUTPUT_DIR}/.X_train.npy")
      y_train = np.load(f"{OUTPUT_DIR}/.y_train.npy")
      X_val = np.load(f"{OUTPUT_DIR}/.X_val.npy")
      y_val = np.load(f"{OUTPUT_DIR}/.y_val.npy")
      y_val_orig = np.load(f"{OUTPUT_DIR}/.y_val_orig.npy")
      input_dim = int(np.load(f"{OUTPUT_DIR}/.input_dim.npy")[0])

      dist.barrier()

      # =================================================================
      # TRAINING
      # =================================================================
      logger.info("=" * 60)
      logger.info("TRAINING MODEL")
      logger.info("=" * 60)

      # Hyperparameters (tuned for better performance)
      BATCH_SIZE = 64  # Smaller batches = more gradient updates
      LEARNING_RATE = 5e-4  # Lower LR for stability
      WEIGHT_DECAY = 1e-3  # Stronger regularization
      HIDDEN_DIMS = [512, 256, 128, 64]  # Deeper network
      DROPOUT = 0.3  # More dropout to prevent overfitting

      if rank == 0:
          mlflow.log_params({
              "batch_size": BATCH_SIZE,
              "learning_rate": LEARNING_RATE,
              "weight_decay": WEIGHT_DECAY,
              "hidden_dims": str(HIDDEN_DIMS),
              "dropout": DROPOUT,
              "input_dim": input_dim,
          })

      train_ds = SalesDataset(X_train, y_train)
      val_ds = SalesDataset(X_val, y_val)
      sampler = DistributedSampler(train_ds, num_replicas=world_size, rank=rank)
      train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2)
      val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

      model = SalesMLP(input_dim, HIDDEN_DIMS, DROPOUT).to(device)
      model = DDP(model, device_ids=[local_rank] if torch.cuda.is_available() else None)

      optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
      scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode="min", factor=0.5, patience=3)
      criterion = nn.MSELoss()  # MSE works well with log-transformed target
      logger.info("Using MSE loss with log-transformed target (equivalent to optimizing RMSLE)")

      best_val_loss = float("inf")
      best_mape = float("inf")

      for epoch in range(NUM_EPOCHS):
          sampler.set_epoch(epoch)
          model.train()
          train_loss = 0.0
          
          for X, y in train_loader:
              X, y = X.to(device), y.to(device)
              optimizer.zero_grad()
              output = model(X)
              loss = criterion(output, y)
              loss.backward()
              torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
              optimizer.step()
              train_loss += loss.item()
          
          train_loss /= len(train_loader)
          
          # Validation
          model.eval()
          val_loss = 0.0
          predictions, actuals = [], []
          
          with torch.no_grad():
              for X, y in val_loader:
                  output = model(X.to(device))
                  val_loss += criterion(output, y.to(device)).item()
                  predictions.extend(output.cpu().numpy())
                  actuals.extend(y.numpy())
          
          val_loss /= len(val_loader)
          scheduler.step(val_loss)
          
          if rank == 0:
              predictions = np.array(predictions)
              actuals = np.array(actuals)
              
              # Unscale predictions for MAPE (handle log transform)
              y_scaler = joblib.load(f"{OUTPUT_DIR}/y_scaler.pkl")
              preds_unscaled = y_scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()
              
              # Reverse log transform if used
              if USE_LOG_TRANSFORM:
                  preds_orig = np.expm1(preds_unscaled)  # expm1 = exp(x) - 1, inverse of log1p
              else:
                  preds_orig = preds_unscaled
              
              # Calculate metrics (filter outliers for stable MAPE)
              mask = np.abs(y_val_orig) > 1000
              mape = np.mean(np.abs((y_val_orig[mask] - preds_orig[mask]) / y_val_orig[mask])) * 100 if mask.sum() > 0 else np.nan
              rmse = np.sqrt(np.mean((y_val_orig - preds_orig) ** 2))
              mae = np.mean(np.abs(y_val_orig - preds_orig))
              
              # Log metrics to MLflow (essential + useful)
              mlflow.log_metrics({
                  "train_loss": train_loss,
                  "val_loss": val_loss,
                  "mape": mape,  # % error (business metric)
                  "rmse": rmse,  # penalizes large errors
                  "mae": mae,    # avg error in $ (interpretable)
                  "learning_rate": optimizer.param_groups[0]['lr'],  # tracks LR schedule
              }, step=epoch)
              
              logger.info(f"Epoch {epoch+1}/{NUM_EPOCHS} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | MAPE: {mape:.1f}% | RMSE: {rmse:.0f}")
              
              if val_loss < best_val_loss:
                  best_val_loss = val_loss
                  best_mape = mape
                  torch.save(model.module.state_dict(), f"{OUTPUT_DIR}/best_model.pt")
                  logger.info("  â†’ Saved best model")
          
          dist.barrier()

      # =================================================================
      # PREDICTION SAMPLES (for validation visualization)
      # =================================================================
      if rank == 0:
          logger.info("Generating prediction samples for lineage...")
          best_model_eval = SalesMLP(input_dim, HIDDEN_DIMS, DROPOUT)
          best_model_eval.load_state_dict(torch.load(f"{OUTPUT_DIR}/best_model.pt"))
          best_model_eval.eval()
          
          with torch.no_grad():
              val_preds = best_model_eval(torch.FloatTensor(X_val)).numpy()
          
          y_scaler_final = joblib.load(f"{OUTPUT_DIR}/y_scaler.pkl")
          preds_unscaled = y_scaler_final.inverse_transform(val_preds.reshape(-1, 1)).flatten()
          
          # Reverse log transform if used
          if USE_LOG_TRANSFORM:
              preds_final = np.expm1(preds_unscaled)
          else:
              preds_final = preds_unscaled
          
          # Sample 20 predictions for visualization
          sample_idx = np.random.choice(len(y_val_orig), min(20, len(y_val_orig)), replace=False)
          prediction_samples = {
              "samples": [
                  {
                      "idx": int(i),
                      "actual": round(float(y_val_orig[i]), 2),
                      "predicted": round(float(preds_final[i]), 2),
                      "error": round(float(preds_final[i] - y_val_orig[i]), 2),
                      "error_pct": round(abs(preds_final[i] - y_val_orig[i]) / max(y_val_orig[i], 1) * 100, 1),
                  }
                  for i in sample_idx
              ],
              "summary": {
                  "total_predictions": len(y_val_orig),
                  "mean_actual": round(float(np.mean(y_val_orig)), 2),
                  "mean_predicted": round(float(np.mean(preds_final)), 2),
                  "correlation": round(float(np.corrcoef(y_val_orig, preds_final)[0, 1]), 4),
              }
          }
          
          pred_samples_path = f"{OUTPUT_DIR}/prediction_samples.json"
          with open(pred_samples_path, "w") as f:
              json.dump(prediction_samples, f, indent=2)
          mlflow.log_artifact(pred_samples_path, artifact_path="validation")
          
          logger.info(f"   Correlation: {prediction_samples['summary']['correlation']:.4f}")
      
      dist.barrier()

      # =================================================================
      # FINAL LOGGING TO MLFLOW + MODEL REGISTRY
      # =================================================================
      if rank == 0:
          logger.info("=" * 60)
          logger.info("LOGGING MODEL & ARTIFACTS TO MLFLOW")
          logger.info("=" * 60)
          
          # Log final metrics
          mlflow.log_metrics({
              "best_val_loss": best_val_loss,
              "best_mape": best_mape,
          })
          
          # Load best model for logging
          best_model = SalesMLP(input_dim, HIDDEN_DIMS, DROPOUT)
          best_model.load_state_dict(torch.load(f"{OUTPUT_DIR}/best_model.pt"))
          best_model.eval()
          
          # Log PyTorch model (without auto-registration to avoid new API)
          logger.info("Logging PyTorch model to MLflow...")
          model_info = mlflow.pytorch.log_model(
              best_model,
              artifact_path="model",
              pip_requirements=["torch>=2.0", "numpy", "scikit-learn"],
          )
          
          # Register model using older API
          logger.info("Registering model in MLflow Model Registry...")
          try:
              from mlflow.tracking import MlflowClient
              client = MlflowClient()
              model_uri = f"runs:/{mlflow.active_run().info.run_id}/model"
              result = mlflow.register_model(model_uri, "sales-forecasting-mlp")
              logger.info(f"Model registered: {result.name} v{result.version}")
          except Exception as e:
              logger.warning(f"Model registration skipped: {e}")
          
          # Log artifacts (scalers, feature list)
          logger.info("Logging artifacts...")
          mlflow.log_artifact(f"{OUTPUT_DIR}/scaler.pkl", artifact_path="preprocessing")
          mlflow.log_artifact(f"{OUTPUT_DIR}/y_scaler.pkl", artifact_path="preprocessing")
          mlflow.log_artifact(f"{OUTPUT_DIR}/feature_cols.pkl", artifact_path="preprocessing")
          
          # Log model metadata as JSON
          import json
          model_metadata = {
              "model_type": "SalesMLP",
              "input_dim": input_dim,
              "hidden_dims": HIDDEN_DIMS,
              "dropout": DROPOUT,
              "feature_columns": joblib.load(f"{OUTPUT_DIR}/feature_cols.pkl"),
              "best_mape": float(best_mape),
              "best_val_loss": float(best_val_loss),
              "training_epochs": NUM_EPOCHS,
          }
          with open(f"{OUTPUT_DIR}/model_metadata.json", "w") as f:
              json.dump(model_metadata, f, indent=2)
          mlflow.log_artifact(f"{OUTPUT_DIR}/model_metadata.json")
          
          # Get run info for Model Registry
          run_id = mlflow.active_run().info.run_id
          artifact_uri = mlflow.get_artifact_uri("model")
          
          logger.info(f"MLflow run ID: {run_id}")
          logger.info(f"Model artifact URI: {artifact_uri}")
          
          # End MLflow run
          mlflow.end_run()
          
          # =================================================================
          # REGISTER MODEL IN OPENSHIFT AI MODEL REGISTRY
          # =================================================================
          MODEL_REGISTRY_URI = os.environ.get("MODEL_REGISTRY_URI", "")
          
          if MODEL_REGISTRY_URI:
              logger.info("=" * 60)
              logger.info("REGISTERING MODEL IN OPENSHIFT AI MODEL REGISTRY")
              logger.info("=" * 60)
              
              try:
                  from model_registry import ModelRegistry
                  from model_registry.types import ModelArtifact, ModelVersion, RegisteredModel
                  
                  # Connect to Model Registry
                  registry = ModelRegistry(
                      server_address=MODEL_REGISTRY_URI,
                      port=8080,
                      author="kubeflow-training",
                      is_secure=False,
                  )
                  
                  # Register model
                  registered_model = registry.register_model(
                      name="sales-forecasting",
                      uri=artifact_uri,
                      version=f"v{datetime.now().strftime('%Y%m%d%H%M%S')}",
                      description=f"Sales demand forecasting MLP - MAPE: {best_mape:.1f}%",
                      model_format_name="pytorch",
                      model_format_version="2.0",
                      storage_key="mlflow",
                      metadata={
                          "mape": str(best_mape),
                          "val_loss": str(best_val_loss),
                          "mlflow_run_id": run_id,
                          "framework": "pytorch",
                      }
                  )
                  
                  logger.info(f"âœ… Model registered in OpenShift AI Model Registry!")
                  logger.info(f"   Model ID: {registered_model.id}")
                  logger.info(f"   Ready for KServe deployment")
                  
              except Exception as e:
                  logger.warning(f"Model Registry registration skipped: {e}")
                  logger.info("Model is still available in MLflow Model Registry")
          
          logger.info("=" * 60)
          logger.info("âœ… TRAINING COMPLETE!")
          logger.info("=" * 60)
          logger.info(f"   Data Source: Feast get_historical_features()")
          logger.info(f"   Feature Service: training_features")
          logger.info(f"   Compute Engine: KubeRay (CodeFlare SDK)")
          logger.info(f"   Best val_loss: {best_val_loss:.4f}")
          logger.info(f"   Best MAPE: {best_mape:.1f}%")
          logger.info(f"   Model saved to: {OUTPUT_DIR}/best_model.pt")
          logger.info(f"   MLflow experiment: {MLFLOW_EXPERIMENT_NAME}")
          logger.info(f"   MLflow run ID: {run_id}")
          logger.info(f"   Model artifact: {artifact_uri}")
          
          # Cleanup temp files
          for f in [".X_train.npy", ".y_train.npy", ".X_val.npy", ".y_val.npy", ".y_val_orig.npy", ".input_dim.npy"]:
              try: os.remove(f"{OUTPUT_DIR}/{f}")
              except: pass

      dist.destroy_process_group()
      TRAIN_SCRIPT

      torchrun --nnodes=$PET_NNODES --nproc_per_node=$PET_NPROC_PER_NODE --node_rank=$PET_NODE_RANK \
        --rdzv_backend=c10d --rdzv_endpoint=$PET_MASTER_ADDR:$PET_MASTER_PORT /tmp/train_mlflow.py

