# Training Job with Full MLflow Integration + Model Registry
# Features:
# - MLflow experiment tracking (metrics, params)
# - MLflow model logging (PyTorch model with signature)
# - MLflow artifact storage (scalers, feature lists)
# - OpenShift AI Model Registry integration
apiVersion: trainer.kubeflow.org/v1alpha1
kind: TrainJob
metadata:
  name: sales-forecasting-mlflow
  namespace: feast-trainer-demo
  labels:
    app.kubernetes.io/name: sales-forecasting-mlflow
spec:
  suspend: false
  runtimeRef:
    name: torch-with-storage
  trainer:
    numNodes: 1
    resourcesPerNode:
      requests:
        cpu: "4"
        memory: "8Gi"
      limits:
        cpu: "8"
        memory: "16Gi"
    env:
    # Data paths (shared with dataprep job)
    - name: DATA_PATH
      value: /shared/data
    - name: OUTPUT_DIR
      value: /shared/models
    - name: NUM_EPOCHS
      value: "15"
    # MLflow configuration
    - name: MLFLOW_TRACKING_URI
      value: "http://mlflow.feast-trainer-demo.svc.cluster.local:5000"
    - name: MLFLOW_EXPERIMENT_NAME
      value: "sales-forecasting"
    - name: MLFLOW_ARTIFACT_ROOT
      value: "/shared/mlflow-artifacts"
    # Model Registry configuration
    - name: MODEL_REGISTRY_URI
      value: "sales-model-registry.feast-trainer-demo.svc.cluster.local:8080"
    # Note: Using pre-computed training data from dataprep job
    # This is the recommended production pattern:
    # - Dataprep creates training datasets (via Feast + KubeRay)
    # - Training consumes prepared data directly (faster, reproducible)
    command:
    - bash
    - -c
    - |
      set -e
      echo "Installing dependencies..."
      # Pin MLflow to match server version to avoid API compatibility issues
      pip install --quiet scikit-learn joblib "mlflow==2.10.0" pyarrow "model-registry==0.3.5"

      cat > /tmp/train_mlflow.py << 'TRAIN_SCRIPT'
      import os
      import logging
      import numpy as np
      import pandas as pd
      import torch
      import torch.nn as nn
      import torch.distributed as dist
      from torch.nn.parallel import DistributedDataParallel as DDP
      from torch.utils.data import DataLoader, Dataset, DistributedSampler
      from sklearn.preprocessing import StandardScaler
      import joblib
      import mlflow
      from datetime import datetime

      logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")
      logger = logging.getLogger(__name__)

      # Configuration
      DATA_PATH = os.environ.get("DATA_PATH", "/shared/data")
      OUTPUT_DIR = os.environ.get("OUTPUT_DIR", "/shared/models")
      NUM_EPOCHS = int(os.environ.get("NUM_EPOCHS", 15))
      MLFLOW_TRACKING_URI = os.environ.get("MLFLOW_TRACKING_URI", "http://mlflow:5000")
      MLFLOW_EXPERIMENT_NAME = os.environ.get("MLFLOW_EXPERIMENT_NAME", "sales-forecasting")

      # =================================================================
      # MODEL DEFINITION
      # =================================================================
      class SalesMLP(nn.Module):
          def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout=0.2):
              super().__init__()
              layers = []
              prev_dim = input_dim
              for dim in hidden_dims:
                  layers.extend([
                      nn.Linear(prev_dim, dim),
                      nn.BatchNorm1d(dim),
                      nn.ReLU(),
                      nn.Dropout(dropout)
                  ])
                  prev_dim = dim
              layers.append(nn.Linear(prev_dim, 1))
              self.net = nn.Sequential(*layers)
              self.hidden_dims = hidden_dims
              self.dropout = dropout
              
          def forward(self, x):
              return self.net(x).squeeze(-1)

      class SalesDataset(Dataset):
          def __init__(self, X, y):
              self.X = torch.tensor(X, dtype=torch.float32)
              self.y = torch.tensor(y, dtype=torch.float32)
          def __len__(self): return len(self.X)
          def __getitem__(self, i): return self.X[i], self.y[i]

      # =================================================================
      # SETUP
      # =================================================================
      backend = 'nccl' if torch.cuda.is_available() else 'gloo'
      dist.init_process_group(backend=backend)
      rank, world_size = dist.get_rank(), dist.get_world_size()
      local_rank = int(os.environ.get("LOCAL_RANK", 0))
      device = torch.device(f"cuda:{local_rank}" if torch.cuda.is_available() else "cpu")
      logger.info(f"DDP: rank={rank}/{world_size}, device={device}")

      # =================================================================
      # MLFLOW SETUP (Rank 0 only)
      # =================================================================
      if rank == 0:
          mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
          mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)
          
          # Start MLflow run
          run = mlflow.start_run(run_name=f"train-{datetime.now().strftime('%Y%m%d-%H%M%S')}")
          logger.info(f"MLflow tracking URI: {MLFLOW_TRACKING_URI}")
          logger.info(f"MLflow run ID: {run.info.run_id}")
          
          # Log initial parameters
          mlflow.log_params({
              "num_epochs": NUM_EPOCHS,
              "world_size": world_size,
              "device": str(device),
          })

      # =================================================================
      # LOAD PRE-COMPUTED TRAINING DATA
      # =================================================================
      # Production pattern: Dataprep job (via Feast + KubeRay) creates training data
      # Training job consumes prepared data directly - faster & reproducible
      if rank == 0:
          os.makedirs(OUTPUT_DIR, exist_ok=True)
          
          logger.info("=" * 60)
          logger.info("LOADING PRE-COMPUTED TRAINING DATA")
          logger.info("=" * 60)
          logger.info("Note: Data was prepared by dataprep job using Feast + KubeRay")
          
          # Load pre-computed features from dataprep job
          logger.info(f"Loading data from {DATA_PATH}...")
          training_df = pd.read_parquet(f"{DATA_PATH}/features.parquet")
          training_df["date"] = pd.to_datetime(training_df["date"])
          
          logger.info(f"Loaded {len(training_df):,} rows, {len(training_df.columns)} columns")
          logger.info(f"Columns: {list(training_df.columns)}")
          
          # Temporal split (80/20 based on time)
          training_df = training_df.sort_values("date")
          split_idx = int(len(training_df) * 0.8)
          train_df = training_df.iloc[:split_idx]
          val_df = training_df.iloc[split_idx:]
          
          logger.info(f"Train: {len(train_df):,}, Val: {len(val_df):,}")
          
          # Log dataset info to MLflow
          mlflow.log_params({
              "total_rows": len(training_df),
              "train_rows": len(train_df),
              "val_rows": len(val_df),
              "data_source": "pre-computed (Feast + KubeRay)",
          })
          
          # Feature columns (exclude identifiers and target)
          feature_cols = [c for c in training_df.columns 
                          if c not in ["store_id", "dept_id", "date", "event_timestamp", "weekly_sales"]
                          and training_df[c].dtype in [np.float64, np.int64, np.float32, np.int32]]
          
          X_train = train_df[feature_cols].fillna(0).values
          y_train = train_df["weekly_sales"].values
          X_val = val_df[feature_cols].fillna(0).values
          y_val = val_df["weekly_sales"].values
          
          # Scale features
          scaler = StandardScaler()
          X_train = scaler.fit_transform(X_train)
          X_val = scaler.transform(X_val)
          
          # Scale target
          y_scaler = StandardScaler()
          y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()
          y_val_scaled = y_scaler.transform(y_val.reshape(-1, 1)).flatten()
          
          # Save scalers
          joblib.dump(scaler, f"{OUTPUT_DIR}/scaler.pkl")
          joblib.dump(y_scaler, f"{OUTPUT_DIR}/y_scaler.pkl")
          joblib.dump(feature_cols, f"{OUTPUT_DIR}/feature_cols.pkl")
          
          logger.info("Data loading and preprocessing complete")
          
          # Save for other ranks
          np.save(f"{OUTPUT_DIR}/.X_train.npy", X_train)
          np.save(f"{OUTPUT_DIR}/.y_train.npy", y_train_scaled)
          np.save(f"{OUTPUT_DIR}/.X_val.npy", X_val)
          np.save(f"{OUTPUT_DIR}/.y_val.npy", y_val_scaled)
          np.save(f"{OUTPUT_DIR}/.y_val_orig.npy", y_val)
          
          input_dim = X_train.shape[1]
          np.save(f"{OUTPUT_DIR}/.input_dim.npy", np.array([input_dim]))
          
          logger.info("Features preprocessed and saved")

      dist.barrier()

      # Load data on all ranks
      X_train = np.load(f"{OUTPUT_DIR}/.X_train.npy")
      y_train = np.load(f"{OUTPUT_DIR}/.y_train.npy")
      X_val = np.load(f"{OUTPUT_DIR}/.X_val.npy")
      y_val = np.load(f"{OUTPUT_DIR}/.y_val.npy")
      y_val_orig = np.load(f"{OUTPUT_DIR}/.y_val_orig.npy")
      input_dim = int(np.load(f"{OUTPUT_DIR}/.input_dim.npy")[0])

      dist.barrier()

      # =================================================================
      # TRAINING
      # =================================================================
      logger.info("=" * 60)
      logger.info("TRAINING MODEL")
      logger.info("=" * 60)

      # Hyperparameters
      BATCH_SIZE = 256
      LEARNING_RATE = 1e-3
      WEIGHT_DECAY = 1e-4
      HIDDEN_DIMS = [256, 128, 64]
      DROPOUT = 0.2

      if rank == 0:
          mlflow.log_params({
              "batch_size": BATCH_SIZE,
              "learning_rate": LEARNING_RATE,
              "weight_decay": WEIGHT_DECAY,
              "hidden_dims": str(HIDDEN_DIMS),
              "dropout": DROPOUT,
              "input_dim": input_dim,
          })

      train_ds = SalesDataset(X_train, y_train)
      val_ds = SalesDataset(X_val, y_val)
      sampler = DistributedSampler(train_ds, num_replicas=world_size, rank=rank)
      train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2)
      val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

      model = SalesMLP(input_dim, HIDDEN_DIMS, DROPOUT).to(device)
      model = DDP(model, device_ids=[local_rank] if torch.cuda.is_available() else None)

      optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
      scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode="min", factor=0.5, patience=3)
      criterion = nn.MSELoss()

      best_val_loss = float("inf")
      best_mape = float("inf")

      for epoch in range(NUM_EPOCHS):
          sampler.set_epoch(epoch)
          model.train()
          train_loss = 0.0
          
          for X, y in train_loader:
              X, y = X.to(device), y.to(device)
              optimizer.zero_grad()
              output = model(X)
              loss = criterion(output, y)
              loss.backward()
              torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
              optimizer.step()
              train_loss += loss.item()
          
          train_loss /= len(train_loader)
          
          # Validation
          model.eval()
          val_loss = 0.0
          predictions, actuals = [], []
          
          with torch.no_grad():
              for X, y in val_loader:
                  output = model(X.to(device))
                  val_loss += criterion(output, y.to(device)).item()
                  predictions.extend(output.cpu().numpy())
                  actuals.extend(y.numpy())
          
          val_loss /= len(val_loader)
          scheduler.step(val_loss)
          
          if rank == 0:
              predictions = np.array(predictions)
              actuals = np.array(actuals)
              
              # Unscale predictions for MAPE
              y_scaler = joblib.load(f"{OUTPUT_DIR}/y_scaler.pkl")
              preds_orig = y_scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()
              
              # Calculate MAPE
              mask = np.abs(y_val_orig) > 1000
              mape = np.mean(np.abs((y_val_orig[mask] - preds_orig[mask]) / y_val_orig[mask])) * 100 if mask.sum() > 0 else np.nan
              rmse = np.sqrt(np.mean((y_val_orig - preds_orig) ** 2))
              mae = np.mean(np.abs(y_val_orig - preds_orig))
              
              # Log metrics to MLflow
              mlflow.log_metrics({
                  "train_loss": train_loss,
                  "val_loss": val_loss,
                  "mape": mape,
                  "rmse": rmse,
                  "mae": mae,
                  "learning_rate": optimizer.param_groups[0]['lr']
              }, step=epoch)
              
              logger.info(f"Epoch {epoch+1}/{NUM_EPOCHS} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | MAPE: {mape:.1f}% | RMSE: {rmse:.0f}")
              
              if val_loss < best_val_loss:
                  best_val_loss = val_loss
                  best_mape = mape
                  torch.save(model.module.state_dict(), f"{OUTPUT_DIR}/best_model.pt")
                  logger.info("  → Saved best model")
          
          dist.barrier()

      # =================================================================
      # FINAL LOGGING TO MLFLOW + MODEL REGISTRY
      # =================================================================
      if rank == 0:
          logger.info("=" * 60)
          logger.info("LOGGING MODEL & ARTIFACTS TO MLFLOW")
          logger.info("=" * 60)
          
          # Log final metrics
          mlflow.log_metrics({
              "best_val_loss": best_val_loss,
              "best_mape": best_mape,
          })
          
          # Load best model for logging
          best_model = SalesMLP(input_dim, HIDDEN_DIMS, DROPOUT)
          best_model.load_state_dict(torch.load(f"{OUTPUT_DIR}/best_model.pt"))
          best_model.eval()
          
          # Create model signature for inference
          from mlflow.models.signature import ModelSignature
          from mlflow.types.schema import Schema, TensorSpec
          
          input_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, input_dim), "features")])
          output_schema = Schema([TensorSpec(np.dtype(np.float32), (-1,), "prediction")])
          signature = ModelSignature(inputs=input_schema, outputs=output_schema)
          
          # Log PyTorch model with signature
          logger.info("Logging PyTorch model to MLflow...")
          mlflow.pytorch.log_model(
              best_model,
              artifact_path="model",
              signature=signature,
              registered_model_name="sales-forecasting-mlp",  # Auto-register in MLflow Model Registry
              pip_requirements=["torch>=2.0", "numpy", "scikit-learn"],
          )
          
          # Log artifacts (scalers, feature list)
          logger.info("Logging artifacts...")
          mlflow.log_artifact(f"{OUTPUT_DIR}/scaler.pkl", artifact_path="preprocessing")
          mlflow.log_artifact(f"{OUTPUT_DIR}/y_scaler.pkl", artifact_path="preprocessing")
          mlflow.log_artifact(f"{OUTPUT_DIR}/feature_cols.pkl", artifact_path="preprocessing")
          
          # Log model metadata as JSON
          import json
          model_metadata = {
              "model_type": "SalesMLP",
              "input_dim": input_dim,
              "hidden_dims": HIDDEN_DIMS,
              "dropout": DROPOUT,
              "feature_columns": joblib.load(f"{OUTPUT_DIR}/feature_cols.pkl"),
              "best_mape": float(best_mape),
              "best_val_loss": float(best_val_loss),
              "training_epochs": NUM_EPOCHS,
          }
          with open(f"{OUTPUT_DIR}/model_metadata.json", "w") as f:
              json.dump(model_metadata, f, indent=2)
          mlflow.log_artifact(f"{OUTPUT_DIR}/model_metadata.json")
          
          # Get run info for Model Registry
          run_id = mlflow.active_run().info.run_id
          artifact_uri = mlflow.get_artifact_uri("model")
          
          logger.info(f"MLflow run ID: {run_id}")
          logger.info(f"Model artifact URI: {artifact_uri}")
          
          # End MLflow run
          mlflow.end_run()
          
          # =================================================================
          # REGISTER MODEL IN OPENSHIFT AI MODEL REGISTRY
          # =================================================================
          MODEL_REGISTRY_URI = os.environ.get("MODEL_REGISTRY_URI", "")
          
          if MODEL_REGISTRY_URI:
              logger.info("=" * 60)
              logger.info("REGISTERING MODEL IN OPENSHIFT AI MODEL REGISTRY")
              logger.info("=" * 60)
              
              try:
                  from model_registry import ModelRegistry
                  from model_registry.types import ModelArtifact, ModelVersion, RegisteredModel
                  
                  # Connect to Model Registry
                  registry = ModelRegistry(
                      server_address=MODEL_REGISTRY_URI,
                      port=8080,
                      author="kubeflow-training",
                      is_secure=False,
                  )
                  
                  # Register model
                  registered_model = registry.register_model(
                      name="sales-forecasting",
                      uri=artifact_uri,
                      version=f"v{datetime.now().strftime('%Y%m%d%H%M%S')}",
                      description=f"Sales demand forecasting MLP - MAPE: {best_mape:.1f}%",
                      model_format_name="pytorch",
                      model_format_version="2.0",
                      storage_key="mlflow",
                      metadata={
                          "mape": str(best_mape),
                          "val_loss": str(best_val_loss),
                          "mlflow_run_id": run_id,
                          "framework": "pytorch",
                      }
                  )
                  
                  logger.info(f"✅ Model registered in OpenShift AI Model Registry!")
                  logger.info(f"   Model ID: {registered_model.id}")
                  logger.info(f"   Ready for KServe deployment")
                  
              except Exception as e:
                  logger.warning(f"Model Registry registration skipped: {e}")
                  logger.info("Model is still available in MLflow Model Registry")
          
          logger.info("=" * 60)
          logger.info("✅ TRAINING COMPLETE!")
          logger.info("=" * 60)
          logger.info(f"   Best val_loss: {best_val_loss:.4f}")
          logger.info(f"   Best MAPE: {best_mape:.1f}%")
          logger.info(f"   Model saved to: {OUTPUT_DIR}/best_model.pt")
          logger.info(f"   MLflow experiment: {MLFLOW_EXPERIMENT_NAME}")
          logger.info(f"   MLflow run ID: {run_id}")
          logger.info(f"   Model artifact: {artifact_uri}")
          
          # Cleanup temp files
          for f in [".X_train.npy", ".y_train.npy", ".X_val.npy", ".y_val.npy", ".y_val_orig.npy", ".input_dim.npy"]:
              try: os.remove(f"{OUTPUT_DIR}/{f}")
              except: pass

      dist.destroy_process_group()
      TRAIN_SCRIPT

      torchrun --nnodes=$PET_NNODES --nproc_per_node=$PET_NPROC_PER_NODE --node_rank=$PET_NODE_RANK \
        --rdzv_backend=c10d --rdzv_endpoint=$PET_MASTER_ADDR:$PET_MASTER_PORT /tmp/train_mlflow.py

