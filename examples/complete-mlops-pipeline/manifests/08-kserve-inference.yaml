# KServe Inference for Sales Forecasting Model
# Deploys the trained PyTorch model as a REST API endpoint
---
# ConfigMap with the serving script
apiVersion: v1
kind: ConfigMap
metadata:
  name: sales-forecast-serve
  namespace: feast-trainer-demo
  labels:
    app: sales-forecasting
data:
  serve.py: |
    #!/usr/bin/env python3
    """Sales forecasting inference server - loads model architecture from metadata"""
    import os, json, torch, torch.nn as nn, joblib, numpy as np
    from flask import Flask, request, jsonify
    import logging

    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")
    logger = logging.getLogger(__name__)

    app = Flask(__name__)
    model, scalers, feature_cols, metadata = None, None, None, None

    # =====================================================
    # MODEL DEFINITION (must match training notebook)
    # =====================================================
    class SalesMLP(nn.Module):
        """Simple MLP matching the training notebook architecture"""
        def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout=0.2):
            super().__init__()
            layers = []
            prev_dim = input_dim
            for h_dim in hidden_dims:
                layers.extend([
                    nn.Linear(prev_dim, h_dim),
                    nn.BatchNorm1d(h_dim),
                    nn.ReLU(),
                    nn.Dropout(dropout)
                ])
                prev_dim = h_dim
            layers.append(nn.Linear(prev_dim, 1))
            self.net = nn.Sequential(*layers)

        def forward(self, x):
            return self.net(x).squeeze(-1)

    def load_model():
        global model, scalers, feature_cols, metadata
        model_dir = os.getenv("MODEL_DIR", "/shared/models")

        logger.info(f"Loading model from {model_dir}...")

        # Load metadata
        with open(f"{model_dir}/model_metadata.json") as f:
            metadata = json.load(f)

        hidden_dims = metadata.get("hidden_dims", [256, 128, 64])
        dropout = metadata.get("dropout", 0.2)
        input_dim = metadata["input_dim"]

        # Build and load model
        model = SalesMLP(input_dim, hidden_dims, dropout)
        model.load_state_dict(torch.load(f"{model_dir}/best_model.pt", map_location="cpu", weights_only=True))
        model.eval()

        # Load scalers
        scalers = joblib.load(f"{model_dir}/scalers.joblib")
        feature_cols = metadata["feature_columns"]

        logger.info(f"Model loaded: {len(feature_cols)} features, arch={hidden_dims}")
        logger.info(f"Best MAPE: {metadata.get('best_mape', 'N/A')}%")

    @app.route("/health", methods=["GET"])
    def health():
        return jsonify({"status": "healthy", "model": "sales-forecast"})

    @app.route("/v1/models/sales-forecast", methods=["GET"])
    def model_info():
        return jsonify({
            "name": "sales-forecast",
            "features": feature_cols,
            "hidden_dims": metadata.get("hidden_dims"),
            "best_mape": metadata.get("best_mape"),
            "best_val_loss": metadata.get("best_val_loss"),
            "device_trained": metadata.get("device_type", "unknown")
        })

    @app.route("/v1/models/sales-forecast:predict", methods=["POST"])
    def predict():
        try:
            data = request.json

            # Support both formats:
            # {"instances": [[f1, f2, ...]]} - array format
            # {"instances": [{"lag_1": 1.0, ...}]} - dict format
            instances = data.get("instances", [])

            X = np.array([
                [inst.get(c, 0) if isinstance(inst, dict) else inst[i]
                 for i, c in enumerate(feature_cols)]
                for inst in instances
            ])

            X_scaled = scalers["scaler_X"].transform(X)

            with torch.no_grad():
                preds = model(torch.FloatTensor(X_scaled)).numpy()

            predictions = scalers["scaler_y"].inverse_transform(preds.reshape(-1, 1)).flatten()

            return jsonify({"predictions": predictions.tolist()})

        except Exception as e:
            logger.error(f"Prediction error: {e}")
            return jsonify({"error": str(e)}), 500

    @app.route("/v1/models/sales-forecast:explain", methods=["POST"])
    def explain():
        """Return feature importance from first layer weights"""
        weights = model.net[0].weight.abs().mean(dim=0).detach().numpy()
        importance = {f: float(w) / weights.sum() for f, w in zip(feature_cols, weights)}
        return jsonify({
            "feature_importance": dict(sorted(importance.items(), key=lambda x: -x[1]))
        })

    if __name__ == "__main__":
        load_model()
        app.run(host="0.0.0.0", port=8080, threaded=True)

---
# InferenceService with custom container
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: sales-forecast
  namespace: feast-trainer-demo
  labels:
    app: sales-forecasting
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    minReplicas: 1
    maxReplicas: 3
    containers:
      - name: kserve-container
        # Use training image - has PyTorch pre-installed
        image: quay.io/modh/training:py311-cuda124-torch251
        command: ["/bin/bash", "-c"]
        args:
          - pip install -q flask joblib numpy scikit-learn && python /scripts/serve.py
        ports:
          - containerPort: 8080
            protocol: TCP
        env:
          - name: MODEL_DIR
            value: /shared/models
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "2"
            memory: "4Gi"
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 90
          periodSeconds: 30
        volumeMounts:
          - name: scripts
            mountPath: /scripts
          - name: model-storage
            mountPath: /shared
    volumes:
      - name: scripts
        configMap:
          name: sales-forecast-serve
      - name: model-storage
        persistentVolumeClaim:
          claimName: feast-pvc

---
# Non-headless service for Route (KServe creates headless service by default)
apiVersion: v1
kind: Service
metadata:
  name: sales-forecast-external
  namespace: feast-trainer-demo
  labels:
    app: sales-forecasting
spec:
  selector:
    app: isvc.sales-forecast-predictor
  ports:
    - name: http
      port: 8080
      targetPort: 8080
  type: ClusterIP

---
# OpenShift Route for external access
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: sales-forecast
  namespace: feast-trainer-demo
  labels:
    app: sales-forecasting
spec:
  to:
    kind: Service
    name: sales-forecast-external
  port:
    targetPort: http
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
