# KubeRay Cluster for Distributed Feast Feature Processing
# Based on: https://feast.dev/blog/feast-ray-distributed-processing/
# Requires: KubeRay operator installed in the cluster
apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: feast-ray
  namespace: feast-trainer-demo
  labels:
    app: feast-ray
    ray.io/cluster: feast-ray
  annotations:
    # Disable ODH TLS for internal cluster communication
    odh.ray.io/secure-trusted-network: "false"
spec:
  rayVersion: '2.35.0'
  # Autoscaling disabled for Kueue compatibility
  enableInTreeAutoscaling: false
  
  # Head node configuration
  headGroupSpec:
    serviceType: ClusterIP
    rayStartParams:
      dashboard-host: '0.0.0.0'
      num-cpus: '2'
      block: 'true'
      ray-client-server-port: '10001'
    template:
      metadata:
        labels:
          app: feast-ray
          component: head
          ray.io/cluster: feast-ray
      spec:
        containers:
        - name: ray-head
          image: quay.io/modh/ray:2.35.0-py311-cu121
          ports:
          - containerPort: 6379
            name: gcs
          - containerPort: 8265
            name: dashboard
          - containerPort: 10001
            name: client
          - containerPort: 8080
            name: metrics
          resources:
            limits:
              cpu: "4"
              memory: "8Gi"
            requests:
              cpu: "2"
              memory: "4Gi"
          env:
          - name: RAY_GRAFANA_HOST
            value: "http://grafana:3000"
          - name: RAY_PROMETHEUS_HOST
            value: "http://prometheus:9090"
          # Pre-install Feast on head node
          - name: RAY_RUNTIME_ENV_SETUP
            value: "pip install feast[postgres,ray]==0.59.0 psycopg2-binary"
          volumeMounts:
          - name: ray-logs
            mountPath: /tmp/ray
          - name: shared-storage
            mountPath: /shared
        volumes:
        - name: ray-logs
          emptyDir: {}
        - name: shared-storage
          persistentVolumeClaim:
            claimName: feast-pvc
  
  # Worker nodes configuration
  workerGroupSpecs:
  - replicas: 2
    minReplicas: 1
    maxReplicas: 4
    groupName: workers
    rayStartParams:
      num-cpus: '4'
      block: 'true'
    template:
      metadata:
        labels:
          app: feast-ray
          component: worker
          ray.io/cluster: feast-ray
      spec:
        containers:
        - name: ray-worker
          image: quay.io/modh/ray:2.35.0-py311-cu121
          resources:
            limits:
              cpu: "4"
              memory: "8Gi"
            requests:
              cpu: "2"
              memory: "4Gi"
          env:
          - name: RAY_DISABLE_MEMORY_MONITOR
            value: "1"
          volumeMounts:
          - name: ray-logs
            mountPath: /tmp/ray
          - name: shared-storage
            mountPath: /shared
        volumes:
        - name: ray-logs
          emptyDir: {}
        - name: shared-storage
          persistentVolumeClaim:
            claimName: feast-pvc
---
# Service for Ray head node
apiVersion: v1
kind: Service
metadata:
  name: feast-ray-head
  namespace: feast-trainer-demo
  labels:
    app: feast-ray
    ray.io/cluster: feast-ray
spec:
  selector:
    app: feast-ray
    component: head
  ports:
  - name: client
    port: 10001
    targetPort: 10001
  - name: dashboard
    port: 8265
    targetPort: 8265
  - name: gcs
    port: 6379
    targetPort: 6379
  - name: metrics
    port: 8080
    targetPort: 8080
  type: ClusterIP
---
# Route for Ray Dashboard (OpenShift)
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: feast-ray-dashboard
  namespace: feast-trainer-demo
spec:
  to:
    kind: Service
    name: feast-ray-head
  port:
    targetPort: dashboard
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
