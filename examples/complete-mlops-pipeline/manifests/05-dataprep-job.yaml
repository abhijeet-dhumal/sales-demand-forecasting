# Feast Feature Store Setup (Data Preparation)
# 
# This Job prepares the Feast Feature Store:
# 1. Generates sales data (parquet files)
# 2. Creates Feast project with TWO configs:
#    - feature_store.yaml (file-based) - for apply/materialize
#    - feature_store_ray.yaml (Ray) - for training's get_historical_features()
# 3. feast apply - registers features to PostgreSQL registry
# 4. feast materialize-incremental - populates online store
# 5. Tests online feature retrieval
#
# NOTE: get_historical_features() is called in Training Job using Ray config
#
# Architecture:
# - Registry: PostgreSQL (SQL-based, durable)
# - Offline Store: File (apply/materialize) | Ray (training)
# - Online Store: PostgreSQL (low-latency serving)
#
apiVersion: batch/v1
kind: Job
metadata:
  name: feast-dataprep
  namespace: feast-trainer-demo
  labels:
    app: feast-dataprep
    pipeline: sales-forecasting
    component: feature-engineering
spec:
  backoffLimit: 2
  ttlSecondsAfterFinished: 3600
  template:
    metadata:
      labels:
        app: feast-dataprep
        pipeline: sales-forecasting
    spec:
      serviceAccountName: feast-sa
      restartPolicy: Never
      containers:
        - name: feast-ray
          # CRITICAL: Must match RayCluster Python version for Ray client compatibility
          image: quay.io/modh/ray:2.52.1-py312-cu128
          resources:
            requests:
              cpu: "2"
              memory: "4Gi"
            limits:
              cpu: "4"
              memory: "8Gi"
          env:
            # PostgreSQL configuration
            - name: POSTGRES_HOST
              value: "postgres.feast-trainer-demo.svc.cluster.local"
            # KubeRay/CodeFlare SDK configuration
            - name: FEAST_RAY_USE_KUBERAY
              value: "true"
            - name: FEAST_RAY_CLUSTER_NAME
              value: "feast-ray"
            - name: FEAST_RAY_NAMESPACE
              value: "feast-trainer-demo"
            - name: FEAST_RAY_SKIP_TLS
              value: "true"
            # Data paths
            - name: DATA_DIR
              value: "/shared/data"
            - name: FEATURE_REPO_DIR
              value: "/shared/feature_repo"
            # Data generation config
            - name: DATA_CONFIG
              value: '{"start_date": "2022-01-01", "weeks": 104, "stores": 10, "departments": 5, "seed": 42}'
            # Python unbuffered output
            - name: PYTHONUNBUFFERED
              value: "1"
          volumeMounts:
            - name: shared-data
              mountPath: /shared
            - name: scripts
              mountPath: /scripts
          command:
            - /bin/bash
            - -c
            - |
              set -e
              
              echo "============================================================"
              echo "üçï FEAST + RAY DISTRIBUTED FEATURE ENGINEERING"
              echo "============================================================"
              
              # Install Feast with PostgreSQL, Ray, and CodeFlare SDK support
              echo "üì¶ Installing dependencies..."
              pip install --target=/tmp/pylibs -q feast[postgres,ray]==0.59.0 codeflare-sdk psycopg2-binary
              
              # Add to Python path
              export PYTHONPATH=/tmp/pylibs:$PYTHONPATH
              export PATH=/tmp/pylibs/bin:$PATH
              
              # Setup in-cluster auth for CodeFlare SDK
              export FEAST_RAY_AUTH_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
              export FEAST_RAY_AUTH_SERVER="https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}"
              echo "üîê In-cluster auth configured for CodeFlare SDK"
              
              # Run Feast workflow
              echo "üöÄ Running Feast workflow with KubeRay..."
              bash /scripts/run.sh
      volumes:
        - name: shared-data
          persistentVolumeClaim:
            claimName: feast-pvc
        - name: scripts
          configMap:
            name: feast-dataprep-scripts
---
# ConfigMap with all Feast + Ray scripts
apiVersion: v1
kind: ConfigMap
metadata:
  name: feast-dataprep-scripts
  namespace: feast-trainer-demo
  labels:
    app: feast-dataprep
    pipeline: sales-forecasting
data:
  # Feast Feature Store Configuration
  # File-based offline store for feast apply/materialize (stable)
  # Training Job uses Ray config for get_historical_features()
  feature_store.yaml: |
    project: sales_forecasting
    provider: local
    
    registry:
      registry_type: sql
      path: postgresql+psycopg2://feast:feast123@postgres.feast-trainer-demo.svc.cluster.local:5432/feast
      cache_ttl_seconds: 60
    
    # File-based offline store (stable for apply/materialize)
    offline_store:
      type: file
    
    # PostgreSQL online store for low-latency serving
    online_store:
      type: postgres
      host: postgres.feast-trainer-demo.svc.cluster.local
      port: 5432
      database: feast
      user: feast
      password: feast123
    
    entity_key_serialization_version: 3
  
  # Ray-enabled config (used by Training Job for get_historical_features)
  feature_store_ray.yaml: |
    project: sales_forecasting
    provider: local
    
    registry:
      registry_type: sql
      path: postgresql+psycopg2://feast:feast123@postgres.feast-trainer-demo.svc.cluster.local:5432/feast
      cache_ttl_seconds: 60
    
    # Ray offline store in KubeRay mode (for distributed historical retrieval)
    offline_store:
      type: ray
      storage_path: /shared/data/ray_storage
      use_kuberay: true
      kuberay_conf:
        cluster_name: "feast-ray"
        namespace: "feast-trainer-demo"
        skip_tls: true
      broadcast_join_threshold_mb: 100
      enable_distributed_joins: true
      enable_ray_logging: true
    
    online_store:
      type: postgres
      host: postgres.feast-trainer-demo.svc.cluster.local
      port: 5432
      database: feast
      user: feast
      password: feast123
    
    entity_key_serialization_version: 3

  # Feature definitions with On-Demand Features
  # Based on official Feast templates pattern
  features.py: |
    from datetime import timedelta
    import pandas as pd
    from feast import Entity, FeatureView, Field, FileSource, FeatureService
    from feast.on_demand_feature_view import on_demand_feature_view
    from feast.types import Float32, Float64, Int64, String
    
    # Entities - business objects we're tracking
    store = Entity(
        name="store_id",
        join_keys=["store_id"],
        description="Retail store identifier",
    )
    department = Entity(
        name="dept_id",
        join_keys=["dept_id"],
        description="Department within a store",
    )
    
    # Sales features (batch) - weekly aggregated metrics
    sales_features = FeatureView(
        name="sales_features",
        description="Weekly sales metrics with lag features for demand forecasting",
        entities=[store, department],
        ttl=timedelta(days=90),  # 90 days freshness - tune based on use case
        schema=[
            Field(name="weekly_sales", dtype=Float32, description="Target: total weekly sales ($)"),
            Field(name="is_holiday", dtype=Int64, description="Holiday week indicator"),
            Field(name="temperature", dtype=Float32, description="Average weekly temperature (F)"),
            Field(name="fuel_price", dtype=Float32, description="Average fuel price ($/gal)"),
            Field(name="cpi", dtype=Float32, description="Consumer Price Index"),
            Field(name="unemployment", dtype=Float32, description="Unemployment rate (%)"),
            Field(name="lag_1", dtype=Float32, description="Sales 1 week ago"),
            Field(name="lag_2", dtype=Float32, description="Sales 2 weeks ago"),
            Field(name="lag_4", dtype=Float32, description="Sales 4 weeks ago"),
            Field(name="lag_8", dtype=Float32, description="Sales 8 weeks ago"),
            Field(name="lag_52", dtype=Float32, description="Sales 52 weeks ago (YoY)"),
            Field(name="rolling_mean_4w", dtype=Float32, description="4-week rolling average"),
        ],
        source=FileSource(path="/shared/data/sales_features.parquet", timestamp_field="event_timestamp"),
        online=True,
        tags={"team": "demand-forecasting", "tier": "critical"},
    )
    
    # Store features (batch) - slowly changing dimensions
    store_features = FeatureView(
        name="store_features",
        description="Store attributes (slowly changing dimensions)",
        entities=[store],
        ttl=timedelta(days=365),  # Store attributes change rarely
        schema=[
            Field(name="store_type", dtype=String, description="Store classification (A/B/C)"),
            Field(name="store_size", dtype=Int64, description="Store size in sq ft"),
            Field(name="region", dtype=String, description="Geographic region"),
        ],
        source=FileSource(path="/shared/data/store_features.parquet", timestamp_field="event_timestamp"),
        online=True,
        tags={"team": "demand-forecasting", "tier": "standard"},
    )
    
    # On-demand feature view: derived features computed at request time
    # Uses python mode with singleton=True for row-by-row processing
    @on_demand_feature_view(
        sources=[sales_features, store_features],
        schema=[
            Field(name="sales_per_sqft", dtype=Float64),
            Field(name="sales_velocity", dtype=Float64),
            Field(name="yoy_growth", dtype=Float64),
        ],
        mode="python",
        singleton=True,
    )
    def derived_features(inputs: dict) -> dict:
        weekly_sales = float(inputs.get("weekly_sales") or 0)
        store_size = float(inputs.get("store_size") or 1) or 1
        lag_1 = float(inputs.get("lag_1") or 1) or 1
        lag_52 = float(inputs.get("lag_52") or 1) or 1
        return {
            "sales_per_sqft": weekly_sales / store_size,
            "sales_velocity": (weekly_sales - lag_1) / lag_1,
            "yoy_growth": (weekly_sales - lag_52) / lag_52,
        }
    
    # On-demand feature view: trend analysis
    @on_demand_feature_view(
        sources=[sales_features],
        schema=[
            Field(name="trend_score", dtype=Float64),
            Field(name="volatility", dtype=Float64),
        ],
        mode="python",
        singleton=True,
    )
    def trend_features(inputs: dict) -> dict:
        weekly_sales = float(inputs.get("weekly_sales") or 0)
        lag_1 = float(inputs.get("lag_1") or 1) or 1
        lag_4 = float(inputs.get("lag_4") or 1) or 1
        rolling_mean = float(inputs.get("rolling_mean_4w") or 1) or 1
        short_trend = (weekly_sales - lag_1) / lag_1
        medium_trend = (weekly_sales - lag_4) / lag_4
        return {
            "trend_score": short_trend * 0.6 + medium_trend * 0.4,
            "volatility": abs(weekly_sales - rolling_mean) / rolling_mean,
        }
    
    # Feature Services - curated feature sets for different use cases
    # This ensures training/serving consistency (prevents train-serve skew)
    
    # Training: use raw batch features only (ODFVs are for inference-time computation)
    training_features = FeatureService(
        name="training_features",
        description="Full feature set for model training (raw batch features, includes target)",
        features=[
            sales_features[["weekly_sales", "lag_1", "lag_2", "lag_4", "lag_8", "lag_52",
                           "rolling_mean_4w", "temperature", "fuel_price", "cpi", "unemployment", "is_holiday"]],
            store_features[["store_type", "store_size", "region"]],
        ],
        tags={"stage": "training", "model": "demand-forecasting"},
    )
    
    inference_features = FeatureService(
        name="inference_features",
        description="Features for real-time inference (excludes target)",
        features=[
            sales_features[["lag_1", "lag_2", "lag_4", "rolling_mean_4w",
                           "temperature", "fuel_price", "cpi", "unemployment"]],
            store_features[["store_size"]],
            derived_features,
            trend_features,
        ],
        tags={"stage": "inference", "model": "demand-forecasting"},
    )
    
    monitoring_features = FeatureService(
        name="monitoring_features",
        description="Features for drift detection and model monitoring",
        features=[
            sales_features[["weekly_sales", "rolling_mean_4w", "lag_1", "lag_52"]],
            trend_features,
        ],
        tags={"stage": "monitoring", "model": "demand-forecasting"},
    )

  # Data generator
  data_generator.py: |
    """Generate sample sales data"""
    import os
    import json
    import pandas as pd
    import numpy as np
    from datetime import datetime, timedelta, timezone
    from pathlib import Path
    
    config = json.loads(os.getenv("DATA_CONFIG", '{"start_date": "2022-01-01", "weeks": 104, "stores": 10, "departments": 5, "seed": 42}'))
    data_dir = os.getenv("DATA_DIR", "/shared/data")
    
    np.random.seed(config.get("seed", 42))
    base_date = datetime.fromisoformat(config["start_date"]).replace(tzinfo=timezone.utc)
    
    records = []
    for week in range(config["weeks"]):
        week_date = base_date + timedelta(weeks=week)
        week_of_year = week % 52
        seasonal = 1 + 0.3 * np.sin(2 * np.pi * week_of_year / 52)
        holiday = 1.5 if 47 <= week_of_year <= 52 else 1.0
        
        for store_id in range(1, config["stores"] + 1):
            store_base = 50000 + store_id * 5000
            for dept_id in range(1, config["departments"] + 1):
                dept_factor = 0.5 + dept_id * 0.2
                records.append({
                    "store_id": store_id,
                    "dept_id": dept_id,
                    "event_timestamp": week_date,
                    "weekly_sales": round(max(0, store_base * dept_factor * seasonal * holiday + np.random.normal(0, 2000)), 2),
                    "is_holiday": int(holiday > 1),
                    "temperature": round(60 + 20 * np.sin(2 * np.pi * week_of_year / 52) + np.random.normal(0, 5), 1),
                    "fuel_price": round(3 + 0.5 * np.random.random(), 2),
                    "cpi": round(220 + week * 0.1, 1),
                    "unemployment": round(5 + np.random.normal(0, 0.5), 1)
                })
    
    Path(data_dir).mkdir(parents=True, exist_ok=True)
    
    sales_df = pd.DataFrame(records)
    sales_df = sales_df.sort_values(["store_id", "dept_id", "event_timestamp"])
    
    # Add lag features
    for lag in [1, 2, 4, 8, 52]:
        sales_df[f"lag_{lag}"] = sales_df.groupby(["store_id", "dept_id"])["weekly_sales"].shift(lag)
    sales_df["rolling_mean_4w"] = sales_df.groupby(["store_id", "dept_id"])["weekly_sales"].transform(
        lambda x: x.rolling(4, min_periods=1).mean())
    sales_df = sales_df.fillna(0)
    
    # Save parquet files
    sales_df.to_parquet(f"{data_dir}/sales_features.parquet", index=False)
    
    training_df = sales_df.rename(columns={"event_timestamp": "date"})
    training_df.to_parquet(f"{data_dir}/features.parquet", index=False)
    
    entities_df = sales_df[["store_id", "dept_id", "event_timestamp"]].rename(columns={"event_timestamp": "date"})
    entities_df.to_parquet(f"{data_dir}/entities.parquet", index=False)
    
    stores_df = pd.DataFrame([{
        "store_id": i,
        "event_timestamp": base_date,
        "store_type": ["A", "B", "C"][i % 3],
        "store_size": 100000 + i * 10000,
        "region": f"region_{(i - 1) // 3 + 1}"
    } for i in range(1, config["stores"] + 1)])
    stores_df.to_parquet(f"{data_dir}/store_features.parquet", index=False)
    
    print(f"‚úÖ Generated {len(sales_df):,} sales records, {len(stores_df)} stores")
    print(f"   Files: {data_dir}/")

  # Main execution script
  run.sh: |
    #!/bin/bash
    set -e
    
    DATA_DIR="${DATA_DIR:-/shared/data}"
    FEATURE_REPO="${FEATURE_REPO_DIR:-/shared/feature_repo}"
    
    echo ""
    echo "üìã Configuration:"
    echo "   DATA_DIR: $DATA_DIR"
    echo "   FEATURE_REPO: $FEATURE_REPO"
    echo "   Ray Mode: KubeRay (CodeFlare SDK)"
    
    # Step 1: Generate data
    echo ""
    echo "üìä Step 1: Generate Sales Data"
    echo "============================================================"
    python /scripts/data_generator.py
    
    # Step 2: Setup Feast project
    echo ""
    echo "‚öôÔ∏è  Step 2: Setup Feast Project"
    echo "============================================================"
    mkdir -p "$FEATURE_REPO"
    mkdir -p "$DATA_DIR/ray_storage"
    mkdir -p "$DATA_DIR/saved_datasets"
    # Clean up stale files
    rm -f "$FEATURE_REPO"/*.py "$FEATURE_REPO"/*.yaml 2>/dev/null || true
    # Copy Feast configs and features
    cp /scripts/feature_store.yaml "$FEATURE_REPO/"           # File-based (for apply/materialize)
    cp /scripts/feature_store_ray.yaml "$FEATURE_REPO/"       # Ray-enabled (for training)
    cp /scripts/features.py "$FEATURE_REPO/"
    echo "   ‚úÖ Project: $FEATURE_REPO"
    echo "   üìÅ Config: feature_store.yaml (file-based)"
    echo "   üìÅ Config: feature_store_ray.yaml (Ray/KubeRay)"
    
    # Step 3: Apply features
    echo ""
    echo "üìù Step 3: feast apply (register features)"
    echo "============================================================"
    cd "$FEATURE_REPO"
    feast apply
    echo "   ‚úÖ Features registered to PostgreSQL registry"
    
    # Step 4: Materialize to online store
    echo ""
    echo "üöÄ Step 4: feast materialize (Populate Online Store)"
    echo "============================================================"
    echo "   Materializing features to PostgreSQL online store..."
    # Full materialization from data start to now
    # For production, use feast materialize-incremental for subsequent runs
    feast materialize 2022-01-01T00:00:00 $(date -u +%Y-%m-%dT%H:%M:%S)
    echo "   ‚úÖ Features materialized to PostgreSQL online store"
    
    # Step 5: Verify Features (including On-Demand)
    echo ""
    echo "üîç Step 5: Verify Features (Batch + On-Demand)"
    echo "============================================================"
    python - << 'VERIFY'
    from feast import FeatureStore
    import pandas as pd
    from datetime import datetime, timezone, timedelta
    
    store = FeatureStore(repo_path=".")
    
    print("   üìã Registered Objects:")
    print(f"      Entities: {[e.name for e in store.list_entities()]}")
    print(f"      FeatureViews: {[fv.name for fv in store.list_feature_views()]}")
    print(f"      OnDemandFeatureViews: {[odfv.name for odfv in store.list_on_demand_feature_views()]}")
    print(f"      FeatureServices: {[fs.name for fs in store.list_feature_services()]}")
    
    # Online lookup with batch features
    print("\n   üîÑ Online Feature Lookup (Batch Features):")
    online = store.get_online_features(
        features=["sales_features:weekly_sales", "sales_features:lag_1", "store_features:store_size"],
        entity_rows=[{"store_id": 1, "dept_id": 1}]
    ).to_dict()
    
    ws = online.get('weekly_sales', [None])[0]
    lag = online.get('lag_1', [None])[0]
    size = online.get('store_size', [None])[0]
    
    print(f"      weekly_sales: ${ws:,.0f}" if ws else "      weekly_sales: N/A (run materialize first)")
    print(f"      lag_1: ${lag:,.0f}" if lag else "      lag_1: N/A")
    print(f"      store_size: {size:,} sqft" if size else "      store_size: N/A")
    
    # Test On-Demand Feature Views
    print("\n   ‚ö° On-Demand Features (Real-time Computation):")
    inference_result = store.get_online_features(
        features=[
            "sales_features:weekly_sales",
            "sales_features:lag_1",
            "sales_features:rolling_mean_4w",
            "store_features:store_size",
            "derived_features:sales_per_sqft",
            "derived_features:sales_velocity",
            "trend_features:trend_score",
        ],
        entity_rows=[{"store_id": 1, "dept_id": 1}]
    ).to_dict()
    
    sps = inference_result.get('sales_per_sqft', [None])[0]
    sv = inference_result.get('sales_velocity', [None])[0]
    ts = inference_result.get('trend_score', [None])[0]
    
    print(f"      sales_per_sqft: {sps:.2f}" if sps is not None else "      sales_per_sqft: N/A")
    print(f"      sales_velocity: {sv:.2%}" if sv is not None else "      sales_velocity: N/A")
    print(f"      trend_score: {ts:.3f}" if ts is not None else "      trend_score: N/A")
    
    print("\n   ‚úÖ Feature retrieval (batch + on-demand) working!")
    VERIFY
    
    # Note: get_historical_features() is called in Training Job (not here)
    # This avoids redundant Ray calls and demonstrates full Feast integration in training
    
    echo ""
    echo "============================================================"
    echo "‚úÖ FEAST FEATURE STORE READY!"
    echo "============================================================"
    echo "   üìÅ Data: $DATA_DIR"
    echo "   üìù Feature Repo: $FEATURE_REPO"
    echo "   üîó Registry: PostgreSQL"
    echo "   ‚ö° Offline Store: Ray (KubeRay + CodeFlare SDK)"
    echo "   üíæ Online Store: PostgreSQL (materialized)"
    echo ""
    echo "   Next Steps:"
    echo "   - Training Job will call get_historical_features()"
    echo "   - Inference Service will call get_online_features()"
    echo "   - Both use Feature Services for consistency"
