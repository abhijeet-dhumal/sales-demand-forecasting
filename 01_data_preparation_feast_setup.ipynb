{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Data Preparation & Feast Setup\n",
    "\n",
    "This notebook covers:\n",
    "- Downloading Walmart sales dataset from Kaggle\n",
    "- Feature engineering (lags, rolling stats) - **embedded in notebook**\n",
    "- Registering features with Feast\n",
    "- Testing feature retrieval\n",
    "\n",
    "**Prerequisites:**\n",
    "- Kaggle API credentials (environment variables recommended)\n",
    "- Competition rules accepted: https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install feast==0.54.0 kaggle==1.7.4.5 pandas==2.2.3 pyarrow==17.0.0 scikit-learn==1.6.1 psycopg2-binary==2.9.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful !\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from feast import FeatureStore\n",
    "import time\n",
    "\n",
    "print('Imports successful !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Kaggle Credentials\n",
    "\n",
    "**Option 1: Environment Variables (Recommended)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle credentials configured (env vars)\n"
     ]
    }
   ],
   "source": [
    "# Set your Kaggle credentials (uncomment and fill in)\n",
    "os.environ['KAGGLE_USERNAME'] = '<>'\n",
    "os.environ['KAGGLE_KEY'] = '<>'\n",
    "\n",
    "# Verify configuration\n",
    "has_env = 'KAGGLE_USERNAME' in os.environ and 'KAGGLE_KEY' in os.environ\n",
    "has_file = (Path.home() / '.kaggle' / 'kaggle.json').exists()\n",
    "\n",
    "if has_env:\n",
    "    print(f'Kaggle credentials configured (env vars)')\n",
    "elif has_file:\n",
    "    print(f'Kaggle credentials configured (file)')\n",
    "else:\n",
    "    print('Kaggle credentials not configured!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files already exist\n",
      "\n",
      "Downloaded files:\n",
      "   features.csv: 0.6 MB\n",
      "   stores.csv: 0.0 MB\n",
      "   train.csv: 12.2 MB\n"
     ]
    }
   ],
   "source": [
    "# Setup paths\n",
    "data_dir = Path('/shared/datasets')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if already downloaded\n",
    "required_files = ['train.csv', 'features.csv', 'stores.csv']\n",
    "files_exist = all((data_dir / f).exists() for f in required_files)\n",
    "\n",
    "if files_exist:\n",
    "    print('CSV files already exist')\n",
    "else:\n",
    "    print('Downloading from Kaggle...')\n",
    "    \n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    \n",
    "    # Initialize and authenticate\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    \n",
    "    # Download competition files\n",
    "    competition = 'walmart-recruiting-store-sales-forecasting'\n",
    "    api.competition_download_files(competition, path=str(data_dir))\n",
    "    \n",
    "    # Extract main zip\n",
    "    main_zip = data_dir / f'{competition}.zip'\n",
    "    if main_zip.exists():\n",
    "        with zipfile.ZipFile(main_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "        main_zip.unlink()  # Remove zip after extraction\n",
    "    \n",
    "    # Extract individual CSV zips if needed\n",
    "    for zip_name in ['train.csv.zip', 'features.csv.zip']:\n",
    "        zip_path = data_dir / zip_name\n",
    "        if zip_path.exists():\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(data_dir)\n",
    "            zip_path.unlink()\n",
    "    \n",
    "    print('Download complete!')\n",
    "\n",
    "# Verify files\n",
    "print('\\nDownloaded files:')\n",
    "for file in sorted(data_dir.glob('*.csv')):\n",
    "    size_mb = file.stat().st_size / 1024 / 1024\n",
    "    print(f'   {file.name}: {size_mb:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Summary:\n",
      "   Sales records: 421,570\n",
      "   Stores: 45\n",
      "   Departments: 81\n",
      "   Date range: 2010-02-05 to 2012-10-26\n",
      "\n",
      "Columns:\n",
      "   train.csv: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday']\n",
      "   features.csv: ['Store', 'Date', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday']\n",
      "   stores.csv: ['Store', 'Type', 'Size']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>IsHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>46039.49</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>41595.55</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  Dept       Date  Weekly_Sales  IsHoliday\n",
       "0      1     1 2010-02-05      24924.50      False\n",
       "1      1     1 2010-02-12      46039.49       True\n",
       "2      1     1 2010-02-19      41595.55      False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CSV files\n",
    "train_df = pd.read_csv(data_dir / 'train.csv')\n",
    "features_df = pd.read_csv(data_dir / 'features.csv')\n",
    "stores_df = pd.read_csv(data_dir / 'stores.csv')\n",
    "\n",
    "# Convert dates\n",
    "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
    "features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
    "\n",
    "print('Dataset Summary:')\n",
    "print(f'   Sales records: {len(train_df):,}')\n",
    "print(f'   Stores: {train_df[\"Store\"].nunique()}')\n",
    "print(f'   Departments: {train_df[\"Dept\"].nunique()}')\n",
    "print(f'   Date range: {train_df[\"Date\"].min().date()} to {train_df[\"Date\"].max().date()}')\n",
    "print(f'\\nColumns:')\n",
    "print(f'   train.csv: {list(train_df.columns)}')\n",
    "print(f'   features.csv: {list(features_df.columns)}')\n",
    "print(f'   stores.csv: {list(stores_df.columns)}')\n",
    "\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering: Sales Features\n",
    "\n",
    "Compute time-series features (lags, rolling stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing time-series features...\n",
      "\n",
      "   Processed 500/3331 store-dept combinations...\n",
      "   Processed 1000/3331 store-dept combinations...\n",
      "   Processed 1500/3331 store-dept combinations...\n",
      "   Processed 2000/3331 store-dept combinations...\n",
      "   Processed 2500/3331 store-dept combinations...\n",
      "   Processed 3000/3331 store-dept combinations...\n",
      "\n",
      "Sales features created: (421570, 11)\n",
      "CPU times: user 5.43 s, sys: 178 ms, total: 5.61 s\n",
      "Wall time: 5.66 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>dept</th>\n",
       "      <th>date</th>\n",
       "      <th>weekly_sales</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>sales_lag_1</th>\n",
       "      <th>sales_lag_2</th>\n",
       "      <th>sales_lag_4</th>\n",
       "      <th>sales_rolling_mean_4</th>\n",
       "      <th>sales_rolling_mean_12</th>\n",
       "      <th>sales_rolling_std_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24924.500000</td>\n",
       "      <td>24924.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>46039.49</td>\n",
       "      <td>True</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35481.995000</td>\n",
       "      <td>35481.995000</td>\n",
       "      <td>14930.552614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>41595.55</td>\n",
       "      <td>False</td>\n",
       "      <td>46039.49</td>\n",
       "      <td>24924.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37519.846667</td>\n",
       "      <td>37519.846667</td>\n",
       "      <td>11131.900957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store  dept       date  weekly_sales  is_holiday  sales_lag_1  sales_lag_2  \\\n",
       "0      1     1 2010-02-05      24924.50       False          NaN          NaN   \n",
       "1      1     1 2010-02-12      46039.49        True     24924.50          NaN   \n",
       "2      1     1 2010-02-19      41595.55       False     46039.49      24924.5   \n",
       "\n",
       "   sales_lag_4  sales_rolling_mean_4  sales_rolling_mean_12  \\\n",
       "0          NaN          24924.500000           24924.500000   \n",
       "1          NaN          35481.995000           35481.995000   \n",
       "2          NaN          37519.846667           37519.846667   \n",
       "\n",
       "   sales_rolling_std_4  \n",
       "0             0.000000  \n",
       "1         14930.552614  \n",
       "2         11131.900957  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Computing time-series features...\\n')\n",
    "\n",
    "# Sort by store, dept, date\n",
    "train_df = train_df.sort_values(['Store', 'Dept', 'Date'])\n",
    "\n",
    "# Compute features for each store-dept combination\n",
    "sales_list = []\n",
    "total = train_df[['Store', 'Dept']].drop_duplicates().shape[0]\n",
    "\n",
    "for idx, (store_dept, group) in enumerate(train_df.groupby(['Store', 'Dept']), 1):\n",
    "    if idx % 500 == 0:\n",
    "        print(f'   Processed {idx}/{total} store-dept combinations...')\n",
    "    \n",
    "    group = group.sort_values('Date').copy()\n",
    "    \n",
    "    # Lag features (previous weeks)\n",
    "    group['sales_lag_1'] = group['Weekly_Sales'].shift(1)\n",
    "    group['sales_lag_2'] = group['Weekly_Sales'].shift(2)\n",
    "    group['sales_lag_4'] = group['Weekly_Sales'].shift(4)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    group['sales_rolling_mean_4'] = group['Weekly_Sales'].rolling(4, min_periods=1).mean()\n",
    "    group['sales_rolling_mean_12'] = group['Weekly_Sales'].rolling(12, min_periods=1).mean()\n",
    "    group['sales_rolling_std_4'] = group['Weekly_Sales'].rolling(4, min_periods=1).std().fillna(0)\n",
    "    \n",
    "    sales_list.append(group)\n",
    "\n",
    "sales_df = pd.concat(sales_list, ignore_index=True)\n",
    "\n",
    "# Rename to feast convention (lowercase)\n",
    "sales_df = sales_df.rename(columns={\n",
    "    'Store': 'store',\n",
    "    'Dept': 'dept',\n",
    "    'Date': 'date',\n",
    "    'Weekly_Sales': 'weekly_sales',\n",
    "    'IsHoliday': 'is_holiday'\n",
    "})\n",
    "\n",
    "# Select final columns\n",
    "sales_df = sales_df[[\n",
    "    'store', 'dept', 'date', 'weekly_sales', 'is_holiday',\n",
    "    'sales_lag_1', 'sales_lag_2', 'sales_lag_4',\n",
    "    'sales_rolling_mean_4', 'sales_rolling_mean_12', 'sales_rolling_std_4'\n",
    "]]\n",
    "\n",
    "print(f'\\nSales features created: {sales_df.shape}')\n",
    "sales_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering: Store Features\n",
    "\n",
    "Merge external factors with store metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating store features...\n",
      "\n",
      "Store features created: (606242, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>dept</th>\n",
       "      <th>date</th>\n",
       "      <th>temperature</th>\n",
       "      <th>fuel_price</th>\n",
       "      <th>cpi</th>\n",
       "      <th>unemployment</th>\n",
       "      <th>markdown1</th>\n",
       "      <th>markdown2</th>\n",
       "      <th>markdown3</th>\n",
       "      <th>markdown4</th>\n",
       "      <th>markdown5</th>\n",
       "      <th>total_markdown</th>\n",
       "      <th>has_markdown</th>\n",
       "      <th>store_type</th>\n",
       "      <th>store_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>42.31</td>\n",
       "      <td>2.572</td>\n",
       "      <td>211.096358</td>\n",
       "      <td>8.106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>38.51</td>\n",
       "      <td>2.548</td>\n",
       "      <td>211.242170</td>\n",
       "      <td>8.106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>39.93</td>\n",
       "      <td>2.514</td>\n",
       "      <td>211.289143</td>\n",
       "      <td>8.106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store  dept       date  temperature  fuel_price         cpi  unemployment  \\\n",
       "0      1     1 2010-02-05        42.31       2.572  211.096358         8.106   \n",
       "1      1     1 2010-02-12        38.51       2.548  211.242170         8.106   \n",
       "2      1     1 2010-02-19        39.93       2.514  211.289143         8.106   \n",
       "\n",
       "   markdown1  markdown2  markdown3  markdown4  markdown5  total_markdown  \\\n",
       "0        0.0        0.0        0.0        0.0        0.0             0.0   \n",
       "1        0.0        0.0        0.0        0.0        0.0             0.0   \n",
       "2        0.0        0.0        0.0        0.0        0.0             0.0   \n",
       "\n",
       "   has_markdown store_type  store_size  \n",
       "0             0          A      151315  \n",
       "1             0          A      151315  \n",
       "2             0          A      151315  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Creating store features...\\n')\n",
    "\n",
    "# Merge features with stores\n",
    "store_df = features_df.merge(stores_df, on='Store', how='left')\n",
    "\n",
    "# Get unique store-dept combinations from train\n",
    "unique_combos = train_df[['Store', 'Dept']].drop_duplicates()\n",
    "\n",
    "# Expand store features to store+dept level\n",
    "store_expanded = []\n",
    "for _, row in unique_combos.iterrows():\n",
    "    store_data = store_df[store_df['Store'] == row['Store']].copy()\n",
    "    store_data['Dept'] = row['Dept']\n",
    "    store_expanded.append(store_data)\n",
    "\n",
    "store_expanded_df = pd.concat(store_expanded, ignore_index=True)\n",
    "\n",
    "# Fill missing markdowns with 0\n",
    "markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
    "for col in markdown_cols:\n",
    "    store_expanded_df[col] = store_expanded_df[col].fillna(0)\n",
    "\n",
    "# Compute total markdown\n",
    "store_expanded_df['total_markdown'] = store_expanded_df[markdown_cols].sum(axis=1)\n",
    "store_expanded_df['has_markdown'] = (store_expanded_df['total_markdown'] > 0).astype(int)\n",
    "\n",
    "# Rename to feast convention\n",
    "store_expanded_df = store_expanded_df.rename(columns={\n",
    "    'Store': 'store',\n",
    "    'Dept': 'dept',\n",
    "    'Date': 'date',\n",
    "    'Temperature': 'temperature',\n",
    "    'Fuel_Price': 'fuel_price',\n",
    "    'CPI': 'cpi',\n",
    "    'Unemployment': 'unemployment',\n",
    "    'MarkDown1': 'markdown1',\n",
    "    'MarkDown2': 'markdown2',\n",
    "    'MarkDown3': 'markdown3',\n",
    "    'MarkDown4': 'markdown4',\n",
    "    'MarkDown5': 'markdown5',\n",
    "    'Type': 'store_type',\n",
    "    'Size': 'store_size'\n",
    "})\n",
    "\n",
    "# Select columns\n",
    "store_expanded_df = store_expanded_df[[\n",
    "    'store', 'dept', 'date', 'temperature', 'fuel_price', 'cpi', 'unemployment',\n",
    "    'markdown1', 'markdown2', 'markdown3', 'markdown4', 'markdown5',\n",
    "    'total_markdown', 'has_markdown', 'store_type', 'store_size'\n",
    "]]\n",
    "\n",
    "print(f'Store features created: {store_expanded_df.shape}')\n",
    "store_expanded_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save to Parquet for Feast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting dataframes for efficient Ray merge_asof...\n",
      "✓ Dataframes sorted by (store, dept, date) and (store, date)\n",
      "\n",
      "Saved feature files:\n",
      "   ✓ sales_features.parquet: 18.5 MB\n",
      "   ✓ store_features.parquet: 0.4 MB\n",
      "\n",
      "Feature engineering complete!\n",
      "   Sales features: 421,570 records\n",
      "   Store features: 606,242 records\n"
     ]
    }
   ],
   "source": [
    "# Save feature files\n",
    "features_dir= Path(\"/shared/feature_repo/data\")\n",
    "\n",
    "os.makedirs(features_dir, exist_ok=True)\n",
    "\n",
    "# Ray's offline store requires sorted data to avoid slow manual point-in-time joins\n",
    "print('Sorting dataframes for efficient Ray merge_asof...')\n",
    "sales_df = sales_df.sort_values(['store', 'dept', 'date'], ascending=True).reset_index(drop=True)\n",
    "store_expanded_df = store_expanded_df.sort_values(['store', 'date'], ascending=True).reset_index(drop=True)\n",
    "print('✓ Dataframes sorted by (store, dept, date) and (store, date)\\n')\n",
    "\n",
    "sales_df.to_parquet(features_dir / 'sales_features.parquet', index=False)\n",
    "store_expanded_df.to_parquet(features_dir / 'store_features.parquet', index=False)\n",
    "\n",
    "print('Saved feature files:')\n",
    "for file in ['sales_features.parquet', 'store_features.parquet']:\n",
    "    path =  features_dir/ file\n",
    "    size_mb = path.stat().st_size / 1024 / 1024\n",
    "    print(f'   ✓ {file}: {size_mb:.1f} MB')\n",
    "\n",
    "print('\\nFeature engineering complete!')\n",
    "print(f'   Sales features: {len(sales_df):,} records')\n",
    "print(f'   Store features: {len(store_expanded_df):,} records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize Feast Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r feature_repo/ /shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feast initialized\n",
      "   Project: sales_demand_forecasting\n",
      "   Offline store: file\n"
     ]
    }
   ],
   "source": [
    "store = FeatureStore(repo_path='/shared/feature_repo')\n",
    "\n",
    "print('Feast initialized')\n",
    "print(f'   Project: {store.project}')\n",
    "print(f'   Offline store: {store.config.offline_store.type}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Register Features with Feast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No project found in the repository. Using project name sales_demand_forecasting defined in feature_store.yaml\n",
      "Applying changes for project sales_demand_forecasting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.12/site-packages/feast/feature_store.py:583: RuntimeWarning: On demand feature view is an experimental feature. This API is stable, but the functionality does not scale well for offline retrieval\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created project sales_demand_forecasting\n",
      "Created entity dept\n",
      "Created entity store\n",
      "Created feature view sales_history_features\n",
      "Created feature view store_external_features\n",
      "Created on demand feature view temporal_transformations\n",
      "Created on demand feature view feature_transformations\n",
      "Created feature service demand_forecasting_service\n",
      "\n",
      "Created sqlite table sales_demand_forecasting_sales_history_features\n",
      "Created sqlite table sales_demand_forecasting_store_external_features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /shared/feature_repo\n",
    "feast apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered Feature Views:\n",
      "    sales_history_features: 8 features\n",
      "    store_external_features: 13 features\n",
      "\n",
      " Feature Services:\n",
      "    demand_forecasting_service\n"
     ]
    }
   ],
   "source": [
    "# Refresh store\n",
    "store = FeatureStore(repo_path='/shared/feature_repo')\n",
    "\n",
    "print('Registered Feature Views:')\n",
    "for fv in store.list_feature_views():\n",
    "    print(f'    {fv.name}: {len(fv.features)} features')\n",
    "\n",
    "print('\\n Feature Services:')\n",
    "for fs in store.list_feature_services():\n",
    "    print(f'    {fs.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Feature Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrieving features from Feast offline store...\n",
      "   Entity rows: 1,000\n",
      "\n",
      " Retrieved in 2.93 seconds\n",
      "   Shape: (1000, 31)\n",
      "   Features: 31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>dept</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>weekly_sales</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>sales_lag_1</th>\n",
       "      <th>sales_lag_2</th>\n",
       "      <th>sales_lag_4</th>\n",
       "      <th>sales_rolling_mean_4</th>\n",
       "      <th>sales_rolling_mean_12</th>\n",
       "      <th>...</th>\n",
       "      <th>has_markdown</th>\n",
       "      <th>store_type</th>\n",
       "      <th>store_size</th>\n",
       "      <th>sales_velocity</th>\n",
       "      <th>sales_acceleration</th>\n",
       "      <th>demand_stability_score</th>\n",
       "      <th>sales_normalized</th>\n",
       "      <th>temperature_normalized</th>\n",
       "      <th>sales_per_sqft</th>\n",
       "      <th>markdown_efficiency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>67</td>\n",
       "      <td>2010-02-05 00:00:00+00:00</td>\n",
       "      <td>6939.56</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6939.56</td>\n",
       "      <td>6939.56</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>203750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.034698</td>\n",
       "      <td>0.358421</td>\n",
       "      <td>0.034059</td>\n",
       "      <td>6939.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-02-05 00:00:00+00:00</td>\n",
       "      <td>5926.11</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5926.11</td>\n",
       "      <td>5926.11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>112238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.029631</td>\n",
       "      <td>0.468105</td>\n",
       "      <td>0.052799</td>\n",
       "      <td>5926.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>2010-02-05 00:00:00+00:00</td>\n",
       "      <td>7473.91</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7473.91</td>\n",
       "      <td>7473.91</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>206302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.037370</td>\n",
       "      <td>0.468105</td>\n",
       "      <td>0.036228</td>\n",
       "      <td>7473.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   store  dept           event_timestamp  weekly_sales  is_holiday  \\\n",
       "0     31    67 2010-02-05 00:00:00+00:00       6939.56       False   \n",
       "1     12     6 2010-02-05 00:00:00+00:00       5926.11       False   \n",
       "2     28    29 2010-02-05 00:00:00+00:00       7473.91       False   \n",
       "\n",
       "   sales_lag_1  sales_lag_2  sales_lag_4  sales_rolling_mean_4  \\\n",
       "0          NaN          NaN          NaN               6939.56   \n",
       "1          NaN          NaN          NaN               5926.11   \n",
       "2          NaN          NaN          NaN               7473.91   \n",
       "\n",
       "   sales_rolling_mean_12  ...  has_markdown  store_type  store_size  \\\n",
       "0                6939.56  ...             0           A      203750   \n",
       "1                5926.11  ...             0           B      112238   \n",
       "2                7473.91  ...             0           A      206302   \n",
       "\n",
       "   sales_velocity  sales_acceleration  demand_stability_score  \\\n",
       "0             NaN                 NaN                     1.0   \n",
       "1             NaN                 NaN                     1.0   \n",
       "2             NaN                 NaN                     1.0   \n",
       "\n",
       "   sales_normalized  temperature_normalized  sales_per_sqft  \\\n",
       "0          0.034698                0.358421        0.034059   \n",
       "1          0.029631                0.468105        0.052799   \n",
       "2          0.037370                0.468105        0.036228   \n",
       "\n",
       "   markdown_efficiency  \n",
       "0              6939.56  \n",
       "1              5926.11  \n",
       "2              7473.91  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample 1000 records for testing\n",
    "entity_df = sales_df[['store', 'dept', 'date']].sample(1000, random_state=42).copy()\n",
    "entity_df = entity_df.rename(columns={'date': 'event_timestamp'})\n",
    "\n",
    "print(f' Retrieving features from Feast offline store...')\n",
    "print(f'   Entity rows: {len(entity_df):,}')\n",
    "\n",
    "start = time.time()\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=store.get_feature_service('demand_forecasting_service'),\n",
    ").to_df()\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f'\\n Retrieved in {elapsed:.2f} seconds')\n",
    "print(f'   Shape: {training_df.shape}')\n",
    "print(f'   Features: {training_df.shape[1]}')\n",
    "\n",
    "training_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Completed:\n",
    "✅ Downloaded Walmart dataset (421K records)  \n",
    "✅ Computed time-series features (lags, rolling stats)  \n",
    "✅ Created store features (external factors, metadata)  \n",
    "✅ Saved to Parquet (sales_features, store_features)  \n",
    "✅ Registered with Feast (feature views, services)  \n",
    "✅ Tested offline feature retrieval (25-32 features)\n",
    "\n",
    "### Next Steps:\n",
    "Proceed to **Notebook 02** to submit distributed training with Kubeflow Training SDK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
