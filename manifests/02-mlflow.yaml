# MLflow Server for Experiment Tracking
#
# Uses:
# - PostgreSQL backend for tracking data (metrics, params, runs)
# - Shared PVC for artifact storage (models, plots, etc.)
#
# No MinIO required - artifacts stored directly on shared PVC
#
# =============================================================================
# SETUP (after deploying with kubectl apply -k manifests/)
# =============================================================================
#
# 1. Create MLflow database:
#    kubectl exec -n feast-trainer-demo deploy/postgres -- \
#      psql -U postgres -c "CREATE DATABASE mlflow OWNER feast;"
#
# 2. Restart MLflow to pick up the database:
#    kubectl rollout restart deploy/mlflow -n feast-trainer-demo
#
# =============================================================================
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mlflow
  namespace: feast-trainer-demo
  labels:
    app: mlflow
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mlflow
  template:
    metadata:
      labels:
        app: mlflow
    spec:
      initContainers:
      - name: wait-for-postgres
        image: quay.io/modh/runtime-images:runtime-cuda-tensorflow-ubi9-python-3.11-2025a-20250109
        command:
        - /bin/bash
        - -c
        - |
          echo "Waiting for PostgreSQL..."
          for i in {1..30}; do
            if pg_isready -h postgres.feast-trainer-demo.svc.cluster.local -U feast -d mlflow 2>/dev/null; then
              echo "PostgreSQL is ready"
              exit 0
            fi
            echo "Waiting... ($i/30)"
            sleep 2
          done
          echo "Warning: PostgreSQL may not be ready, proceeding anyway"
      containers:
      - name: mlflow
        image: quay.io/modh/mlflow-server:v2.20.2-rhoai-24
        args:
        - mlflow
        - server
        - --host=0.0.0.0
        - --port=5000
        - --backend-store-uri=postgresql+psycopg2://feast:feast123@postgres.feast-trainer-demo.svc.cluster.local:5432/mlflow
        - --default-artifact-root=/shared/mlflow-artifacts
        - --serve-artifacts
        ports:
        - containerPort: 5000
          name: http
        volumeMounts:
        - name: shared-storage
          mountPath: /shared
        resources:
          requests:
            cpu: 200m
            memory: 512Mi
          limits:
            cpu: 1
            memory: 2Gi
        readinessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 30
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 60
          periodSeconds: 30
      volumes:
      - name: shared-storage
        persistentVolumeClaim:
          claimName: shared
---
apiVersion: v1
kind: Service
metadata:
  name: mlflow
  namespace: feast-trainer-demo
  labels:
    app: mlflow
spec:
  selector:
    app: mlflow
  ports:
  - name: http
    port: 5000
    targetPort: 5000
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: mlflow
  namespace: feast-trainer-demo
spec:
  to:
    kind: Service
    name: mlflow
  port:
    targetPort: http
  tls:
    termination: edge
#
# =============================================================================
# ACCESS URLs (after deployment)
# =============================================================================
#
# MLflow UI: https://mlflow-feast-trainer-demo.apps.<cluster>/
#
# =============================================================================
# SDK Configuration
# =============================================================================
#
# In-cluster (training jobs):
#   MLFLOW_TRACKING_URI=http://mlflow.feast-trainer-demo.svc.cluster.local:5000
#
# External (notebooks via workbench):
#   MLFLOW_TRACKING_URI=https://mlflow-feast-trainer-demo.apps.<cluster>/
#
