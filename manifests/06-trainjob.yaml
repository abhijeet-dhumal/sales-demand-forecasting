# Sales Forecasting Multi-Node Training Job
#
# Uses built-in torch-distributed ClusterTrainingRuntime for DDP training with:
# - Kubeflow TrainJob for multi-node orchestration (uses torchrun)
# - Feast get_historical_features() via remote KubeRay cluster
# - MLflow for experiment tracking
# - Training script mounted from ConfigMap (generated by Kustomize)
#
# Prerequisites:
# - Infrastructure deployed: kubectl apply -k manifests/
# - Data prepared via dataprep job (05-dataprep-job.yaml)
#
# Usage:
#   kubectl apply -f 06-trainjob.yaml
#   kubectl logs -f -l trainer.kubeflow.org/trainjob-name=sales-training
#
---
apiVersion: trainer.kubeflow.org/v1alpha1
kind: TrainJob
metadata:
  name: sales-training
  namespace: feast-trainer-demo
  labels:
    app: sales-training
    pipeline: sales-forecasting
spec:
  runtimeRef:
    name: torch-distributed
    kind: ClusterTrainingRuntime
  trainer:
    image: quay.io/modh/training:py312-cuda128-torch280
    numNodes: 2
    numProcPerNode: 1
    resourcesPerNode:
      requests:
        cpu: "4"
        memory: "8Gi"
        # nvidia.com/gpu: "1"  # Uncomment to request GPU (uses NCCL backend)
      limits:
        cpu: "8"
        memory: "16Gi"
        # nvidia.com/gpu: "1"  # Uncomment to limit GPU
    env:
    # Data paths
    - name: DATA_PATH
      value: /shared/data
    - name: OUTPUT_DIR
      value: /shared/models
    - name: FEATURE_REPO
      value: /shared/feature_repo
    # Training hyperparameters
    - name: NUM_EPOCHS
      value: "20"
    - name: BATCH_SIZE
      value: "512"
    - name: LEARNING_RATE
      value: "0.001"
    - name: EARLY_STOP_PATIENCE
      value: "5"
    # Ray TLS config
    - name: RAY_USE_TLS
      value: "1"
    - name: RAY_TLS_CA_CERT
      value: /ray-tls/ca.crt
    - name: RAY_TLS_SERVER_CERT
      value: /ray-tls/tls.crt
    - name: RAY_TLS_SERVER_KEY
      value: /ray-tls/tls.key
    - name: RAY_CLUSTER_NAME
      value: "feast-ray"
    - name: RAY_NAMESPACE
      value: "feast-trainer-demo"
    # MLflow config
    - name: MLFLOW_TRACKING_URI
      value: "http://mlflow.feast-trainer-demo.svc.cluster.local:5000"
    - name: MLFLOW_EXPERIMENT_NAME
      value: "sales-forecasting-ddp"
    # Model Registry config
    - name: MODEL_REGISTRY_URL
      value: "https://model-catalog.rhoai-model-registries.svc.cluster.local:8443"
    - name: REGISTERED_MODEL_NAME
      value: "sales-forecast"
    command:
    - bash
    - -c
    - |
      set -e
      echo "============================================================"
      echo "SALES FORECASTING - MULTI-NODE DDP TRAINING"
      echo "PET_NNODES=$PET_NNODES PET_NODE_RANK=$PET_NODE_RANK"
      echo "PET_MASTER_ADDR=$PET_MASTER_ADDR PET_MASTER_PORT=$PET_MASTER_PORT"
      echo "============================================================"
      
      # Install dependencies
      pip install --quiet --target=/tmp/pylibs \
        "feast[postgres,ray]==0.59.0" \
        codeflare-sdk \
        psycopg2-binary \
        scikit-learn \
        joblib \
        mlflow \
        model-registry
      
      export PYTHONPATH=/tmp/pylibs:$PYTHONPATH
      export PYTHONUNBUFFERED=1
      
      # Feast/Ray auth setup
      export FEAST_RAY_AUTH_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
      export FEAST_RAY_AUTH_SERVER="https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}"
      export FEAST_RAY_SKIP_TLS=true
      
      # Launch distributed training using torchrun with PET_* env vars from Training Operator
      torchrun \
        --nnodes=$PET_NNODES \
        --nproc_per_node=$PET_NPROC_PER_NODE \
        --node_rank=$PET_NODE_RANK \
        --master_addr=$PET_MASTER_ADDR \
        --master_port=$PET_MASTER_PORT \
        /scripts/train_ddp.py
  podTemplateOverrides:
  - targetJobs:
    - name: node
    spec:
      serviceAccountName: feast-sa
      volumes:
      - name: shared-storage
        persistentVolumeClaim:
          claimName: shared
      - name: ray-tls
        secret:
          secretName: ray-worker-secret-feast-ray
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 4Gi
      - name: training-scripts
        configMap:
          name: training-scripts
          defaultMode: 0755
      containers:
      - name: node
        volumeMounts:
        - name: shared-storage
          mountPath: /shared
        - name: ray-tls
          mountPath: /ray-tls
          readOnly: true
        - name: dshm
          mountPath: /dev/shm
        - name: training-scripts
          mountPath: /scripts
          readOnly: true
