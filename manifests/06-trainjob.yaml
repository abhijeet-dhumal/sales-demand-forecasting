# Sales Forecasting Training Job
#
# Full MLOps pipeline:
# - Feast get_historical_features() via remote KubeRay cluster
# - PyTorch with GPU acceleration
# - MLflow for experiment tracking (metrics, params, artifacts)
# - RHOAI Model Registry for production model versioning
#
# Prerequisites:
# - RayCluster "feast-ray" deployed (03-raycluster.yaml)
# - MLflow server running (02-mlflow.yaml)
# - Data prepared via dataprep job (05-dataprep-job.yaml)
# - Model Registry "sales-model-registry" in rhoai-model-registries namespace
#
# Usage:
#   kubectl apply -f 06-trainjob.yaml
#   kubectl logs -f -l app=sales-training
#
apiVersion: batch/v1
kind: Job
metadata:
  name: sales-training
  namespace: feast-trainer-demo
  labels:
    app: sales-training
    pipeline: sales-forecasting
spec:
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: sales-training
    spec:
      serviceAccountName: feast-sa
      restartPolicy: Never
      containers:
      - name: trainer
        image: quay.io/modh/training:py312-cuda128-torch280
        resources:
          requests:
            cpu: "4"
            memory: "8Gi"
            nvidia.com/gpu: "1"
          limits:
            cpu: "8"
            memory: "16Gi"
            nvidia.com/gpu: "1"
        env:
        - name: DATA_PATH
          value: /shared/data
        - name: OUTPUT_DIR
          value: /shared/models
        - name: FEATURE_REPO
          value: /shared/feature_repo
        - name: NUM_EPOCHS
          value: "20"
        - name: BATCH_SIZE
          value: "512"
        # Ray TLS for mTLS connection to KubeRay cluster
        - name: RAY_USE_TLS
          value: "1"
        - name: RAY_TLS_CA_CERT
          value: /ray-tls/ca.crt
        - name: RAY_TLS_SERVER_CERT
          value: /ray-tls/tls.crt
        - name: RAY_TLS_SERVER_KEY
          value: /ray-tls/tls.key
        - name: RAY_CLUSTER_NAME
          value: "feast-ray"
        - name: RAY_NAMESPACE
          value: "feast-trainer-demo"
        # MLflow for experiment tracking (standalone server, no auth required)
        - name: MLFLOW_TRACKING_URI
          value: "http://mlflow.feast-trainer-demo.svc.cluster.local:5000"
        - name: MLFLOW_EXPERIMENT_NAME
          value: "sales-forecasting"
        # RHOAI Model Registry for production models
        - name: MODEL_REGISTRY_URL
          value: "http://sales-model-registry.rhoai-model-registries.svc.cluster.local:8080"
        - name: REGISTERED_MODEL_NAME
          value: "sales-forecast"
        volumeMounts:
        - name: shared-storage
          mountPath: /shared
        - name: ray-tls
          mountPath: /ray-tls
          readOnly: true
        - name: dshm
          mountPath: /dev/shm
        command:
        - bash
        - -c
        - |
          set -e
          echo "============================================================"
          echo "SALES FORECASTING TRAINING"
          echo "Feast + KubeRay + MLflow + Model Registry"
          echo "============================================================"
          
          echo "Installing dependencies..."
          pip install --quiet --target=/tmp/pylibs "feast[postgres,ray]==0.59.0" codeflare-sdk psycopg2-binary scikit-learn joblib mlflow model-registry
          export PYTHONPATH=/tmp/pylibs:$PYTHONPATH
          
          # Configure CodeFlare SDK authentication
          export FEAST_RAY_AUTH_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
          export FEAST_RAY_AUTH_SERVER="https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}"
          export FEAST_RAY_SKIP_TLS=true
          
          python3 << 'PYTHON_SCRIPT'
          import os
          import json
          import logging
          import numpy as np
          import pandas as pd
          import torch
          import torch.nn as nn
          from torch.utils.data import DataLoader, Dataset
          from sklearn.preprocessing import StandardScaler
          import joblib
          from datetime import datetime, timezone, timedelta
          import time
          import shutil
          import requests
          
          logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
          logger = logging.getLogger(__name__)
          
          DATA_PATH = os.environ.get("DATA_PATH", "/shared/data")
          OUTPUT_DIR = os.environ.get("OUTPUT_DIR", "/shared/models")
          FEATURE_REPO = os.environ.get("FEATURE_REPO", "/shared/feature_repo")
          NUM_EPOCHS = int(os.environ.get("NUM_EPOCHS", 20))
          BATCH_SIZE = int(os.environ.get("BATCH_SIZE", 512))
          
          # ================================================================
          # MLFLOW SETUP
          # ================================================================
          mlflow_enabled = False
          mlflow_run = None
          try:
              import mlflow
              tracking_uri = os.environ.get("MLFLOW_TRACKING_URI")
              if tracking_uri:
                  mlflow.set_tracking_uri(tracking_uri)
                  experiment_name = os.environ.get("MLFLOW_EXPERIMENT_NAME", "sales-forecasting")
                  mlflow.set_experiment(experiment_name)
                  mlflow_enabled = True
                  logger.info(f"MLflow tracking: {tracking_uri}")
                  logger.info(f"Experiment: {experiment_name}")
          except Exception as e:
              logger.warning(f"MLflow setup failed: {e}")
          
          class SalesMLP(nn.Module):
              def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout=0.2):
                  super().__init__()
                  layers = []
                  prev_dim = input_dim
                  for dim in hidden_dims:
                      layers.extend([nn.Linear(prev_dim, dim), nn.BatchNorm1d(dim), nn.ReLU(), nn.Dropout(dropout)])
                      prev_dim = dim
                  layers.append(nn.Linear(prev_dim, 1))
                  self.net = nn.Sequential(*layers)
              def forward(self, x):
                  return self.net(x).squeeze(-1)
          
          class SalesDataset(Dataset):
              def __init__(self, X, y):
                  self.X = torch.tensor(X, dtype=torch.float32)
                  self.y = torch.tensor(y, dtype=torch.float32)
              def __len__(self): return len(self.X)
              def __getitem__(self, i): return self.X[i], self.y[i]
          
          device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
          logger.info(f"Device: {device}")
          os.makedirs(OUTPUT_DIR, exist_ok=True)
          
          # ================================================================
          # FEAST: Get historical features via remote KubeRay cluster
          # ================================================================
          logger.info("=" * 60)
          logger.info("FEAST: get_historical_features() via Remote KubeRay")
          logger.info("=" * 60)
          
          from feast import FeatureStore
          
          ray_config = f"{FEATURE_REPO}/feature_store_ray.yaml"
          if os.path.exists(ray_config):
              shutil.copy(ray_config, f"{FEATURE_REPO}/feature_store.yaml")
              logger.info(f"Using Ray config: {ray_config}")
          
          store = FeatureStore(repo_path=FEATURE_REPO)
          
          logger.info("Creating entity DataFrame...")
          entity_rows = []
          base_date = datetime(2022, 1, 1, tzinfo=timezone.utc)
          NUM_WEEKS, NUM_STORES, NUM_DEPTS = 104, 45, 14
          for week in range(NUM_WEEKS):
              event_ts = base_date + timedelta(weeks=week)
              for store_id in range(1, NUM_STORES + 1):
                  for dept_id in range(1, NUM_DEPTS + 1):
                      entity_rows.append({"store_id": store_id, "dept_id": dept_id, "event_timestamp": event_ts})
          entity_df = pd.DataFrame(entity_rows)
          logger.info(f"Entity DataFrame: {len(entity_df):,} rows")
          
          logger.info("Calling get_historical_features() via remote KubeRay...")
          feature_start = time.time()
          training_data = store.get_historical_features(
              entity_df=entity_df,
              features=store.get_feature_service("training_features"),
          )
          training_df = training_data.to_df()
          feature_time = time.time() - feature_start
          logger.info(f"Features retrieved in {feature_time:.1f}s ({len(training_df):,} rows)")
          
          # ================================================================
          # DATA PREPARATION
          # ================================================================
          training_df = training_df.dropna(subset=["weekly_sales"]).sort_values("event_timestamp")
          split_idx = int(len(training_df) * 0.8)
          train_df, val_df = training_df.iloc[:split_idx], training_df.iloc[split_idx:]
          logger.info(f"Train: {len(train_df):,}, Val: {len(val_df):,}")
          
          exclude_cols = ["store_id", "dept_id", "date", "event_timestamp", "weekly_sales"]
          feature_cols = [c for c in training_df.columns if c not in exclude_cols and training_df[c].dtype in [np.float64, np.int64, np.float32, np.int32]]
          logger.info(f"Features ({len(feature_cols)}): {feature_cols}")
          
          X_train, y_train = train_df[feature_cols].fillna(0).values, train_df["weekly_sales"].values
          X_val, y_val = val_df[feature_cols].fillna(0).values, val_df["weekly_sales"].values
          
          scaler = StandardScaler()
          X_train, X_val = scaler.fit_transform(X_train), scaler.transform(X_val)
          y_scaler = StandardScaler()
          y_train_scaled = y_scaler.fit_transform(np.log1p(y_train).reshape(-1, 1)).flatten()
          y_val_scaled = y_scaler.transform(np.log1p(y_val).reshape(-1, 1)).flatten()
          
          joblib.dump({"scaler_X": scaler, "scaler_y": y_scaler, "use_log_transform": True}, f"{OUTPUT_DIR}/scalers.joblib")
          joblib.dump(feature_cols, f"{OUTPUT_DIR}/feature_cols.pkl")
          
          # ================================================================
          # MLFLOW: Start Run and Log Parameters
          # ================================================================
          if mlflow_enabled:
              mlflow_run = mlflow.start_run(run_name=f"sales-mlp-{datetime.now().strftime('%Y%m%d-%H%M%S')}")
              mlflow.log_params({
                  "model_type": "SalesMLP",
                  "hidden_dims": "256,128,64",
                  "dropout": 0.2,
                  "learning_rate": 1e-3,
                  "weight_decay": 0.01,
                  "batch_size": BATCH_SIZE,
                  "num_epochs": NUM_EPOCHS,
                  "num_features": len(feature_cols),
                  "train_samples": len(train_df),
                  "val_samples": len(val_df),
                  "device": str(device),
              })
              mlflow.set_tags({
                  "pipeline": "sales-forecasting",
                  "feast_version": "0.59.0",
                  "feature_store": "ray",
              })
              mlflow.log_metric("feature_retrieval_time", feature_time)
          
          # ================================================================
          # PYTORCH TRAINING
          # ================================================================
          train_dataset, val_dataset = SalesDataset(X_train, y_train_scaled), SalesDataset(X_val, y_val_scaled)
          train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
          val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)
          
          input_dim = X_train.shape[1]
          model = SalesMLP(input_dim).to(device)
          optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)
          scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)
          criterion = nn.MSELoss()
          best_val_loss = float('inf')
          best_mape = float('inf')
          
          logger.info("=" * 60)
          logger.info("TRAINING")
          logger.info("=" * 60)
          
          training_start = time.time()
          for epoch in range(NUM_EPOCHS):
              model.train()
              train_loss = 0.0
              for X_batch, y_batch in train_loader:
                  X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                  optimizer.zero_grad()
                  loss = criterion(model(X_batch), y_batch)
                  loss.backward()
                  torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                  optimizer.step()
                  train_loss += loss.item()
              train_loss /= len(train_loader)
              
              model.eval()
              val_loss = 0.0
              val_preds = []
              with torch.no_grad():
                  for X_batch, y_batch in val_loader:
                      X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                      pred = model(X_batch)
                      val_loss += criterion(pred, y_batch).item()
                      val_preds.extend(pred.cpu().numpy())
              val_loss /= len(val_loader)
              
              val_preds = np.array(val_preds)
              pred_orig = np.expm1(y_scaler.inverse_transform(val_preds.reshape(-1, 1)).flatten())
              mask = y_val > 1000
              mape = np.mean(np.abs((y_val[mask] - pred_orig[mask]) / y_val[mask])) * 100
              
              scheduler.step()
              
              # Log metrics to MLflow
              if mlflow_enabled:
                  mlflow.log_metrics({
                      "train_loss": train_loss,
                      "val_loss": val_loss,
                      "mape": mape,
                      "learning_rate": scheduler.get_last_lr()[0],
                  }, step=epoch)
              
              if (epoch + 1) % 5 == 0 or epoch == 0:
                  logger.info(f"Epoch {epoch+1}/{NUM_EPOCHS} - Train: {train_loss:.4f}, Val: {val_loss:.4f}, MAPE: {mape:.2f}%")
              
              if val_loss < best_val_loss:
                  best_val_loss = val_loss
                  best_mape = mape
                  torch.save(model.state_dict(), f"{OUTPUT_DIR}/model_best.pt")
          
          training_time = time.time() - training_start
          
          # Save final model and metadata
          torch.save(model.state_dict(), f"{OUTPUT_DIR}/model_final.pt")
          metadata = {
              "model_type": "SalesMLP",
              "input_dim": input_dim,
              "hidden_dims": [256, 128, 64],
              "dropout": 0.2,
              "best_val_loss": float(best_val_loss),
              "best_mape": float(best_mape),
              "feature_columns": feature_cols,
              "num_epochs": NUM_EPOCHS,
              "batch_size": BATCH_SIZE,
              "training_time_seconds": round(training_time, 2),
              "feature_retrieval_time_seconds": round(feature_time, 2),
              "train_samples": len(train_df),
              "val_samples": len(val_df),
              "trained_at": datetime.now(timezone.utc).isoformat()
          }
          with open(f"{OUTPUT_DIR}/model_metadata.json", "w") as f:
              json.dump(metadata, f, indent=2)
          
          # ================================================================
          # MLFLOW: Log Artifacts and Final Metrics
          # ================================================================
          if mlflow_enabled:
              mlflow.log_metrics({
                  "best_val_loss": best_val_loss,
                  "best_mape": best_mape,
                  "training_time": training_time,
              })
              mlflow.log_artifact(f"{OUTPUT_DIR}/model_best.pt")
              mlflow.log_artifact(f"{OUTPUT_DIR}/model_final.pt")
              mlflow.log_artifact(f"{OUTPUT_DIR}/model_metadata.json")
              mlflow.log_artifact(f"{OUTPUT_DIR}/scalers.joblib")
              mlflow.log_artifact(f"{OUTPUT_DIR}/feature_cols.pkl")
              
              run_id = mlflow_run.info.run_id
              artifact_uri = f"{OUTPUT_DIR}"
              mlflow.end_run()
              logger.info(f"MLflow run completed: {run_id}")
          else:
              run_id = datetime.now().strftime('%Y%m%d_%H%M%S')
              artifact_uri = OUTPUT_DIR
          
          # ================================================================
          # MODEL REGISTRY: Register production model
          # ================================================================
          logger.info("=" * 60)
          logger.info("REGISTERING MODEL IN RHOAI MODEL REGISTRY")
          logger.info("=" * 60)
          
          model_registry_url = os.environ.get("MODEL_REGISTRY_URL")
          registered_model_name = os.environ.get("REGISTERED_MODEL_NAME", "sales-forecast")
          version_id = datetime.now().strftime('%Y%m%d_%H%M%S')
          
          if model_registry_url:
              try:
                  from model_registry import ModelRegistry
                  
                  registry = ModelRegistry(server_address=model_registry_url, author="training-job", is_secure=False)
                  
                  rm = registry.register_model(
                      registered_model_name,
                      uri=f"pvc://shared/models",
                      version=version_id,
                      description=f"Sales forecast MLP - MAPE: {best_mape:.2f}%, Val Loss: {best_val_loss:.4f}",
                      model_format_name="pytorch",
                      model_format_version="2.0",
                      metadata={
                          "mape": str(round(best_mape, 2)),
                          "val_loss": str(round(best_val_loss, 4)),
                          "input_dim": str(input_dim),
                          "hidden_dims": "256,128,64",
                          "mlflow_run_id": run_id if mlflow_enabled else "none",
                          "feature_store": "feast",
                          "training_time": str(round(training_time, 1)),
                      }
                  )
                  logger.info(f"Model registered: {registered_model_name} v{version_id}")
              except Exception as e:
                  logger.warning(f"Model Registry registration failed: {e}")
                  # Try REST API fallback
                  try:
                      resp = requests.get(f"{model_registry_url}/api/model_registry/v1alpha3/registered_models", 
                                         params={"name": registered_model_name}, timeout=10)
                      if resp.status_code == 200 and resp.json().get("items"):
                          registered_model_id = resp.json()["items"][0]["id"]
                      else:
                          resp = requests.post(f"{model_registry_url}/api/model_registry/v1alpha3/registered_models",
                                              json={"name": registered_model_name, "description": "Sales forecast model"}, timeout=10)
                          resp.raise_for_status()
                          registered_model_id = resp.json()["id"]
                      
                      resp = requests.post(
                          f"{model_registry_url}/api/model_registry/v1alpha3/model_versions",
                          json={
                              "registeredModelId": registered_model_id,
                              "name": version_id,
                              "description": f"MAPE: {best_mape:.2f}%",
                              "customProperties": {
                                  "mape": {"stringValue": str(round(best_mape, 2))},
                                  "mlflow_run_id": {"stringValue": run_id if mlflow_enabled else "none"},
                              }
                          }, timeout=10
                      )
                      resp.raise_for_status()
                      logger.info(f"Model registered via REST: {registered_model_name} v{version_id}")
                  except Exception as rest_error:
                      logger.warning(f"REST API failed: {rest_error}")
          
          logger.info("=" * 60)
          logger.info("TRAINING COMPLETE")
          logger.info(f"  Best MAPE: {best_mape:.2f}%")
          logger.info(f"  Best Val Loss: {best_val_loss:.4f}")
          logger.info(f"  Training Time: {training_time:.1f}s")
          logger.info(f"  MLflow Run: {run_id if mlflow_enabled else 'disabled'}")
          logger.info(f"  Model Registry: {registered_model_name} v{version_id}")
          logger.info("=" * 60)
          PYTHON_SCRIPT
      volumes:
      - name: shared-storage
        persistentVolumeClaim:
          claimName: shared
      - name: ray-tls
        secret:
          secretName: ray-worker-secret-feast-ray
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 4Gi
