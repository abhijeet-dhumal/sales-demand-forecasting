# KubeRay Cluster for Distributed Feast Feature Processing
# GPU-ENABLED for accelerated data processing
# Requires: KubeRay operator installed in the cluster
apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: feast-ray
  namespace: feast-trainer-demo
  labels:
    app: feast-ray
    ray.io/cluster: feast-ray
  annotations:
    # Disable ODH TLS for internal cluster communication
    odh.ray.io/secure-trusted-network: "false"
spec:
  rayVersion: '2.52.1'
  # Autoscaling disabled for Kueue compatibility
  enableInTreeAutoscaling: false
  
  # Head node configuration - GPU enabled for accelerated processing
  headGroupSpec:
    serviceType: ClusterIP
    rayStartParams:
      dashboard-host: '0.0.0.0'
      num-cpus: '4'
      num-gpus: '1'  # 1 GPU for head node
      block: 'true'
      ray-client-server-port: '10001'
      object-store-memory: '4000000000'  # 4GB for object store
    template:
      metadata:
        labels:
          app: feast-ray
          component: head
          ray.io/cluster: feast-ray
      spec:
        containers:
        - name: ray-head
          image: quay.io/modh/ray:2.52.1-py312-cu128
          ports:
          - containerPort: 6379
            name: gcs
          - containerPort: 8265
            name: dashboard
          - containerPort: 10001
            name: client
          - containerPort: 8080
            name: metrics
          resources:
            limits:
              cpu: "8"
              memory: "24Gi"
              nvidia.com/gpu: "1"
            requests:
              cpu: "4"
              memory: "16Gi"
              nvidia.com/gpu: "1"
          # Create symlink so workbench paths (/opt/app-root/src/shared) work on Ray pods
          lifecycle:
            postStart:
              exec:
                command: ["/bin/sh", "-c", "mkdir -p /opt/app-root/src && ln -sf /shared /opt/app-root/src/shared"]
          env:
          - name: RAY_GRAFANA_HOST
            value: "http://grafana:3000"
          - name: RAY_PROMETHEUS_HOST
            value: "http://prometheus:9090"
          - name: FEAST_DATA_ROOT
            value: "/shared/data"
          # Pre-install Feast on head node
          - name: RAY_RUNTIME_ENV_SETUP
            value: "pip install feast[postgres,ray]==0.59.0 psycopg2-binary"
          volumeMounts:
          - name: ray-logs
            mountPath: /tmp/ray
          - name: shared-storage
            mountPath: /shared
          - name: dshm
            mountPath: /dev/shm
        volumes:
        - name: ray-logs
          emptyDir: {}
        - name: shared-storage
          persistentVolumeClaim:
            claimName: shared
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 8Gi
  
  # Worker nodes configuration - GPU enabled for distributed processing
  workerGroupSpecs:
  - replicas: 1
    minReplicas: 1
    maxReplicas: 2
    groupName: workers
    rayStartParams:
      num-cpus: '4'
      num-gpus: '1'  # 1 GPU for each worker
      block: 'true'
      object-store-memory: '4000000000'  # 4GB for object store
    template:
      metadata:
        labels:
          app: feast-ray
          component: worker
          ray.io/cluster: feast-ray
      spec:
        containers:
        - name: ray-worker
          image: quay.io/modh/ray:2.52.1-py312-cu128
          # Create symlink so workbench paths work on Ray pods
          lifecycle:
            postStart:
              exec:
                command: ["/bin/sh", "-c", "mkdir -p /opt/app-root/src && ln -sf /shared /opt/app-root/src/shared"]
          resources:
            limits:
              cpu: "8"
              memory: "24Gi"
              nvidia.com/gpu: "1"
            requests:
              cpu: "4"
              memory: "16Gi"
              nvidia.com/gpu: "1"
          env:
          - name: RAY_DISABLE_MEMORY_MONITOR
            value: "1"
          volumeMounts:
          - name: ray-logs
            mountPath: /tmp/ray
          - name: shared-storage
            mountPath: /shared
          - name: dshm
            mountPath: /dev/shm
        volumes:
        - name: ray-logs
          emptyDir: {}
        - name: shared-storage
          persistentVolumeClaim:
            claimName: shared
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 8Gi
---
# Service for Ray head node
apiVersion: v1
kind: Service
metadata:
  name: feast-ray-head
  namespace: feast-trainer-demo
  labels:
    app: feast-ray
    ray.io/cluster: feast-ray
spec:
  selector:
    app: feast-ray
    component: head
  ports:
  - name: client
    port: 10001
    targetPort: 10001
  - name: dashboard
    port: 8265
    targetPort: 8265
  - name: gcs
    port: 6379
    targetPort: 6379
  - name: metrics
    port: 8080
    targetPort: 8080
  type: ClusterIP
---
# Route for Ray Dashboard (OpenShift)
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: feast-ray-dashboard
  namespace: feast-trainer-demo
spec:
  to:
    kind: Service
    name: feast-ray-head
  port:
    targetPort: dashboard
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
